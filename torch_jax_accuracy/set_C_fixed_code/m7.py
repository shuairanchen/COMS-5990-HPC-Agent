"This is a version of JAX that provides more comprehensive output and\nimplement more utilities and complete translation that recreates the full\npipeline of the original PyTorch code (data loading, model definition,\ntraining loop, and evaluation), but it does so using JAXâ€™s functional\nstyle with Flax for the model, optax for optimization, and TFDS for data.\n\nError code will be recorded from this code version (if applicable)"\n\nimport time\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport optax\nfrom flax import linen as nn\nfrom functools import partial\n\n# ---------------------------\n# Data Loading and Preprocessing\n# ---------------------------\ndef preprocess(example):\n    # Convert image to float32, scale to [0,1] then normalize to [-1,1]\n    image = np.array(example['image'], dtype=np.float32) / 255.0\n    image = (image - 0.5) / 0.5\n    # Ensure image shape is (28, 28, 1)\n    if image.ndim == 2:\n        image = np.expand_dims(image, -1)\n    label = example['label']\n    return image, label\n\ndef get_datasets(batch_size=64):\n    # Load MNIST using TensorFlow Datasets\n    train_ds = tfds.load('mnist', split='train', shuffle_files=True)\n    test_ds  = tfds.load('mnist', split='test',  shuffle_files=False)\n\n    # Convert training dataset to numpy arrays and create batches\n    train_images, train_labels = [], []\n    for example in tfds.as_numpy(train_ds):\n        img, lab = preprocess(example)\n        train_images.append(img)\n        train_labels.append(lab)\n    train_images = np.stack(train_images)\n    train_labels = np.array(train_labels)\n\n    # Convert test dataset to numpy arrays and create batches\n    test_images, test_labels = [], []\n    for example in tfds.as_numpy(test_ds):\n        img, lab = preprocess(example)\n        test_images.append(img)\n        test_labels.append(lab)\n    test_images = np.stack(test_images)\n    test_labels = np.array(test_labels)\n\n    # Create batches\n    train_batches = [(train_images[i:i+batch_size], train_labels[i:i+batch_size])\n                     for i in range(0, len(train_labels), batch_size)]\n    test_batches = [(test_images[i:i+batch_size], test_labels[i:i+batch_size])\n                    for i in range(0, len(test_labels), batch_size)]\n\n    return train_batches, test_batches\n\n# ---------------------------\n# Model Definition using Flax Linen\n# ---------------------------\nclass SimpleNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # Flatten the input (28x28 pixels becomes 784)\n        x = x.reshape((x.shape[0], -1))\n        x = nn.Dense(features=128)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        return x\n\ndef create_train_state(rng, learning_rate):\n    model = SimpleNN()\n    # Initialize parameters with dummy input: shape (1, 28, 28, 1)\n    dummy_input = jnp.ones((1, 28, 28, 1))\n    params = model.init(rng, dummy_input)\n    # Use SGD optimizer with learning rate 0.01\n    optimizer = optax.sgd(learning_rate)\n    opt_state = optimizer.init(params)\n    return model, params, optimizer, opt_state\n\n# ---------------------------\n# Loss Function and Training Step\n# ---------------------------\ndef loss_fn(params, model, batch):\n    images, labels = batch\n    logits = model.apply(params, images)\n    one_hot = jax.nn.one_hot(labels, num_classes=10)\n    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n    return loss\n\n# @jax.jit\n@partial(jax.jit, static_argnames=('model', 'optimizer'))\ndef train_step(params, opt_state, batch, model, optimizer):\n    loss, grads = jax.value_and_grad(loss_fn)(params, model, batch)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\n# ---------------------------\n# Main Training and Evaluation Loop\n# ---------------------------\ndef main():\n    batch_size = 64\n    epochs = 5\n    # Load datasets\n    train_batches, test_batches = get_datasets(batch_size)\n\n    # Initialize model and optimizer\n    rng = jax.random.PRNGKey(0)\n    model, params, optimizer, opt_state = create_train_state(rng, learning_rate=0.01)\n\n    # Training Loop with Benchmarking\n    for epoch in range(epochs):\n        start_time = time.time()\n        for batch in train_batches:\n            params, opt_state, loss = train_step(params, opt_state, batch, model, optimizer)\n        end_time = time.time()\n        training_time = end_time - start_time\n        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}, Time: {training_time:.4f}s")\n\n    # Evaluation on Test Set with Benchmarking\n    correct = 0\n    total = 0\n    start_time = time.time()\n    for batch in test_batches:\n        images, labels = batch\n        logits = model.apply(params, images)\n        predictions = jnp.argmax(logits, axis=1)\n        correct += int(jnp.sum(predictions == labels))\n        total += images.shape[0]\n    end_time = time.time()\n    testing_time = end_time - start_time\n    accuracy = 100 * correct / total\n    print(f"Test Accuracy: {accuracy:.2f}%, Testing Time: {testing_time:.4f}s")\n\nif __name__ == "__main__":\n    main()