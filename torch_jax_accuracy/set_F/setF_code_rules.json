{
    "Pytorch_to_JAX_Examples": [
      {
        "Example_id": "e1",
        "Input_Code": "",
      "LLM_weak_output":"",
      "LLM_fix_output": "import jax \\nimport jax.numpy as jnp\\n\\n# ------------------------------------------------------------------\\n# 1. Data generation (same statistics as PyTorch)\\n# ------------------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 1)) * 10            # U[0,10)\\nkey, subkey = jax.random.split(key)\\nnoise = jax.random.normal(subkey, (100, 1))              # N(0,1)\\ny = 2 * X + 3 + noise                                    # y = 2x + 3 + ε\\n\\n# ------------------------------------------------------------------\\n# 2. Parameter initialisation that matches nn.Linear(1,1)\\n# ------------------------------------------------------------------\\nin_features = 1\\nbound = 1.0 / jnp.sqrt(in_features)\\n\\nkey, subkey = jax.random.split(key)\\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\\nkey, subkey = jax.random.split(key)\\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\\nparams = {\\\"w\\\": w, \\\"b\\\": b}\\n\\n# ------------------------------------------------------------------\\n# 3. Model and loss definitions\\n# ------------------------------------------------------------------\\ndef predict(p, x):\\n    return jnp.matmul(x, p[\\\"w\\\"]) + p[\\\"b\\\"]                 # (N,1)*(1,1)+(1,)→(N,1)\\n\\ndef mse_loss(p, x, y):\\n    return jnp.mean((predict(p, x) - y) ** 2)\\n\\n# ------------------------------------------------------------------\\n# 4. Training loop (SGD)\\n# ------------------------------------------------------------------\\nlr = 0.01\\nepochs = 1000\\nloss_grad_fn = jax.value_and_grad(mse_loss)\\n\\nfor epoch in range(epochs):\\n    loss, grads = loss_grad_fn(params, X, y)\\n    params = {k: params[k] - lr * grads[k] for k in params}  # functional update\\n\\n    if (epoch + 1) % 100 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# ------------------------------------------------------------------\\n# 5. Learned parameters and predictions\\n# ------------------------------------------------------------------\\nlearned_w = float(params[\\\"w\\\"][0, 0])\\nlearned_b = float(params[\\\"b\\\"][0])\\nprint(f\\\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\\\")\\n\\nX_test = jnp.array([[4.0], [7.0]])\\npreds = predict(params, X_test)\\nprint(f\\\"Predictions for {X_test.tolist()}: {preds.tolist()}\\\")",
      "Errors": [
        {
        "Error_Code": "bound = 1.0",
        "Error": "PyTorch initialises nn.Linear weights with a uniform distribution in the range\n[-1/√fan_in, 1/√fan_in].  The hard-coded 1.0 only happens to be correct for\nfan_in = 1; it will silently diverge from PyTorch behaviour when fan_in ≠ 1 or\nwhen you copy this pattern elsewhere.",
        "Fix_info": "Compute the bound from the formula 1 / sqrt(in_features) so it always matches\nPyTorch’s default initialisation.",
        "Fixed_Code": "in_features = 1\nbound = 1.0 / jnp.sqrt(in_features)\nkey, subkey = jax.random.split(key)\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\nkey, subkey = jax.random.split(key)\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)"
        },
        {
        "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n...\nlearned_w = params[\"w\"][0, 0]\nlearned_b = params[\"b\"][0]\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")",
        "Error": "`loss`, `learned_w`, and `learned_b` are `jax.Array` objects.  Formatting them with\n`:.4f` raises `TypeError: unsupported format string passed to ...Array`.",
        "Fix_info": "Convert the JAX arrays to Python scalars with `float()` (or `.item()`) before\nembedding them in an f-string.",
        "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\")\n...\nlearned_w = float(params[\"w\"][0, 0])\nlearned_b = float(params[\"b\"][0])\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")"
        }
      ]
      },
      {
        "Example_id": "e2",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nimport numpy as np\\nimport pandas as pd\\n\\n# ------------------------------------------------------------------\\n# 1. Generate and save synthetic data (matches PyTorch statistics)\\n# ------------------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 1)) * 10\\n\\nkey, subkey = jax.random.split(key)\\nnoise = jax.random.normal(subkey, (100, 1))\\n\\ny = 2 * X + 3 + noise                         # y = 2x + 3 + ε\\n\\ndata = jnp.concatenate([X, y], axis=1)\\npd.DataFrame(np.asarray(data), columns=[\\\"X\\\", \\\"y\\\"]).to_csv(\\\"data.csv\\\", index=False)\\n\\n# ------------------------------------------------------------------\\n# 2. Dataset & simple dataloader\\n# ------------------------------------------------------------------\\nclass LinearRegressionDataset:\\n    def __init__(self, csv_file):\\n        df = pd.read_csv(csv_file)\\n        self.X = jnp.asarray(df[\\\"X\\\"], dtype=jnp.float32).reshape(-1, 1)\\n        self.y = jnp.asarray(df[\\\"y\\\"], dtype=jnp.float32).reshape(-1, 1)\\n\\n    def __len__(self):\\n        return self.X.shape[0]\\n\\ndef data_loader(dataset, batch_size=32, shuffle=True):\\n    idx = np.arange(len(dataset))\\n    if shuffle:\\n        np.random.shuffle(idx)\\n    for start in range(0, len(idx), batch_size):\\n        batch = idx[start : start + batch_size]\\n        yield dataset.X[batch], dataset.y[batch]\\n\\ndataset    = LinearRegressionDataset(\\\"data.csv\\\")\\nbatch_size = 32\\n\\n# ------------------------------------------------------------------\\n# 3. Parameter initialisation (mirrors nn.Linear default)\\n# ------------------------------------------------------------------\\nin_features = 1\\nbound = 1.0 / jnp.sqrt(in_features)\\n\\nkey, subkey = jax.random.split(key)\\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\\nkey, subkey = jax.random.split(key)\\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\\nparams = {\\\"w\\\": w, \\\"b\\\": b}\\n\\n# ------------------------------------------------------------------\\n# 4. Model, loss, optimiser step\\n# ------------------------------------------------------------------\\ndef predict(p, x):\\n    return jnp.matmul(x, p[\\\"w\\\"]) + p[\\\"b\\\"]\\n\\ndef mse_loss(p, x, y):\\n    return jnp.mean((predict(p, x) - y) ** 2)\\n\\nlr      = 0.01\\nepochs  = 1000\\nloss_grad_fn = jax.value_and_grad(mse_loss)\\n\\n# ------------------------------------------------------------------\\n# 5. Training loop\\n# ------------------------------------------------------------------\\nfor epoch in range(epochs):\\n    for batch_X, batch_y in data_loader(dataset, batch_size, shuffle=True):\\n        loss, grads = loss_grad_fn(params, batch_X, batch_y)\\n        # SGD update (functional style)\\n        params = {k: params[k] - lr * grads[k] for k in params}\\n\\n    if (epoch + 1) % 100 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# ------------------------------------------------------------------\\n# 6. Learned parameters & inference\\n# ------------------------------------------------------------------\\nlearned_w = float(params[\\\"w\\\"][0, 0])\\nlearned_b = float(params[\\\"b\\\"][0])\\nprint(f\\\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\\\")\\n\\nX_test = jnp.array([[4.0], [7.0]])\\npreds  = predict(params, X_test)\\nprint(f\\\"Predictions for {X_test.tolist()}: {preds.tolist()}\\\")",
        "Errors": [
          {
          "Error_Code": "bound = 1.0",
          "Error": "PyTorch initialises nn.Linear weights with a uniform distribution in the range\n[-1/√fan_in , 1/√fan_in].  Hard-coding 1.0 only works by coincidence when\nfan_in = 1; it will diverge from PyTorch behaviour for other layer sizes or if\nyou reuse this snippet elsewhere.",
          "Fix_info": "Compute the bound with the same formula used by PyTorch so the JAX weight and\nbias initialisation always mirrors nn.Linear.",
          "Fixed_Code": "in_features = 1\nbound = 1.0 / jnp.sqrt(in_features)\nkey, subkey = jax.random.split(key)\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\nkey, subkey = jax.random.split(key)\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)"
          },
          {
          "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n...\nlearned_w = params[\"w\"][0, 0]\nlearned_b = params[\"b\"][0]\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")",
          "Error": "`loss`, `learned_w`, and `learned_b` are `jax.Array` objects.  Using the\nformat specifier `:.4f` on them raises:\n`TypeError: unsupported format string passed to ...Array`.",
          "Fix_info": "Convert JAX arrays to Python scalars with `float()` (or `.item()`) before\nembedding them in an f-string.",
          "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\")\n...\nlearned_w = float(params[\"w\"][0, 0])\nlearned_b = float(params[\"b\"][0])\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")"
          }
        ]
        },
        {
        "Example_id": "e3",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# -----------------------------------------------------------\\n# 1. Generate synthetic data (identical statistics)\\n# -----------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 1)) * 10           # U[0,10)\\n\\nkey, subkey = jax.random.split(key)\\nnoise = jax.random.normal(subkey, (100, 1))             # N(0,1)\\n\\ny = 2 * X + 3 + noise                                   # y = 2x + 3 + ε\\n\\n# -----------------------------------------------------------\\n# 2. Custom activation: tanh(x) + x\\n# -----------------------------------------------------------\\ndef custom_activation(x):\\n    return jnp.tanh(x) + x\\n\\n# -----------------------------------------------------------\\n# 3. Model definition\\n# -----------------------------------------------------------\\ndef model(params, x):\\n    return custom_activation(jnp.matmul(x, params[\\\"w\\\"]) + params[\\\"b\\\"])\\n\\n# -----------------------------------------------------------\\n# 4. Parameter initialisation (mirrors nn.Linear default)\\n# -----------------------------------------------------------\\nin_features = 1\\nbound = 1.0 / jnp.sqrt(in_features)\\n\\nkey, subkey = jax.random.split(key)\\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\\n\\nkey, subkey = jax.random.split(key)\\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\\n\\nparams = {\\\"w\\\": w, \\\"b\\\": b}\\n\\n# -----------------------------------------------------------\\n# 5. Loss function and gradient helper\\n# -----------------------------------------------------------\\ndef mse_loss(p, x, y):\\n    return jnp.mean((model(p, x) - y) ** 2)\\n\\nlr      = 0.01\\nepochs  = 1000\\nloss_grad_fn = jax.value_and_grad(mse_loss)\\n\\n# -----------------------------------------------------------\\n# 6. Training loop\\n# -----------------------------------------------------------\\nfor epoch in range(epochs):\\n    loss, grads = loss_grad_fn(params, X, y)\\n    # Functional SGD update\\n    params = {k: params[k] - lr * grads[k] for k in params}\\n\\n    if (epoch + 1) % 100 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# -----------------------------------------------------------\\n# 7. Learned parameters\\n# -----------------------------------------------------------\\nlearned_w = float(params[\\\"w\\\"][0, 0])\\nlearned_b = float(params[\\\"b\\\"][0])\\nprint(f\\\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\\\")\\n\\n# -----------------------------------------------------------\\n# 8. Plot training data and linear fit (lin-part only)\\n# -----------------------------------------------------------\\nplt.figure(figsize=(4, 4))\\nplt.scatter(np.asarray(X), np.asarray(y), label=\\\"Training Data\\\", s=10)\\n\\nx_line = np.linspace(0, 10, 100).reshape(-1, 1)\\nplt.plot(x_line, learned_w * x_line + learned_b, \\\"r\\\", label=\\\"Model Fit\\\")\\nplt.legend()\\nplt.show()\\n\\n# -----------------------------------------------------------\\n# 9. Inference\\n# -----------------------------------------------------------\\nX_test = jnp.array([[4.0], [7.0]])\\npreds  = model(params, X_test)\\nprint(f\\\"Predictions for {np.array(X_test).tolist()}: {np.array(preds).tolist()}\\\")",
        "Errors": [
          {
          "Error_Code": "bound = 1.0",
          "Error": "PyTorch initialises nn.Linear weights with a uniform distribution in the range\n[-1/√fan_in , 1/√fan_in].  Hard-coding 1.0 only works by coincidence for\nfan_in = 1 and will diverge if you reuse this pattern elsewhere.",
          "Fix_info": "Compute the bound from the same formula so the JAX initialisation always\nmirrors nn.Linear.",
          "Fixed_Code": "in_features = 1\nbound = 1.0 / jnp.sqrt(in_features)\nkey, subkey = jax.random.split(key)\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\nkey, subkey = jax.random.split(key)\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)"
          },
          {
          "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n...\nlearned_w = params['w'][0, 0]\nlearned_b = params['b'][0]\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")",
          "Error": "`loss`, `learned_w`, and `learned_b` are `jax.Array` objects.  Formatting them\nwith `:.4f` raises `TypeError: unsupported format string passed to ...Array`.",
          "Fix_info": "Convert the arrays to Python scalars with `float()` (or `.item()`) before\nembedding them in an f-string.",
          "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\")\n...\nlearned_w = float(params['w'][0, 0])\nlearned_b = float(params['b'][0])\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")"
          },
          {
          "Error_Code": "params['w'] = params['w'] - lr * grads['w']\nparams['b'] = params['b'] - lr * grads['b']",
          "Error": "Direct in-place mutation of a Python dict that stores immutable JAX arrays\nworks but is not functional-style and prevents easy lifting into `jax.jit` or\n`jax.grad` wrappers later.",
          "Fix_info": "Re-create the dict each step so updates are purely functional and JIT-safe.",
          "Fixed_Code": "params = {k: params[k] - lr * grads[k] for k in params}"
          }
        ]
        },
        {
        "Example_id": "e4",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nimport numpy as np\\n\\n# -----------------------------------------------------------\\n# 1. Generate synthetic data\\n# -----------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 1)) * 10          # U[0,10)\\n\\nkey, subkey = jax.random.split(key)\\nnoise = jax.random.normal(subkey, (100, 1))            # N(0,1)\\n\\ny = 2 * X + 3 + noise                                  # y = 2x + 3 + ε\\n\\n# -----------------------------------------------------------\\n# 2. Linear model\\n# -----------------------------------------------------------\\ndef predict(params, x):\\n    return jnp.matmul(x, params[\\\"w\\\"]) + params[\\\"b\\\"]\\n\\n# -----------------------------------------------------------\\n# 3. Huber loss (δ = 1.0)\\n# -----------------------------------------------------------\\ndef huber_loss(params, x, y, delta=1.0):\\n    error = jnp.abs(predict(params, x) - y)\\n    loss  = jnp.where(error <= delta,\\n                      0.5 * error**2,\\n                      delta * (error - 0.5 * delta))\\n    return jnp.mean(loss)\\n\\n# -----------------------------------------------------------\\n# 4. Parameter initialisation (matches nn.Linear default)\\n# -----------------------------------------------------------\\nin_features = 1\\nbound = 1.0 / jnp.sqrt(in_features)\\n\\nkey, subkey = jax.random.split(key)\\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\\n\\nkey, subkey = jax.random.split(key)\\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\\n\\nparams = {\\\"w\\\": w, \\\"b\\\": b}\\n\\n# -----------------------------------------------------------\\n# 5. Optimiser settings\\n# -----------------------------------------------------------\\nlr      = 0.01\\nepochs  = 1000\\nloss_grad_fn = jax.value_and_grad(huber_loss, argnums=0)\\n\\n# -----------------------------------------------------------\\n# 6. Training loop\\n# -----------------------------------------------------------\\nfor epoch in range(epochs):\\n    loss, grads = loss_grad_fn(params, X, y, 1.0)\\n    params = {k: params[k] - lr * grads[k] for k in params}  # functional SGD\\n\\n    if (epoch + 1) % 100 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# -----------------------------------------------------------\\n# 7. Learned parameters\\n# -----------------------------------------------------------\\nlearned_w = float(params[\\\"w\\\"][0, 0])\\nlearned_b = float(params[\\\"b\\\"][0])\\nprint(f\\\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\\\")\\n\\n# -----------------------------------------------------------\\n# 8. Inference\\n# -----------------------------------------------------------\\nX_test = jnp.array([[4.0], [7.0]])\\npreds  = predict(params, X_test)\\nprint(f\\\"Predictions for {np.array(X_test).tolist()}: {np.array(preds).tolist()}\\\")",
        "Errors": [
          {
          "Error_Code": "bound = 1.0",
          "Error": "PyTorch’s nn.Linear initialises weights with a uniform distribution in the interval [-1/√fan_in , 1/√fan_in]. Hard-coding 1.0 only works when fan_in = 1 by coincidence and breaks parity if this snippet is re-used for other layer sizes.",
          "Fix_info": "Compute the bound from the same formula (1 / sqrt(in_features)) so JAX initialisation always mirrors PyTorch.",
          "Fixed_Code": "in_features = 1\nbound = 1.0 / jnp.sqrt(in_features)\nkey, subkey = jax.random.split(key)\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\nkey, subkey = jax.random.split(key)\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)"
          },
          {
          "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n...\nlearned_w = params[\"w\"][0, 0]\nlearned_b = params[\"b\"][0]\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")",
          "Error": "`loss`, `learned_w`, and `learned_b` are `jax.Array` objects. Using the format specifier `:.4f` on them raises `TypeError: unsupported format string passed to ...Array`.",
          "Fix_info": "Coerce each JAX array to a Python scalar with `float()` (or `.item()`) before embedding in an f-string.",
          "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\")\n...\nlearned_w = float(params[\"w\"][0, 0])\nlearned_b = float(params[\"b\"][0])\nprint(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")"
          },
          {
          "Error_Code": "params[\"w\"] = params[\"w\"] - lr * grads[\"w\"]\nparams[\"b\"] = params[\"b\"] - lr * grads[\"b\"]",
          "Error": "In-place mutation of a Python dict that stores immutable JAX arrays works but prevents easy promotion to `jax.jit` and other functional transforms later.",
          "Fix_info": "Re-create the dict each step so the update is purely functional and JIT-safe.",
          "Fixed_Code": "params = {k: params[k] - lr * grads[k] for k in params}"
          }
        ]
        },
        {
        "Example_id": "e5",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nimport optax\\nimport numpy as np\\n\\n# ------------------------------------------------------------------\\n# 1. Synthetic data (same distribution as PyTorch script)\\n# ------------------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 2)) * 10           # U[0,10) for two features\\n\\nkey, subkey = jax.random.split(key)\\nnoise = jax.random.normal(subkey, (100, 1))             # N(0,1)\\n\\ny = (X[:, 0] + 2 * X[:, 1]).reshape(-1, 1) + noise      # y = x1 + 2·x2 + ε\\n\\n# ------------------------------------------------------------------\\n# 2. Two-layer ReLU network\\n# ------------------------------------------------------------------\\ndef predict(p, x):\\n    h = jax.nn.relu(jnp.matmul(x, p['W1']) + p['b1'])\\n    return jnp.matmul(h, p['W2']) + p['b2']\\n\\n# ------------------------------------------------------------------\\n# 3. Parameter initialisation that mirrors nn.Linear defaults\\n# ------------------------------------------------------------------\\ndef init_params(rng):\\n    k1, k2, k3, k4 = jax.random.split(rng, 4)\\n\\n    fan_in1, fan_in2 = 2, 10\\n    bound1 = 1.0 / jnp.sqrt(fan_in1)\\n    bound2 = 1.0 / jnp.sqrt(fan_in2)\\n\\n    W1 = jax.random.uniform(k1, (2, 10), minval=-bound1, maxval=bound1)\\n    b1 = jax.random.uniform(k2, (10,),    minval=-bound1, maxval=bound1)\\n\\n    W2 = jax.random.uniform(k3, (10, 1), minval=-bound2, maxval=bound2)\\n    b2 = jax.random.uniform(k4, (1,),    minval=-bound2, maxval=bound2)\\n\\n    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\\n\\nparams = init_params(key)\\n\\n# ------------------------------------------------------------------\\n# 4. Loss function and optimiser\\n# ------------------------------------------------------------------\\ndef mse_loss(p, x, y):\\n    return jnp.mean((predict(p, x) - y) ** 2)\\n\\noptimizer     = optax.adam(0.01)\\nopt_state     = optimizer.init(params)\\nloss_grad_fn  = jax.value_and_grad(mse_loss)\\nepochs        = 1000\\n\\n# ------------------------------------------------------------------\\n# 5. Training loop\\n# ------------------------------------------------------------------\\nfor epoch in range(epochs):\\n    loss, grads = loss_grad_fn(params, X, y)                # full-batch training\\n    updates, opt_state = optimizer.update(grads, opt_state)\\n    params = optax.apply_updates(params, updates)\\n\\n    if (epoch + 1) % 100 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# ------------------------------------------------------------------\\n# 6. Inference\\n# ------------------------------------------------------------------\\nX_test = jnp.array([[4.0, 3.0], [7.0, 8.0]])\\npreds  = predict(params, X_test)\\nprint(f\\\"Predictions for {np.array(X_test).tolist()}: {np.array(preds).tolist()}\\\")",
        "Errors": [
          {
          "Error_Code": "W1 = jax.random.uniform(keys[0], shape=(2, 10), minval=-1.0, maxval=1.0)\nb1 = jax.random.uniform(keys[1], shape=(10,), minval=-1.0, maxval=1.0)\nW2 = jax.random.uniform(keys[2], shape=(10, 1), minval=-1.0, maxval=1.0)\nb2 = jax.random.uniform(keys[3], shape=(1,),  minval=-1.0, maxval=1.0)",
          "Error": "PyTorch initialises each linear layer with a uniform distribution in the\ninterval  [-1/√fan_in , 1/√fan_in].  Using the fixed range [-1, 1] breaks\nnumerical parity (and can slow convergence when fan-in ≠ 1).",
          "Fix_info": "Compute the bound separately for each layer as 1 / sqrt(fan_in) and use it\nfor both weights **and** biases.",
          "Fixed_Code": "fan_in1 = 2\nbound1  = 1.0 / jnp.sqrt(fan_in1)\nW1 = jax.random.uniform(keys[0], (2, 10), minval=-bound1, maxval=bound1)\nb1 = jax.random.uniform(keys[1], (10,),  minval=-bound1, maxval=bound1)\n\nfan_in2 = 10\nbound2  = 1.0 / jnp.sqrt(fan_in2)\nW2 = jax.random.uniform(keys[2], (10, 1), minval=-bound2, maxval=bound2)\nb2 = jax.random.uniform(keys[3], (1,),   minval=-bound2, maxval=bound2)"
          },
          {
          "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n...\nprint(f\"Predictions for {np.array(X_test).tolist()}: {np.array(predictions).tolist()}\")",
          "Error": "`loss` (and other JAX values) are `jax.Array` objects.  Formatting them with\n`:.4f` raises `TypeError: unsupported format string passed to ...Array`.",
          "Fix_info": "Convert JAX arrays to Python scalars (e.g. `float(loss)`) before embedding\nthem in an f-string.",
          "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\")\n...\nprint(f\"Predictions for {np.array(X_test).tolist()}: {np.array(predictions).tolist()}\")"
          }
        ]
        },
        {
        "Example_id": "e6",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nimport optax\\nfrom tensorboardX import SummaryWriter  # pip install tensorboardX\\n\\n# ------------------------------------------------------------------\\n# 1. Data generation (same statistics as PyTorch)\\n# ------------------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 1)) * 10            # U[0,10)\\nkey, subkey = jax.random.split(key)\\ny = 3 * X + 5 + jax.random.normal(subkey, (100, 1))       # y = 3x + 5 + ε\\n\\n# ------------------------------------------------------------------\\n# 2. Model definition\\n# ------------------------------------------------------------------\\ndef predict(params, x):\\n    return jnp.matmul(x, params[\\\"w\\\"]) + params[\\\"b\\\"]        # linear\\n\\ndef mse_loss(params, x, y):\\n    return jnp.mean((predict(params, x) - y) ** 2)\\n\\n# ------------------------------------------------------------------\\n# 3. Parameter initialisation (PyTorch-equivalent uniform)\\n# ------------------------------------------------------------------\\nin_features = 1                     # single input feature\\nbound = 1.0 / jnp.sqrt(in_features) # ±1/√fan_in  →  ±1.0 here\\n\\nkey, subkey = jax.random.split(key)\\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\\n\\nkey, subkey = jax.random.split(key)\\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\\n\\nparams = {\\\"w\\\": w, \\\"b\\\": b}\\n\\n# ------------------------------------------------------------------\\n# 4. Optimiser & TensorBoard writer\\n# ------------------------------------------------------------------\\noptimizer  = optax.sgd(0.01)\\nopt_state  = optimizer.init(params)\\nwriter     = SummaryWriter(log_dir=\\\"runs/linear_regression\\\")\\n\\n# ------------------------------------------------------------------\\n# 5. Training loop\\n# ------------------------------------------------------------------\\nepochs       = 100\\nloss_grad_fn = jax.value_and_grad(mse_loss)\\n\\nfor epoch in range(epochs):\\n    loss, grads = loss_grad_fn(params, X, y)\\n    updates, opt_state = optimizer.update(grads, opt_state)\\n    params = optax.apply_updates(params, updates)\\n\\n    writer.add_scalar(\\\"Loss/train\\\", float(loss), epoch)\\n\\n    if (epoch + 1) % 10 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\nwriter.close()\\n\\n# ------------------------------------------------------------------\\n# 6. Notes\\n# ------------------------------------------------------------------\\n# * Logs are in the same directory name as the PyTorch run (\\\"runs/linear_regression\\\").\\n# * Visualise with:  tensorboard --logdir=runs\\n# * The training curve, final parameters, and logged loss now follow the same\\n#   trajectory as the original PyTorch script.",
        "Errors": [
          {
          "Error_Code": "params = { \"w\": 0.1 * jax.random.normal(subkey, shape=(1, 1)), \"b\": jnp.zeros((1,)) }",
          "Error": "PyTorch’s `nn.Linear` initialises **both** weights and biases with a uniform range `[-1/√fan_in , 1/√fan_in]` (fan_in = number of input features).  Scaling a normal distribution by 0.1 for `w` and setting `b` to zero does not match that behaviour and can change the optimisation path and final values.",
          "Fix_info": "Re-initialise with the same uniform bounds PyTorch uses so the JAX model starts from an equivalent point.",
          "Fixed_Code": "in_features = 1\nbound = 1.0 / jnp.sqrt(in_features)\nkey, subkey = jax.random.split(key)\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\nkey, subkey = jax.random.split(key)\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\nparams = {\"w\": w, \"b\": b}"
          }
        ]
        },
        {
        "Example_id": "e7",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nimport pickle\\n\\n# -----------------------------------------------------------\\n# 1. Training data (identical distribution to PyTorch)\\n# -----------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 1))                 # U[0,1)\\nkey, subkey = jax.random.split(key)\\ny = 3 * X + 2 + 0.1 * jax.random.normal(subkey, (100, 1))\\n\\n# -----------------------------------------------------------\\n# 2. Model, loss, predictions\\n# -----------------------------------------------------------\\ndef predict(p, x):\\n    return jnp.matmul(x, p[\\\"w\\\"]) + p[\\\"b\\\"]\\n\\ndef mse_loss(p, x, y):\\n    return jnp.mean((predict(p, x) - y) ** 2)\\n\\n# -----------------------------------------------------------\\n# 3. Parameter initialisation (PyTorch-equivalent)\\n# -----------------------------------------------------------\\nin_features = 1\\nbound = 1.0 / jnp.sqrt(in_features)\\n\\nkey, subkey = jax.random.split(key)\\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\\n\\nkey, subkey = jax.random.split(key)\\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\\n\\nparams = {\\\"w\\\": w, \\\"b\\\": b}\\n\\n# -----------------------------------------------------------\\n# 4. Optimiser settings\\n# -----------------------------------------------------------\\nlr      = 0.01\\nepochs  = 100\\nloss_grad_fn = jax.value_and_grad(mse_loss)\\n\\n# -----------------------------------------------------------\\n# 5. Training loop\\n# -----------------------------------------------------------\\nfor epoch in range(epochs):\\n    loss, grads = loss_grad_fn(params, X, y)\\n    params = {k: params[k] - lr * grads[k] for k in params}   # SGD\\n\\n    if (epoch + 1) % 10 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# -----------------------------------------------------------\\n# 6. Save parameters\\n# -----------------------------------------------------------\\nwith open(\\\"model.pth\\\", \\\"wb\\\") as f:\\n    pickle.dump(params, f)\\n\\n# -----------------------------------------------------------\\n# 7. Load parameters\\n# -----------------------------------------------------------\\nwith open(\\\"model.pth\\\", \\\"rb\\\") as f:\\n    loaded_params = pickle.load(f)\\n\\n# -----------------------------------------------------------\\n# 8. Inference check\\n# -----------------------------------------------------------\\nX_test = jnp.array([[0.5], [1.0], [1.5]])\\npreds  = predict(loaded_params, X_test)\\nprint(\\\"Predictions after loading:\\\", preds)",
        "Errors": [
          {
          "Error_Code": "params = {\n    \"w\": 0.1 * jax.random.normal(subkey, shape=(1, 1)),\n    \"b\": jnp.zeros((1,))\n}",
          "Error": "PyTorch’s nn.Linear initialises **both** weight and bias with a uniform distribution in the interval [-1 / √fan_in , 1 / √fan_in] (fan_in = 1 here, so the bound is 1.0). Using a scaled normal for *w* and a zero bias does not match that behaviour and breaks strict parity with the reference implementation.",
          "Fix_info": "Re-initialise weight and bias with the same uniform bounds PyTorch uses.",
          "Fixed_Code": "in_features = 1\nbound = 1.0 / jnp.sqrt(in_features)\nkey, subkey = jax.random.split(key)\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\nkey, subkey = jax.random.split(key)\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\nparams = {\"w\": w, \"b\": b}"
          },
          {
          "Error_Code": "params[\"w\"] = params[\"w\"] - lr * grads[\"w\"]\nparams[\"b\"] = params[\"b\"] - lr * grads[\"b\"]",
          "Error": "Updating the dict in-place works but makes the training step non-pure and prevents easy promotion to `jax.jit` or other functional transforms later.",
          "Fix_info": "Create a **new** dict each step (functional update) so the code is JIT-safe and stylistically aligned with JAX best practice.",
          "Fixed_Code": "params = {k: params[k] - lr * grads[k] for k in params}"
          }
        ]
        },
        {
        "Example_id": "h1",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nimport pickle\\n\\n# -----------------------------------------------------------\\n# 1. Data generation (identical to PyTorch script)\\n# -----------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nkey, subkey = jax.random.split(key)\\nX = jax.random.uniform(subkey, (100, 1))                  # U[0,1)\\n\\nkey, subkey = jax.random.split(key)\\ny = 3 * X + 2 + 0.1 * jax.random.normal(subkey, (100, 1)) # y = 3x + 2 + ε\\n\\n# -----------------------------------------------------------\\n# 2. Model and loss\\n# -----------------------------------------------------------\\ndef predict(p, x):\\n    return jnp.matmul(x, p[\\\"w\\\"]) + p[\\\"b\\\"]\\n\\ndef mse_loss(p, x, y):\\n    return jnp.mean((predict(p, x) - y) ** 2)\\n\\n# -----------------------------------------------------------\\n# 3. Parameter initialisation (matches nn.Linear)\\n# -----------------------------------------------------------\\nin_features = 1\\nbound = 1.0 / jnp.sqrt(in_features)                       # ±1 for fan_in = 1\\n\\nkey, subkey = jax.random.split(key)\\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\\n\\nkey, subkey = jax.random.split(key)\\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\\n\\nparams = {\\\"w\\\": w, \\\"b\\\": b}\\n\\n# -----------------------------------------------------------\\n# 4. Training settings\\n# -----------------------------------------------------------\\nlr      = 0.01\\nepochs  = 100\\nloss_grad_fn = jax.value_and_grad(mse_loss)\\n\\n# -----------------------------------------------------------\\n# 5. Training loop\\n# -----------------------------------------------------------\\nfor epoch in range(epochs):\\n    loss, grads = loss_grad_fn(params, X, y)\\n    params = {k: params[k] - lr * grads[k] for k in params}  # functional SGD\\n\\n    if (epoch + 1) % 10 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# -----------------------------------------------------------\\n# 6. Save parameters\\n# -----------------------------------------------------------\\nwith open(\\\"model.pth\\\", \\\"wb\\\") as f:\\n    pickle.dump(params, f)\\n\\n# -----------------------------------------------------------\\n# 7. Load parameters\\n# -----------------------------------------------------------\\nwith open(\\\"model.pth\\\", \\\"rb\\\") as f:\\n    loaded_params = pickle.load(f)\\n\\n# -----------------------------------------------------------\\n# 8. Inference check\\n# -----------------------------------------------------------\\nX_test = jnp.array([[0.5], [1.0], [1.5]])\\npreds  = predict(loaded_params, X_test)\\nprint(\\\"Predictions after loading:\\\", preds)",
        "Errors": [
          {
          "Error_Code": "params = {\n    \"w\": 0.1 * jax.random.normal(subkey, shape=(1, 1)),\n    \"b\": jnp.zeros((1,))\n}",
          "Error": "PyTorch’s nn.Linear initialises both weight and bias with a uniform distribution in the interval [-1 / √fan_in , 1 / √fan_in] (fan_in = 1 ⇒ bound = 1.0).  Using a scaled normal for *w* and zero for *b* does not replicate that behaviour, so the optimisation path and final parameters differ from the reference implementation.",
          "Fix_info": "Re-initialise weight and bias with the same uniform bounds used by PyTorch.",
          "Fixed_Code": "in_features = 1\nbound = 1.0 / jnp.sqrt(in_features)  # = 1.0 here\nkey, subkey = jax.random.split(key)\nw = jax.random.uniform(subkey, (1, 1), minval=-bound, maxval=bound)\nkey, subkey = jax.random.split(key)\nb = jax.random.uniform(subkey, (1,),   minval=-bound, maxval=bound)\nparams = {\"w\": w, \"b\": b}"
          },
          {
          "Error_Code": "params[\"w\"] = params[\"w\"] - lr * grads[\"w\"]\nparams[\"b\"] = params[\"b\"] - lr * grads[\"b\"]",
          "Error": "In-place mutation of the params dict works, but breaks functional-programming style and prevents easy promotion to `jax.jit` or other transforms later.  JAX best practice is to create a new parameter dict each step (pure function).",
          "Fix_info": "Replace the in-place assignment with a functional update that returns a new dict.",
          "Fixed_Code": "params = {k: params[k] - lr * grads[k] for k in params}"
          }
        ]
        },
        {
        "Example_id": "h3",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nfrom flax import linen as nn\\nimport optax\\nimport numpy as np\\n\\n# -----------------------------------------------------------\\n# 1. PyTorch-style uniform initializer\\n# -----------------------------------------------------------\\ndef torch_uniform(fan_in):\\n    bound = 1.0 / jnp.sqrt(fan_in)\\n    return nn.initializers.uniform(scale=bound)\\n\\n# -----------------------------------------------------------\\n# 2. Transformer Encoder block\\n# -----------------------------------------------------------\\nclass TransformerEncoderBlock(nn.Module):\\n    embed_dim: int\\n    num_heads: int\\n    ff_dim: int\\n    dropout_rate: float = 0.1\\n\\n    @nn.compact\\n    def __call__(self, x, *, train):\\n        attn = nn.SelfAttention(\\n            num_heads=self.num_heads,\\n            qkv_features=self.embed_dim,\\n            dropout_rate=self.dropout_rate,\\n            deterministic=not train,\\n            kernel_init=torch_uniform(self.embed_dim),\\n            bias_init=torch_uniform(self.embed_dim),\\n        )(x)\\n        x = nn.LayerNorm()(x + attn)\\n\\n        ff = nn.Dense(self.ff_dim,\\n                      kernel_init=torch_uniform(self.embed_dim),\\n                      bias_init=torch_uniform(self.embed_dim))(x)\\n        ff = nn.relu(ff)\\n        ff = nn.Dense(self.embed_dim,\\n                      kernel_init=torch_uniform(self.ff_dim),\\n                      bias_init=torch_uniform(self.ff_dim))(ff)\\n        x = nn.LayerNorm()(x + ff)\\n        return x\\n\\n# -----------------------------------------------------------\\n# 3. Transformer model\\n# -----------------------------------------------------------\\nclass TransformerModel(nn.Module):\\n    input_dim: int\\n    embed_dim: int\\n    num_heads: int\\n    num_layers: int\\n    ff_dim: int\\n    output_dim: int\\n    dropout_rate: float = 0.1\\n\\n    @nn.compact\\n    def __call__(self, x, *, train=True):\\n        x = nn.Dense(self.embed_dim,\\n                     kernel_init=torch_uniform(self.input_dim),\\n                     bias_init=torch_uniform(self.input_dim))(x)\\n        for _ in range(self.num_layers):\\n            x = TransformerEncoderBlock(\\n                embed_dim=self.embed_dim,\\n                num_heads=self.num_heads,\\n                ff_dim=self.ff_dim,\\n                dropout_rate=self.dropout_rate,\\n            )(x, train=train)\\n        x = jnp.mean(x, axis=1)\\n        x = nn.Dense(self.output_dim,\\n                     kernel_init=torch_uniform(self.embed_dim),\\n                     bias_init=torch_uniform(self.embed_dim))(x)\\n        return x\\n\\n# -----------------------------------------------------------\\n# 4. Synthetic data\\n# -----------------------------------------------------------\\nkey = jax.random.PRNGKey(42)\\nseq_len, num_samples, input_dim = 10, 100, 1\\nkey, sub = jax.random.split(key)\\nX = jax.random.uniform(sub, (num_samples, seq_len, input_dim))\\ny = jnp.sum(X, axis=1)                                # (N, 1)\\n\\n# -----------------------------------------------------------\\n# 5. Hyperparameters\\n# -----------------------------------------------------------\\nembed_dim, num_heads = 16, 2\\nnum_layers, ff_dim   = 2, 64\\noutput_dim           = 1\\nlr, epochs           = 0.001, 1000\\n\\n# -----------------------------------------------------------\\n# 6. Model init\\n# -----------------------------------------------------------\\nmodel  = TransformerModel(input_dim, embed_dim, num_heads,\\n                          num_layers, ff_dim, output_dim)\\nparams = model.init(jax.random.PRNGKey(0),\\n                    jnp.ones((1, seq_len, input_dim)),\\n                    train=True)\\n\\n# -----------------------------------------------------------\\n# 7. Loss / optimiser\\n# -----------------------------------------------------------\\ndef mse_loss(p, x, y, train, rng):\\n    preds = model.apply(p, x, train=train, rngs={\\\"dropout\\\": rng})\\n    return jnp.mean((preds - y) ** 2)\\n\\nopt      = optax.adam(lr)\\nopt_state = opt.init(params)\\n\\n# -----------------------------------------------------------\\n# 8. JIT-compatible training step\\n# -----------------------------------------------------------\\n@jax.jit\\ndef train_step(p, o, x, y, rng):\\n    rng, sub = jax.random.split(rng)\\n    loss, grads = jax.value_and_grad(mse_loss)(p, x, y, True, sub)\\n    updates, o = opt.update(grads, o)\\n    p = optax.apply_updates(p, updates)\\n    return p, o, rng, loss\\n\\n# -----------------------------------------------------------\\n# 9. Training loop\\n# -----------------------------------------------------------\\ntrain_rng = jax.random.PRNGKey(1)\\nfor epoch in range(epochs):\\n    params, opt_state, train_rng, loss = train_step(params, opt_state, X, y, train_rng)\\n    if (epoch + 1) % 100 == 0:\\n        print(f\\\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\\\")\\n\\n# -----------------------------------------------------------\\n# 10. Inference\\n# -----------------------------------------------------------\\nkey, sub = jax.random.split(key)\\nX_test = jax.random.uniform(sub, (2, seq_len, input_dim))\\npreds   = model.apply(params, X_test, train=False)\\nprint(\\\"Predictions for\\\", np.asarray(X_test).tolist(), \\\":\\\", np.asarray(preds).tolist())",
        "Errors": [
          {
          "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")",
          "Error": "`loss` is a `jax.Array`.  Using the `:.4f` format specifier raises\n`TypeError: unsupported format string passed to ...Array`.",
          "Fix_info": "Convert the JAX array to a Python scalar with `float()` (or `.item()`) before\nembedding it in an f-string.",
          "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\")"
          },
          {
          "Error_Code": "Dense / SelfAttention layers without explicit initialisers",
          "Error": "Flax uses Lecun-normal initialisation by default, whereas PyTorch’s\n`nn.Linear` draws from **U(-1/√fan_in , 1/√fan_in)**.  Starting from different\nweight statistics changes optimisation behaviour.",
          "Fix_info": "Create a helper `uniform_init = nn.initializers.uniform(scale=1.0 / jnp.sqrt(fan_in))`\nand pass it to every `kernel_init` and `bias_init` so JAX matches PyTorch’s initialisation.",
          "Fixed_Code": "uniform_init = nn.initializers.uniform(scale=1.0 / jnp.sqrt(fan_in))\n...\nn.Dense(out_dim, kernel_init=uniform_init, bias_init=uniform_init)"
          },
          {
          "Error_Code": "model.apply(params, x, train=train)   # called inside `mse_loss`",
          "Error": "With `train=True` the Self-Attention block expects a PRNG key for the\n\"dropout\" collection.  Because no key was supplied, Flax raises\n`InvalidRngError: ... needs PRNG for \"dropout\"`.",
          "Fix_info": "Pass a `dropout` PRNG key every time you call `model.apply` when\n`train=True` (or disable dropout by setting `dropout_rate=0` or\n`train=False`).  Thread the key through your JIT-compatible `train_step`.",
          "Fixed_Code": "# --- Inside mse_loss ------------------------------------\ndef mse_loss(p, x, y, train, rng):\n    preds = model.apply(p, x, train=train, rngs={\"dropout\": rng})\n    return jnp.mean((preds - y) ** 2)\n\n# --- JIT-friendly training step -----------------------------\n@jax.jit\ndef train_step(p, opt_state, x, y, rng):\n    rng, sub = jax.random.split(rng)\n    loss, grads = jax.value_and_grad(mse_loss)(p, x, y, True, sub)\n    updates, opt_state = opt.update(grads, opt_state)\n    p = optax.apply_updates(p, updates)\n    return p, opt_state, rng, loss"
          },
          {
          "Error_Code": "p = opt.apply_updates(p, updates)",
          "Error": "`opt` is an `optax.GradientTransformationExtraArgs` object; it does **not** have an `apply_updates` method.  `apply_updates` is a module-level helper in `optax`.",
          "Fix_info": "Replace the method call with the correct functional helper:",
          "Fixed_Code": "p = optax.apply_updates(p, updates)"
          }
        ]
        },
        {
        "Example_id": "h4",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "import jax\\nimport jax.numpy as jnp\\nfrom flax import linen as nn\\nimport optax\\nimport numpy as np\\nfrom functools import partial\\n\\n# ------------------------- helper: PyTorch-style init -------------------------\\ndef torch_uniform(fan_in):\\n    bound = 1.0 / jnp.sqrt(fan_in)\\n    return nn.initializers.uniform(scale=bound)\\n\\n# ------------------------------  Generator  -----------------------------------\\nclass Generator(nn.Module):\\n    latent_dim: int\\n    output_dim: int\\n    @nn.compact\\n    def __call__(self, z):\\n        x = nn.Dense(128, kernel_init=torch_uniform(self.latent_dim),\\n                            bias_init=torch_uniform(self.latent_dim))(z)\\n        x = nn.relu(x)\\n        x = nn.Dense(256, kernel_init=torch_uniform(128),\\n                            bias_init=torch_uniform(128))(x)\\n        x = nn.relu(x)\\n        x = nn.Dense(self.output_dim, kernel_init=torch_uniform(256),\\n                                   bias_init=torch_uniform(256))(x)\\n        return nn.tanh(x)\\n\\n# ----------------------------  Discriminator  ---------------------------------\\nclass Discriminator(nn.Module):\\n    input_dim: int\\n    @nn.compact\\n    def __call__(self, x):                      # returns **logits**\\n        x = nn.Dense(256, kernel_init=torch_uniform(self.input_dim),\\n                            bias_init=torch_uniform(self.input_dim))(x)\\n        x = nn.leaky_relu(x, 0.2)\\n        x = nn.Dense(128, kernel_init=torch_uniform(256),\\n                            bias_init=torch_uniform(256))(x)\\n        x = nn.leaky_relu(x, 0.2)\\n        return nn.Dense(1, kernel_init=torch_uniform(128),\\n                           bias_init=torch_uniform(128))(x)\\n\\n# -----------------------------  BCE-with-logits  ------------------------------\\ndef bce_logits(logits, targets):\\n    return jnp.mean(optax.sigmoid_binary_cross_entropy(logits, targets))\\n\\n# --------------------------------  training  ----------------------------------\\ndef main():\\n    key = jax.random.PRNGKey(42)\\n    num_samples, data_dim, latent_dim = 100, 1, 10\\n\\n    # real data U[-1,1]\\n    key, sub = jax.random.split(key)\\n    real_data = jax.random.uniform(sub, (num_samples, data_dim),\\n                                   minval=-1.0, maxval=1.0)\\n\\n    G = Generator(latent_dim, data_dim)\\n    D = Discriminator(data_dim)\\n\\n    # init params\\n    key, gk = jax.random.split(key)\\n    g_params = G.init(gk, jnp.ones((1, latent_dim)))\\n    key, dk = jax.random.split(key)\\n    d_params = D.init(dk, jnp.ones((1, data_dim)))\\n\\n    g_opt, d_opt = optax.adam(0.001), optax.adam(0.001)\\n    g_state, d_state = g_opt.init(g_params), d_opt.init(d_params)\\n\\n    # ------------------- discriminator update ---------------------------------\\n    @jax.jit\\n    def d_step(d_params, d_state, g_params, real, rng):\\n        rng, sub = jax.random.split(rng)\\n        z = jax.random.normal(sub, (real.shape[0], latent_dim))\\n        fake = jax.lax.stop_gradient(G.apply(g_params, z))\\n\\n        def loss_fn(p):\\n            real_logit = D.apply(p, real)\\n            fake_logit = D.apply(p, fake)\\n            return bce_logits(real_logit, jnp.ones_like(real_logit)) + \\\n                   bce_logits(fake_logit, jnp.zeros_like(fake_logit))\\n\\n        loss, grads = jax.value_and_grad(loss_fn)(d_params)\\n        updates, d_state = d_opt.update(grads, d_state)\\n        d_params = optax.apply_updates(d_params, updates)\\n        return d_params, d_state, loss, rng\\n\\n    # ------------------- generator update -------------------------------------\\n    @partial(jax.jit, static_argnums=3)\\n    def g_step(g_params, g_state, d_params, batch_size, rng):\\n        rng, sub = jax.random.split(rng)\\n        z = jax.random.normal(sub, (batch_size, latent_dim))\\n\\n        def loss_fn(p):\\n            fake = G.apply(p, z)\\n            logits = D.apply(d_params, fake)\\n            return bce_logits(logits, jnp.ones_like(logits))\\n\\n        loss, grads = jax.value_and_grad(loss_fn)(g_params)\\n        updates, g_state = g_opt.update(grads, g_state)\\n        g_params = optax.apply_updates(g_params, updates)\\n        return g_params, g_state, loss, rng\\n\\n    # training loop\\n    epochs, train_rng = 1000, jax.random.PRNGKey(1)\\n    for epoch in range(epochs):\\n        d_params, d_state, loss_D, train_rng = d_step(\\n            d_params, d_state, g_params, real_data, train_rng)\\n        g_params, g_state, loss_G, train_rng = g_step(\\n            g_params, g_state, d_params, num_samples, train_rng)\\n\\n        if (epoch + 1) % 100 == 0:\\n            print(f\\\"Epoch [{epoch + 1}/{epochs}] - \\\"\\n                  f\\\"Loss D: {float(loss_D):.4f}, Loss G: {float(loss_G):.4f}\\\")\\n\\n    # generate 5 samples\\n    key, sub = jax.random.split(key)\\n    z = jax.random.normal(sub, (5, latent_dim))\\n    samples = G.apply(g_params, z)\\n    print(\\\"Generated data:\\\", np.asarray(samples).tolist())\\n\\nif __name__ == __main__:\\n    main()",
        "Errors": [
          {
          "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}] - Loss D: {loss_D:.4f}, Loss G: {loss_G:.4f}\")",
          "Error": "`loss_D` and `loss_G` are `jax.Array` objects. Using `:.4f` on them raises `TypeError: unsupported format string passed to ...Array`.",
          "Fix_info": "Convert each value to a Python scalar with `float()` (or `.item()`) before formatting.",
          "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}] - Loss D: {float(loss_D):.4f}, Loss G: {float(loss_G):.4f}\")"
          },
          {
          "Error_Code": "Dense / SelfAttention layers created without explicit initialisers",
          "Error": "Flax defaults to LeCun-normal initialisation, whereas PyTorch’s `nn.Linear` draws **U(-1 / √fan_in , 1 / √fan_in)**. Different statistics break parity.",
          "Fix_info": "Provide a helper `torch_uniform(fan_in)` and pass it to *every* `Dense` layer’s `kernel_init` **and** `bias_init`.",
          "Fixed_Code": "def torch_uniform(fan_in):\n    bound = 1.0 / jnp.sqrt(fan_in)\n    return nn.initializers.uniform(scale=bound)\n...\nn.Dense(128, kernel_init=torch_uniform(latent_dim), bias_init=torch_uniform(latent_dim))(z)"
          },
          {
          "Error_Code": "z = jax.random.normal(sub, (batch_size, latent_dim))  # inside `g_step`",
          "Error": "`batch_size` is a *tracer* under `@jax.jit`, so the shape tuple contains a non-concrete value, triggering: `TypeError: Shapes must be 1D sequences of concrete integer values`.",
          "Fix_info": "Tell `@jax.jit` that `batch_size` is **static** (compile-time constant) by adding `static_argnums=3` to the decorator, or remove `batch_size` from the argument list.",
          "Fixed_Code": "@jax.jit(static_argnums=3)\ndef g_step(g_params, g_state, d_params, batch_size, rng):\n    rng, sub = jax.random.split(rng)\n    z = jax.random.normal(sub, (batch_size, latent_dim))\n    ..."
          },
          {
          "Error_Code": "@jax.jit(static_argnums=3)",
          "Error": "`jax.jit` expects the *function to be jitted* as its **first positional** argument. Calling it as `jax.jit(static_argnums=3)` immediately triggers `TypeError: jit() missing 1 required positional argument: 'fun'` because no function is supplied yet.",
          "Fix_info": "Wrap `jax.jit` with `functools.partial`, or use `jax.jit` with the function as the first positional argument. In decorator form, the idiomatic pattern is:\n```python\nfrom functools import partial\n@partial(jax.jit, static_argnums=3)\n```",
          "Fixed_Code": "from functools import partial\n@partial(jax.jit, static_argnums=3)\ndef g_step(g_params, g_state, d_params, batch_size, rng):\n    ...  # unchanged body"
          },
          {
          "Error_Code": "x = nn.Dense(1, ...)(x)\nreturn nn.sigmoid(x)",
          "Error": "The Discriminator applies a **sigmoid** before computing BCE. When outputs saturate, gradients vanish and training stalls. Manual BCE on probabilities is numerically unstable once `pred≈0/1`.",
          "Fix_info": "Remove the final sigmoid and use `optax.sigmoid_binary_cross_entropy` which applies sigmoid internally and is stable.",
          "Fixed_Code": "class Discriminator(nn.Module):\n    input_dim: int\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(256, kernel_init=torch_uniform(self.input_dim), bias_init=torch_uniform(self.input_dim))(x)\n        x = nn.leaky_relu(x, 0.2)\n        x = nn.Dense(128, kernel_init=torch_uniform(256), bias_init=torch_uniform(256))(x)\n        x = nn.leaky_relu(x, 0.2)\n        # No sigmoid here – return raw logits\n        return nn.Dense(1, kernel_init=torch_uniform(128), bias_init=torch_uniform(128))(x)"
          },
          {
          "Error_Code": "def bce(pred, target, eps=1e-7):\n    return -jnp.mean(target*log(pred+eps)+...)",
          "Error": "This manual BCE expects probabilities and becomes unstable when `pred≈0` or `1`.",
          "Fix_info": "Replace the handwritten BCE with `optax.sigmoid_binary_cross_entropy`, which takes logits and applies a stable sigmoid.",
          "Fixed_Code": "def bce_logits(logits, target):\n    return jnp.mean(\n        optax.sigmoid_binary_cross_entropy(logits, target)\n    )"
          },
          {
          "Error_Code": "print(f\"... Loss D: {loss_D:.4f}, Loss G: {loss_G:.4f}\")",
          "Error": "`loss_D` and `loss_G` are `jax.Array` objects; the format specifier `:.4f` raises `TypeError`.",
          "Fix_info": "Convert them to Python scalars with `float()` before formatting.",
          "Fixed_Code": "print(f\"... Loss D: {float(loss_D):.4f}, Loss G: {float(loss_G):.4f}\")"
          }
        ]
        },
        {
        "Example_id": "h5",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "",
        "Errors": [
          {
          "Error_Code": "initial_state = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,), self.hidden_dim)",
          "Error": "A **fixed PRNG key** means each batch starts from a different random (but identical across epochs) LSTM state, whereas the PyTorch version initialises all LSTM states with **zeros**.  Random initial states inject noise and hinder convergence.",
          "Fix_info": "Explicitly initialise the carry to zeros so it matches PyTorch’s `nn.LSTM(..., batch_first=True)` default.",
          "Fixed_Code": "initial_state = (\n        jnp.zeros((batch_size, self.hidden_dim), jnp.float32),  # cell state C\n        jnp.zeros((batch_size, self.hidden_dim), jnp.float32)   # hidden H\n    )"
          },
          {
          "Error_Code": "print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss_val:.4f}\")",
          "Error": "`loss_val` is a `jax.Array`; using the `:.4f` format specifier raises `TypeError: unsupported format string passed to ...Array`.",
          "Fix_info": "Convert the value to a Python scalar with `float()` (or `.item()`) before formatting.",
          "Fixed_Code": "print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {float(loss_val):.4f}\")"
          },
          {
          "Error_Code": "cell = nn.LSTMCell()                   # in LSTMLayer.__call__",
          "Error": "Flax’s `nn.LSTMCell` **requires** the keyword argument `features` (the hidden-state size).  Omitting it raises `TypeError: __init__() missing 1 required positional argument: 'features'`.",
          "Fix_info": "Instantiate the cell with `features=self.hidden_dim` so its hidden-size matches the PyTorch layer.",
          "Fixed_Code": "cell = nn.LSTMCell(features=self.hidden_dim)"
          },
          {
          "Error_Code": "lstm_cell = nn.LSTMCell(name=f\"decoder_lstm_cell_{i}\")   # in Decoder.__call__",
          "Error": "The same mistake appears in the Decoder: `features` is missing, causing the identical `TypeError` when the Decoder is initialised.",
          "Fix_info": "Pass the hidden dimension explicitly, preserving the `name=` argument.",
          "Fixed_Code": "lstm_cell = nn.LSTMCell(features=self.hidden_dim, name=f\"decoder_lstm_cell_{i}\")"
          },
          {
          "Error_Code": "final_state, outputs = nn.scan(\n        step, variable_broadcast=\"params\",\n        split_rngs={\"params\": False},\n        in_axes=1, out_axes=1)(init_state, x)",
          "Error": "Calling `nn.scan(...)(init_state, x)` **immediately** creates a transformed function and applies it in one step.  Inside another `nn.Module` the first positional argument expected is `self`, but it instead receives `init_state`, leading to `AttributeError: 'tuple' object has no attribute '_state'`.",
          "Fix_info": "Create the scanned function **first**, assign it to a local variable, and then call it with `(init_state, x)`.  This passes the correct `self` object internally and eliminates the AttributeError.",
          "Fixed_Code": "scan_fn = nn.scan(\n            step,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False},\n            in_axes=1,\n            out_axes=1,\n        )\nfinal_state, outputs = scan_fn(init_state, x)"
          },
          {
          "Error_Code": "cell = nn.LSTMCell(features=self.hidden_dim)\n\ndef step(carry, x_t):\n    carry, y = cell(carry, x_t)",
          "Error": "`nn.LSTMCell` is an **`nn.Module`**.  Closing over a *live* module instance (`cell`) inside `step` confuses the lifting logic used by `nn.scan`.  When the transformation runs it expects `self` but receives the tuple `init_state`, producing `AttributeError: 'tuple' object has no attribute '_state'`.",
          "Fix_info": "Instantiate `nn.LSTMCell` **inside** the scanned `step` each time it is called.  Because `nn.scan` broadcasts the `'params'` collection the parameters remain shared across time steps.",
          "Fixed_Code": "def step(carry, x_t):\n    # instantiate inside the loop so lifting works correctly\n    carry, y = nn.LSTMCell(features=self.hidden_dim)(carry, x_t)\n    return carry, y"
          },
          {
          "Error_Code": "final_state, outputs = scan_fn(init_state, x)",
          "Error": "`nn.scan` returns a **function that first creates a lifted sub-module** and then expects to be called *again* with the carry and inputs.  Calling it only once mistakes `init_state` for `self`, so Flax tries to access `_state` on a tuple and raises `AttributeError: 'tuple' object has no attribute '_state'`.",
          "Fix_info": "Invoke the object returned by `scan_fn` in **two steps**: 1) instantiate the lifted module, 2) call it with `(init_state, x)`.",
          "Fixed_Code": "lifted_scan = scan_fn()                  # create sub-module\nfinal_state, outputs = lifted_scan(init_state, x)"
          },
          {
          "Error_Code": "lifted_scan = scan_fn()  # instantiate sub-module\nfinal_state, outputs = lifted_scan(init_state, x)",
          "Error": "`nn.scan` already returns a callable that expects `(carry, x)` – calling it with no arguments first makes Flax treat the result as a method needing `self`, triggering `TypeError: wrapped_fn() missing 1 required positional argument: 'self'`.",
          "Fix_info": "Call the function returned by `nn.scan` **directly** with `(init_state, x)`— no extra empty call is required.",
          "Fixed_Code": "final_state, outputs = scan_fn(init_state, x)"
          },
          {
          "Error_Code": "def __call__(self, x, init_state):\n    def step(carry, x_t):\n        carry, y = nn.LSTMCell(features=self.hidden_dim)(carry, x_t)\n        return carry, y",
          "Error": "Defining a new `LSTMCell` inside `step` recreates parameters each time and fails to share the same cell across steps.",
          "Fix_info": "Bind a single `LSTMCell` instance outside of `step` (so its parameters are shared), then call that one inside `step`.",
          "Fixed_Code": "class LSTMLayer(nn.Module):\n    hidden_dim: int\n\n    @nn.compact\n    def __call__(self, x, init_state):\n        # Define one shared cell\n        cell = nn.LSTMCell(self.hidden_dim)\n\n        def step(carry, x_t):\n            new_carry, y = cell(carry, x_t)\n            return new_carry, y\n\n        scan_fn = nn.scan(\n            step,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False},\n            in_axes=1,\n            out_axes=1,\n        )\n        final_state, outputs = scan_fn(init_state, x)\n        return outputs, final_state"
          },
          {
          "Error_Code": "init_state = (\n    jnp.zeros((batch, self.hidden_dim), jnp.float32),\n    jnp.zeros((batch, self.hidden_dim), jnp.float32),\n)",
          "Error": "Manually zeroing out states instead of using the cell’s own `initialize_carry` misses the correct shape, dtype, and ordering.",
          "Fix_info": "Call `cell.initialize_carry((batch,), hidden_dim)` on the same `cell` used in `LSTMLayer` to guarantee correct initialization.",
          "Fixed_Code": "cell = nn.LSTMCell(self.hidden_dim)\ninit_state = cell.initialize_carry(\n    (batch,),\n    self.hidden_dim\n)"
          },
          {
          "Error_Code": "init_state = cell.initialize_carry(\n    self.make_rng('params'),  # Or pass a PRNGKey into __call__\n    (batch,),\n    self.hidden_dim\n)",
          "Error": "Passing an RNG key to `initialize_carry` is incorrect: Flax’s `linen.LSTMCell.initialize_carry` is an instance method that takes only `(batch_dims, hidden_size)`.",
          "Fix_info": "Remove the RNG argument so you call:\n```python\ninit_state = cell.initialize_carry((batch,), self.hidden_dim)\n```",
          "Fixed_Code": "init_state = cell.initialize_carry((batch,), self.hidden_dim)"
          }
        ]
        },
        {
        "Example_id": "m2",
        "Input_Code": "",
        "LLM_weak_output":"",
        "LLM_fix_output": "#Strong LLM\\nimport os\\nimport urllib.request\\nimport tarfile\\nimport pickle\\nimport numpy as np\\nimport jax\\nimport jax.numpy as jnp\\nimport flax.linen as nn\\nimport optax\\n\\ndef download_and_extract_cifar10(data_dir='./cifar-10-batches-py'):\\n    if not os.path.exists(data_dir):\\n        url = \\\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\\\"\\n        filename = \\\"cifar-10-python.tar.gz\\\"\\n        print(\\\"Downloading CIFAR-10 dataset...\\\")\\n        urllib.request.urlretrieve(url, filename)\\n        print(\\\"Extracting CIFAR-10 dataset...\\\")\\n        with tarfile.open(filename, 'r:gz') as tar:\\n            tar.extractall()\\n        os.remove(filename)\\n\\ndef load_batch(file):\\n    with open(file, 'rb') as f:\\n        data_dict = pickle.load(f, encoding='bytes')\\n        data = data_dict[b'data']  \\n        labels = data_dict[b'labels']\\n        data = data.reshape(-1, 3, 32, 32)\\n        data = data.transpose(0, 2, 3, 1)\\n        return data, np.array(labels)\\n\\ndef load_cifar10_train(data_dir='./cifar-10-batches-py'):\\n    download_and_extract_cifar10(data_dir)\\n    xs = []\\n    ys = []\\n    for i in range(1, 6):\\n        file = os.path.join(data_dir, f\\\"data_batch_{i}\\\")\\n        data, labels = load_batch(file)\\n        xs.append(data)\\n        ys.append(labels)\\n    x_train = np.concatenate(xs)\\n    y_train = np.concatenate(ys)\\n    return x_train, y_train\\n\\ndef load_cifar10_test(data_dir='./cifar-10-batches-py'):\\n    download_and_extract_cifar10(data_dir)\\n    file = os.path.join(data_dir, \\\"test_batch\\\")\\n    return load_batch(file)\\n\\ndef normalize_images(images):\\n    images = images.astype(np.float32) / 255.0\\n    images = (images - 0.5) / 0.5\\n    return images\\n\\nx_train, y_train = load_cifar10_train()\\nx_test, y_test   = load_cifar10_test()\\nx_train = normalize_images(x_train)\\nx_test  = normalize_images(x_test)\\n\\ndef get_batches(x, y, batch_size=64, shuffle=True):\\n    indices = np.arange(x.shape[0])\\n    if shuffle:\\n        np.random.shuffle(indices)\\n    for start_idx in range(0, x.shape[0] - batch_size + 1, batch_size):\\n        excerpt = indices[start_idx:start_idx+batch_size]\\n        yield x[excerpt], y[excerpt]\\n\\nclass CNNModel(nn.Module):\\n    @nn.compact\\n    def __call__(self, x):\\n        x = nn.Conv(features=32, kernel_size=(3,3), strides=(1,1), padding='SAME')(x)\\n        x = nn.relu(x)\\n        x = nn.Conv(features=64, kernel_size=(3,3), strides=(1,1), padding='SAME')(x)\\n        x = nn.relu(x)\\n        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')\\n        x = x.reshape((x.shape[0], -1))\\n        x = nn.Dense(features=128)(x)\\n        x = nn.relu(x)\\n        x = nn.Dense(features=10)(x)\\n        return x\\n\\nbatch_size = 64\\nrng = jax.random.PRNGKey(0)\\nmodel = CNNModel()\\ndummy_input = jnp.ones((batch_size, 32, 32, 3))\\nparams = model.init(rng, dummy_input)['params']\\noptimizer = optax.adam(learning_rate=0.001)\\nopt_state = optimizer.init(params)\\n\\ndef compute_loss(params, images, labels):\\n    logits = model.apply({'params': params}, images)\\n    one_hot = jax.nn.one_hot(labels, num_classes=10)\\n    return optax.softmax_cross_entropy(logits, one_hot).mean()\\n\\n@jax.jit\\ndef train_step(params, opt_state, images, labels):\\n    loss, grads = jax.value_and_grad(compute_loss)(params, images, labels)\\n    updates, opt_state = optimizer.update(grads, opt_state)\\n    params = optax.apply_updates(params, updates)\\n    return params, opt_state, loss\\n\\nepochs = 10\\nfor epoch in range(epochs):\\n    for images, labels in get_batches(x_train, y_train, batch_size=batch_size, shuffle=True):\\n        images = jnp.array(images)\\n        labels = jnp.array(labels)\\n        params, opt_state, loss = train_step(params, opt_state, images, labels)\\n    print(f\\\"Epoch [{epoch+1}/{epochs}], Loss: {loss:.4f}\\\")\\n\\ntotal, correct = 0, 0\\nfor images, labels in get_batches(x_test, y_test, batch_size=batch_size, shuffle=False):\\n    images = jnp.array(images)\\n    labels = jnp.array(labels)\\n    logits = model.apply({'params': params}, images)\\n    preds  = jnp.argmax(logits, axis=-1)\\n    correct += int(jnp.sum(preds == labels))\\n    total   += images.shape[0]\\naccuracy = 100 * (correct / total)\\nprint(f\\\"Test Accuracy: {accuracy:.2f}%\\\")",
        "Errors": []
        },
      {
        "Example_id": "h6",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.quantization import quantize_dynamic\n\n# Define a simple Language Model (e.g., an LSTM-based model)\nclass LanguageModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n        super(LanguageModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n        output = self.fc(lstm_out[:, -1, :])  # Use the last hidden state for prediction\n        return self.softmax(output)\n\n# Create synthetic training data\ntorch.manual_seed(42)\nvocab_size = 50\nseq_length = 10\nbatch_size = 32\nX_train = torch.randint(0, vocab_size, (batch_size, seq_length))  # Random integer input\ny_train = torch.randint(0, vocab_size, (batch_size,))  # Random target words\n\n# Initialize the model, loss function, and optimizer\nembed_size = 64\nhidden_size = 128\nnum_layers = 2\nmodel = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    output = model(X_train)\n    loss = criterion(output, y_train)\n    loss.backward()\n    optimizer.step()\n\n    # Log progress every epoch\n    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss.item():.4f}\")\n\n# Now, we will quantize the model dynamically to reduce its size and improve inference speed\n# Quantization: Apply dynamic quantization to the language model\nquantized_model = quantize_dynamic(model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n\n# Save the quantized model\ntorch.save(quantized_model.state_dict(), \"quantized_language_model.pth\")\n\n# Load the quantized model and test it\nquantized_model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n\n# Apply dynamic quantization on the model after defining it\nquantized_model = quantize_dynamic(quantized_model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n\n# quantized_model.load_state_dict(torch.load(\"quantized_language_model.pth\"))\nquantized_model.eval()\ntest_input = torch.randint(0, vocab_size, (1, seq_length))\nwith torch.no_grad():\n    prediction = quantized_model(test_input)\n    print(f\"Prediction for input {test_input.tolist()}: {prediction.argmax(dim=1).item()}\")",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.core import freeze, unfreeze\nfrom flax.training import train_state\nimport optax\nimport numpy as np\n\ndef one_hot(labels, num_classes):\n    return jnp.eye(num_classes, dtype=jnp.float32)[labels]\n\ndef cross_entropy_with_integer_labels(logits, labels):\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\nclass LanguageModel(nn.Module):\n    vocab_size: int\n    embed_size: int\n    hidden_size: int\n    num_layers: int = 1\n\n    @nn.compact\n    def __call__(self, x, *, train: bool = True):\n        emb = nn.Embed(self.vocab_size, self.embed_size)(x)\n        batch_size = x.shape[0]\n        carry = nn.LSTMCell.initialize_carry(\n            jax.random.PRNGKey(0),\n            (batch_size,), self.embed_size\n        )\n        lstm_cell = nn.scan(\n            nn.LSTMCell,\n            variable_broadcast='params',\n            split_rngs={'params': False},\n            in_axes=1, out_axes=1, name='lstm'\n        )(features=self.hidden_size)\n        carry, lstm_out = lstm_cell(carry, emb)\n        last_h = lstm_out[:, -1, :]\n        logits = nn.Dense(self.vocab_size)(last_h)\n        probs = nn.softmax(logits)\n        return logits, probs\n\ndef quantize_params(params):\n    qparams = {}\n    for k, v in params.items():\n        if isinstance(v, dict):\n            qparams[k] = quantize_params(v)\n        else:\n            scale = jnp.max(jnp.abs(v)) / 127.\n            qparams[k] = {\n                'int8': jnp.round(v / scale).astype(jnp.int8),\n                'scale': scale.astype(jnp.float32)\n            }\n    return qparams\n\ndef dequantize_params(qparams):\n    params = {}\n    for k, v in qparams.items():\n        if isinstance(v, dict) and 'int8' not in v:\n            params[k] = dequantize_params(v)\n        else:\n            params[k] = v['int8'].astype(jnp.float32) * v['scale']\n    return params\n\nseed = 42\nrng = jax.random.PRNGKey(seed)\n\nvocab_size = 50\nseq_length = 10\nbatch_size = 32\nembed_size = 64\nhidden_size = 128\nnum_layers = 2\nepochs = 5\nlr = 1e-3\n\nnp.random.seed(seed)\nX_train = np.random.randint(0, vocab_size, (batch_size, seq_length))\ny_train = np.random.randint(0, vocab_size, (batch_size,))\n\nmodel = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n\ndef create_train_state(rng):\n    params = model.init(rng, jnp.ones((1, seq_length), dtype=jnp.int32))['params']\n    optimiser = optax.adam(lr)\n    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimiser)\n\nstate = create_train_state(rng)\n\n@jax.jit\ndef train_step(state, batch_x, batch_y):\n    def loss_fn(params):\n        logits, _ = state.apply_fn({'params': params}, batch_x, train=True)\n        loss = cross_entropy_with_integer_labels(logits, batch_y)\n        return loss\n    grad_fn = jax.value_and_grad(loss_fn)\n    loss, grads = grad_fn(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state, loss\n\nfor epoch in range(epochs):\n    state, loss = train_step(state, jnp.array(X_train, dtype=jnp.int32), jnp.array(y_train, dtype=jnp.int32))\n    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item():.4f}\")\n\nquantised_params = quantize_params(state.params)\n\nimport orbax.checkpoint as ocp\ncheckpointer = ocp.PyTreeCheckpointer()\nsave_path = \"quantised_language_model\"\ncheckpointer.save(save_path, quantised_params)\n\nrestored_qparams = checkpointer.restore(save_path)\nrestored_params = freeze(dequantize_params(restored_qparams))\n\nmodel_apply = jax.jit(model.apply)\n\ntest_input = np.random.randint(0, vocab_size, (1, seq_length))\n_, probs = model_apply({'params': restored_params}, jnp.array(test_input, dtype=jnp.int32), train=False)\n\nprediction = int(jnp.argmax(probs, axis=1)[0])\nprint(f\"Prediction for input {test_input.tolist()}: {prediction}\")",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.core import freeze, unfreeze\nfrom flax.training import train_state\nimport optax\nimport numpy as np\nimport os\n\ndef one_hot(labels, num_classes):\n    return jnp.eye(num_classes, dtype=jnp.float32)[labels]\n\ndef cross_entropy_with_integer_labels(logits, labels):\n    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n\nclass LanguageModel(nn.Module):\n    vocab_size: int\n    embed_size: int\n    hidden_size: int\n    num_layers: int = 1\n    @nn.compact\n    def __call__(self, x, *, train: bool = True):\n        emb = nn.Embed(self.vocab_size, self.embed_size)(x)\n        batch_size = x.shape[0]\n        rng = jax.random.PRNGKey(0)\n        key1, key2 = jax.random.split(rng)\n        carry = (\n            jax.random.normal(key1, (batch_size, self.hidden_size)),\n            jax.random.normal(key2, (batch_size, self.hidden_size))\n        )\n        lstm_cell = nn.scan(\n            nn.LSTMCell,\n            variable_broadcast='params',\n            split_rngs={'params': False},\n            in_axes=1, out_axes=1, length=x.shape[1])(features=self.hidden_size, name='lstm')\n        carry, lstm_out = lstm_cell(carry, emb)\n        last_h = lstm_out[:, -1, :]\n        logits = nn.Dense(self.vocab_size)(last_h)\n        probs = nn.softmax(logits)\n        return logits, probs\n\ndef quantize_params(params):\n    qparams = {}\n    for k, v in params.items():\n        if isinstance(v, dict):\n            qparams[k] = quantize_params(v)\n        else:\n            scale = jnp.max(jnp.abs(v)) / 127.\n            qparams[k] = {\n                'int8': jnp.round(v / scale).astype(jnp.int8),\n                'scale': scale.astype(jnp.float32)\n            }\n    return qparams\n\ndef dequantize_params(qparams):\n    params = {}\n    for k, v in qparams.items():\n        if isinstance(v, dict) and 'int8' not in v:\n            params[k] = dequantize_params(v)\n        else:\n            params[k] = v['int8'].astype(jnp.float32) * v['scale']\n    return params\n\nseed = 42\nrng = jax.random.PRNGKey(seed)\n\nvocab_size = 50\nseq_length = 10\nbatch_size = 32\nembed_size = 64\nhidden_size = 128\nnum_layers = 2\nepochs = 5\nlr = 1e-3\n\nnp.random.seed(seed)\nX_train = np.random.randint(0, vocab_size, (batch_size, seq_length))\ny_train = np.random.randint(0, vocab_size, (batch_size,))\n\nmodel = LanguageModel(\n    vocab_size=vocab_size,\n    embed_size=embed_size,\n    hidden_size=hidden_size,\n    num_layers=num_layers,\n)\n\ndef create_train_state(rng):\n    params = model.init(rng, jnp.ones((1, seq_length), dtype=jnp.int32))['params']\n    optimiser = optax.adam(lr)\n    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimiser)\n\nstate = create_train_state(rng)\n\n@jax.jit\ndef train_step(state, batch_x, batch_y):\n    def loss_fn(params):\n        logits, _ = state.apply_fn({'params': params}, batch_x, train=True)\n        loss = cross_entropy_with_integer_labels(logits, batch_y)\n        return loss\n    grad_fn = jax.value_and_grad(loss_fn)\n    loss, grads = grad_fn(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state, loss\n\nfor epoch in range(epochs):\n    state, loss = train_step(state, jnp.array(X_train, dtype=jnp.int32), jnp.array(y_train, dtype=jnp.int32))\n    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item():.4f}\")\n\nquantised_params = quantize_params(state.params)\n\nimport orbax.checkpoint as ocp\ncheckpointer = ocp.PyTreeCheckpointer()\nsave_path = os.path.abspath(\"quantised_language_model\")\nrestored_qparams = checkpointer.restore(save_path)\nrestored_params = freeze(dequantize_params(restored_qparams))\n\nmodel_apply = jax.jit(model.apply)\n\ntest_input = np.random.randint(0, vocab_size, (1, seq_length))\n_, probs = model_apply({'params': restored_params}, jnp.array(test_input, dtype=jnp.int32), train=False)\n\nprediction = int(jnp.argmax(probs, axis=1)[0])\nprint(f\"Prediction for input {test_input.tolist()}: {prediction}\")",
        "Errors": [
          {
            "Error_Code": "nn.LSTMCell.initialize_carry(self.make_rng('lstm'), (batch_size,), self.hidden_size)",
            "Error": "TypeError: 'int' object is not subscriptable",
            "Fix_info": "initialize_carry expects input_shape as a tuple of batch dimensions. Using (batch_size,) directly caused Flax to misinterpret batch_size as an integer, leading to the error. batch_dims ensures the correct tuple format.",
            "Fixed_Code": "carry = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size, self.embed_size))"
          },
          {
            "Error_Code": "model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)",
            "Error": "__init__() takes 1 positional arguments but 5 were given",
            "Fix_info": "flax.linen.Module classes are keyword-only dataclasses, so every field has to be passed by name.",
            "Fixed_Code": "model = LanguageModel(vocab_size=vocab_size, embed_size=embed_size, hidden_size=hidden_size, num_layers=num_layers)"
          },
          {
            "Error_Code": "state = create_train_state(rng) lstm_cell = nn.scan(nn.LSTMCell, variable_broadcast='params', split_rngs={'params': False}, in_axes=1, out_axes=1, length=x.shape[1])(name='lstm')",
            "Error": "TypeError: ScanLSTMCell.__init__() missing 1 required positional argument: 'features'",
            "Fix_info": "Add features=self.hidden_size to nn.LSTMCell inside the nn.scan call, using the hidden_size attribute to define the number of hidden units.",
            "Fixed_Code": ""
          },
          {
            "Error_Code": "lstm_cell = nn.scan(nn.LSTMCell(features=self.hidden_size), variable_broadcast='params',",
            "Error": "TransformTargetError: Linen transformations must be applied to Modules classes or functions taking a Module instance as the first argument.",
            "Fix_info": "Change nn.scan to use nn.LSTMCell class instead of instance, with features=self.hidden_size.",
            "Fixed_Code": "lstm_cell = nn.scan(nn.LSTMCell, variable_broadcast='params', split_rngs={'params': False}, in_axes=1, out_axes=1, length=x.shape[1])(features=self.hidden_size, name='lstm')"
          },
          {
            "Error_Code": "save_path = 'quantised_language_model' checkpointer.save(save_path, quantised_params)",
            "Error": "ValueError: Checkpoint path should be absolute. Got quantised_language_model.orbax-checkpoint-tmp-0",
            "Fix_info": "Convert the relative path to an absolute path (probably not recommended).",
            "Fixed_Code": "save_path = os.path.abspath('quantised_language_model') checkpointer.save(save_path, quantised_params)"
          }
        ]
      },
      {
        "Example_id": "m3",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n\ndef train_test_loop(model, train_loader, test_loader, epochs=10):\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    for epoch in range(epochs):\n        for image, label in train_loader:\n            pred = model(image)\n            loss = criterion(pred, label)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()    \n        print(f\"Training loss at epoch {epoch} = {loss.item()}\")\n    \n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for image_test, label_test in test_loader:\n            pred_test = model(image_test)\n            _, pred_test_vals = torch.max(pred_test, dim=1)\n            total += label_test.size(0)\n            correct += (pred_test_vals == label_test).sum().item()\n    print(f\"Test Accuracy = {(correct * 100)/total}\")\n\nclass VanillaCNNModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(64*16*16, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \ndef config_init(init_type=\"kaiming\"):\n    \n    def kaiming_init(m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.Linear):\n            nn.init.kaiming_normal_(m.weight)\n            nn.init.constant_(m.bias, 0)\n            \n            \n    def xavier_init(m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n                \n    def zeros_init(m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.zeros_(m.weight)\n            nn.init.zeros_(m.bias)\n            \n    def random_init(m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.normal_(m.weight)\n            nn.init.normal_(m.bias)\n    \n\n    initializer_dict = {\"kaiming\": kaiming_init,\n                        \"xavier\": xavier_init,\n                        \"zeros\": zeros_init,\n                        \"random\": random_init}\n    \n    return initializer_dict.get(init_type)\n\nfor name, model in zip([\"Vanilla\", \"Kaiming\", \"Xavier\", \"Zeros\", \"Random\"], [VanillaCNNModel(),\n              VanillaCNNModel().apply(config_init(\"kaiming\")),\n              VanillaCNNModel().apply(config_init(\"xavier\")),\n              VanillaCNNModel().apply(config_init(\"zeros\")),\n              VanillaCNNModel().apply(config_init(\"random\"))\n              ]):\n    print(f\"_________{name}_______________________\")\n    train_test_loop(model, train_loader, test_loader)",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.training import train_state\nimport optax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\n\n# Define the CNN model with configurable initializers.\nclass CNNModel(nn.Module):\n    conv_kernel_init: callable = nn.initializers.lecun_normal()\n    conv_bias_init: callable = nn.initializers.zeros\n    dense_kernel_init: callable = nn.initializers.lecun_normal()\n    dense_bias_init: callable = nn.initializers.zeros\n\n    @nn.compact\n    def __call__(self, x):\n        # First convolution layer: 3 channels -> 32 filters.\n        x = nn.Conv(features=32, kernel_size=(3, 3), strides=(1, 1), padding='SAME',\n                    kernel_init=self.conv_kernel_init, bias_init=self.conv_bias_init)(x)\n        x = nn.relu(x)\n        # Second convolution layer: 32 -> 64 filters.\n        x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME',\n                    kernel_init=self.conv_kernel_init, bias_init=self.conv_bias_init)(x)\n        x = nn.relu(x)\n        # Max pooling: 2x2 window, stride 2.\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n        # Flatten.\n        x = x.reshape((x.shape[0], -1))\n        # Fully connected layer with 128 units.\n        x = nn.Dense(features=128, kernel_init=self.dense_kernel_init, bias_init=self.dense_bias_init)(x)\n        x = nn.relu(x)\n        # Final fully connected layer with 10 outputs.\n        x = nn.Dense(features=10, kernel_init=self.dense_kernel_init, bias_init=self.dense_bias_init)(x)\n        return x\n\n# Create a training state with the model and an Adam optimizer.\ndef create_train_state(rng, model, learning_rate=0.001, batch_size=32):\n    dummy_input = jnp.ones([batch_size, 32, 32, 3], jnp.float32)\n    params = model.init(rng, dummy_input)\n    tx = optax.adam(learning_rate)\n    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n# Define a jitted training step.\n@jax.jit\ndef train_step(state, images, labels):\n    def loss_fn(params):\n        logits = state.apply_fn(params, images)\n        onehot = jax.nn.one_hot(labels, 10)\n        loss = optax.softmax_cross_entropy(logits, onehot).mean()\n        return loss\n    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\n# Evaluate the model on the test dataset.\ndef evaluate_model(state, test_ds):\n    correct = 0\n    total = 0\n    for images, labels in tfds.as_numpy(test_ds):\n        logits = state.apply_fn(state.params, images)\n        predictions = jnp.argmax(logits, axis=-1)\n        correct += np.sum(np.array(predictions) == np.array(labels))\n        total += len(labels)\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy = {accuracy:.2f}%\")\n    return accuracy\n\n# Preprocessing: convert images to float and normalize to [-1, 1]\ndef preprocess(image, label):\n    image = tf.cast(image, tf.float32) / 255.0  # Scale to [0, 1]\n    image = (image - 0.5) / 0.5                 # Normalize to [-1, 1]\n    return image, label\n\ndef main():\n    batch_size = 32\n    num_epochs = 10\n    learning_rate = 0.001\n    rng = jax.random.PRNGKey(0)\n\n    # Load CIFAR-10 training dataset in supervised mode.\n    train_ds = tfds.load('cifar10', split='train', as_supervised=True, download=True)\n    train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n    train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    # Load CIFAR-10 test dataset in supervised mode.\n    test_ds = tfds.load('cifar10', split='test', as_supervised=True, download=True)\n    test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    # Define the initializer configurations.\n    initializer_configs = {\n        \"Vanilla\": {  # Use Flax defaults.\n            \"conv_kernel_init\": nn.initializers.lecun_normal(),\n            \"dense_kernel_init\": nn.initializers.lecun_normal(),\n        },\n        \"Kaiming\": {\n            \"conv_kernel_init\": nn.initializers.kaiming_normal(),\n            \"dense_kernel_init\": nn.initializers.kaiming_normal(),\n        },\n        \"Xavier\": {\n            \"conv_kernel_init\": nn.initializers.xavier_normal(),\n            \"dense_kernel_init\": nn.initializers.xavier_normal(),\n        },\n        \"Zeros\": {\n            \"conv_kernel_init\": nn.initializers.zeros,\n            \"dense_kernel_init\": nn.initializers.zeros,\n        },\n        \"Random\": {\n            \"conv_kernel_init\": nn.initializers.normal(stddev=1.0),\n            \"dense_kernel_init\": nn.initializers.normal(stddev=1.0),\n        }\n    }\n\n    # Loop over the initialization schemes.\n    for name, init_conf in initializer_configs.items():\n        print(f\"_________{name}_______________________\")\n        model = CNNModel(conv_kernel_init=init_conf[\"conv_kernel_init\"],\n                         dense_kernel_init=init_conf[\"dense_kernel_init\"])\n        state = create_train_state(rng, model, learning_rate, batch_size)\n\n        # Training loop.\n        for epoch in range(num_epochs):\n            for images, labels in tfds.as_numpy(train_ds):\n                state, loss = train_step(state, images, labels)\n            print(f\"Training loss at epoch {epoch} = {loss:.4f}\")\n\n        # Evaluate the model on the test set.\n        evaluate_model(state, test_ds)\n\nif __name__ == \"__main__\":\n    main()",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.training import train_state\nimport optax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\n\n# Define the CNN model with configurable initializers.\nclass CNNModel(nn.Module):\n    conv_kernel_init: callable = nn.initializers.lecun_normal()\n    conv_bias_init: callable = nn.initializers.zeros\n    dense_kernel_init: callable = nn.initializers.lecun_normal()\n    dense_bias_init: callable = nn.initializers.zeros\n\n    @nn.compact\n    def __call__(self, x):\n        # First convolution layer: 3 channels -> 32 filters.\n        x = nn.Conv(features=32, kernel_size=(3, 3), strides=(1, 1), padding='SAME',\n                    kernel_init=self.conv_kernel_init, bias_init=self.conv_bias_init)(x)\n        x = nn.relu(x)\n        # Second convolution layer: 32 -> 64 filters.\n        x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME',\n                    kernel_init=self.conv_kernel_init, bias_init=self.conv_bias_init)(x)\n        x = nn.relu(x)\n        # Max pooling: 2x2 window, stride 2.\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n        # Flatten.\n        x = x.reshape((x.shape[0], -1))\n        # Fully connected layer with 128 units.\n        x = nn.Dense(features=128, kernel_init=self.dense_kernel_init, bias_init=self.dense_bias_init)(x)\n        x = nn.relu(x)\n        # Final fully connected layer with 10 outputs.\n        x = nn.Dense(features=10, kernel_init=self.dense_kernel_init, bias_init=self.dense_bias_init)(x)\n        return x\n\n# Create a training state with the model and an Adam optimizer.\ndef create_train_state(rng, model, learning_rate=0.001, batch_size=32):\n    dummy_input = jnp.ones([batch_size, 32, 32, 3], jnp.float32)\n    params = model.init(rng, dummy_input)\n    tx = optax.adam(learning_rate)\n    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n# Define a jitted training step.\n@jax.jit\ndef train_step(state, images, labels):\n    def loss_fn(params):\n        logits = state.apply_fn(params, images)\n        onehot = jax.nn.one_hot(labels, 10)\n        loss = optax.softmax_cross_entropy(logits, onehot).mean()\n        return loss\n    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\n# Evaluate the model on the test dataset.\ndef evaluate_model(state, test_ds):\n    correct = 0\n    total = 0\n    for images, labels in tfds.as_numpy(test_ds):\n        logits = state.apply_fn(state.params, images)\n        predictions = jnp.argmax(logits, axis=-1)\n        correct += np.sum(np.array(predictions) == np.array(labels))\n        total += len(labels)\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy = {accuracy:.2f}%\")\n    return accuracy\n\n# Preprocessing: convert images to float and normalize to [-1, 1]\ndef preprocess(image, label):\n    image = tf.cast(image, tf.float32) / 255.0  # Scale to [0, 1]\n    image = (image - 0.5) / 0.5                 # Normalize to [-1, 1]\n    return image, label\n\ndef main():\n    batch_size = 32\n    num_epochs = 10\n    learning_rate = 0.001\n    rng = jax.random.PRNGKey(0)\n\n    # Load CIFAR-10 training dataset in supervised mode.\n    train_ds = tfds.load('cifar10', split='train', as_supervised=True, download=True)\n    train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n    train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    # Load CIFAR-10 test dataset in supervised mode.\n    test_ds = tfds.load('cifar10', split='test', as_supervised=True, download=True)\n    test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    # Define the initializer configurations.\n    initializer_configs = {\n        \"Vanilla\": {  # Use Flax defaults.\n            \"conv_kernel_init\": nn.initializers.lecun_normal(),\n            \"dense_kernel_init\": nn.initializers.lecun_normal(),\n        },\n        \"Kaiming\": {\n            \"conv_kernel_init\": nn.initializers.kaiming_normal(),\n            \"dense_kernel_init\": nn.initializers.kaiming_normal(),\n        },\n        \"Xavier\": {\n            \"conv_kernel_init\": nn.initializers.xavier_normal(),\n            \"dense_kernel_init\": nn.initializers.xavier_normal(),\n        },\n        \"Zeros\": {\n            \"conv_kernel_init\": nn.initializers.zeros,\n            \"dense_kernel_init\": nn.initializers.zeros,\n        },\n        \"Random\": {\n            \"conv_kernel_init\": nn.initializers.normal(stddev=1.0),\n            \"dense_kernel_init\": nn.initializers.normal(stddev=1.0),\n        }\n    }\n\n    # Loop over the initialization schemes.\n    for name, init_conf in initializer_configs.items():\n        print(f\"_________{name}_______________________\")\n        model = CNNModel(conv_kernel_init=init_conf[\"conv_kernel_init\"],\n                         dense_kernel_init=init_conf[\"dense_kernel_init\"])\n        state = create_train_state(rng, model, learning_rate, batch_size)\n\n        # Training loop.\n        for epoch in range(num_epochs):\n            for images, labels in tfds.as_numpy(train_ds):\n                state, loss = train_step(state, images, labels)\n            print(f\"Training loss at epoch {epoch} = {loss:.4f}\")\n\n        # Evaluate the model on the test set.\n        evaluate_model(state, test_ds)\n\nif __name__ == \"__main__\":\n    main()",
        "Errors":[]

      },
      {
        "Example_id": "m4",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Generate synthetic CT-scan data (batches, slices, RGB) and associated segmentation masks\ntorch.manual_seed(42)\nbatch = 100\nnum_slices = 10\nchannels = 3\nwidth = 256\nheight = 256\n\nct_images = torch.randn(size=(batch, num_slices, channels, width, height))\nsegmentation_masks = (torch.randn(size=(batch, num_slices, 1, width, height))>0).float()\n\nprint(f\"CT images (train examples) shape: {ct_images.shape}\")\nprint(f\"Segmentation binary masks (labels) shape: {segmentation_masks.shape}\")\n\n# Define the MedCNN class and its forward method\nclass MedCNN(nn.Module):\n    def __init__(self, backbone, out_channel=1):\n        super(MedCNN, self).__init__()\n        self.backbone = backbone\n\n        #Downsample\n        self.conv1 = nn.Conv3d(512, 64, kernel_size=(3, 3, 3), padding=1)\n        self.conv2 = nn.Conv3d(64, 64, kernel_size=(3, 3, 3), padding=1)\n\n        #Upsample\n        self.conv_transpose1 = nn.ConvTranspose3d(64, 32, kernel_size=(1, 4, 4), stride=(1, 4, 4))\n        self.conv_transpose2 = nn.ConvTranspose3d(32, 16, kernel_size=(1, 8, 8), stride=(1, 8, 8))\n\n        #Final convolution layer from 16 to 1 channel\n        self.final_conv = nn.Conv3d(16, out_channel, kernel_size=1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        b, d, c, w, h = x.size()  #Input size: [B, D, C, W, H]\n        print(f\"Input shape [B, D, C, W, H]: {b, d, c, w, h}\")\n\n        x = x.view(b*d, c, w, h)  #Input to Resent 2DConv layers [B*D, C, W, H]\n        features = self.backbone(x)\n        print(f\"ResNet output shape[B*D, C, W, H]: {features.shape}\")\n\n        _, new_c, new_w, new_h = features.size()\n        x = features.view(b, d, new_c, new_w, new_h)  #[B, D, C, W, H]\n        x = torch.permute(x, (0, 2, 1, 3, 4))  #rearrange for 3DConv layers [B, C, D, W, H]\n        print(f\"Reshape Resnet output for 3DConv #1 [B, C, D, W, H]: {x.shape}\")\n\n        #Downsampling\n        x = self.relu(self.conv1(x))\n        print(f\"Output shape 3D Conv #1: {x.shape}\")\n        x = self.relu(self.conv2(x))\n        print(f\"Output shape 3D Conv #2: {x.shape}\")\n\n        #Upsampling\n        x = self.relu(self.conv_transpose1(x))\n        print(f\"Output shape 3D Transposed Conv #1: {x.shape}\")\n        x = self.relu(self.conv_transpose2(x))\n        print(f\"Output shape 3D Transposed Conv #2: {x.shape}\")\n\n        #final segmentation\n        x = torch.sigmoid(self.final_conv(x))\n        print(f\"Final shape: {x.shape}\")\n\n        return x\n\ndef compute_dice_loss(pred, labels, eps=1e-8):\n    '''\n    Args\n    pred: [B, D, 1, W, H]\n    labels: [B, D, 1, W, H]\n\n    Returns\n    dice_loss: [B, D, 1, W, H]\n    '''\n    numerator = 2*torch.sum(pred*labels)\n    denominator = torch.sum(pred) + torch.sum(labels) + eps\n    return numerator/denominator\n\nresnet_model = torchvision.models.resnet18(pretrained=True)\nresnet_model = nn.Sequential(*list(resnet_model.children())[:-2])\n\nmodel = MedCNN(backbone=resnet_model)\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nepochs = 5\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    pred = model(ct_images)\n    loss = compute_dice_loss(pred, segmentation_masks)\n    loss.backward()\n    optimizer.step()\n    print(f\"Loss at epoch {epoch}: {loss}\")",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nimport optax\nfrom flax.training import train_state\n\nbatch = 100\nnum_slices = 10\nchannels = 3\nwidth = 256\nheight = 256\n\nkey = jax.random.PRNGKey(42)\nkey, subkey1, subkey2 = jax.random.split(key, 3)\n\nct_images = jax.random.normal(subkey1, (batch, num_slices, channels, width, height))\nsegmentation_masks = (jax.random.normal(subkey2, (batch, num_slices, 1, width, height)) > 0).astype(jnp.float32)\n\nprint(\"CT images (train examples) shape:\", ct_images.shape)\nprint(\"Segmentation binary masks (labels) shape:\", segmentation_masks.shape)\n\nclass DummyResNet18(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=512,\n                    kernel_size=(7, 7),\n                    strides=(32, 32),\n                    padding='SAME',\n                    dimension_numbers=('NCHW', 'OIHW', 'NCHW'))(x)\n        x = nn.relu(x)\n        return x\n\nclass MedCNN(nn.Module):\n    backbone: nn.Module\n    out_channel: int = 1\n\n    @nn.compact\n    def __call__(self, x):\n        b, d, c, w, h = x.shape\n        print(\"Input shape [B, D, C, W, H]:\", (b, d, c, w, h))\n        x = x.reshape((b * d, c, w, h))\n        features = self.backbone(x)\n        print(\"Backbone (ResNet) output shape [B*D, C, W, H]:\", features.shape)\n        _, new_c, new_w, new_h = features.shape\n        x = features.reshape((b, d, new_c, new_w, new_h))\n        x = jnp.transpose(x, (0, 2, 1, 3, 4))\n        print(\"Reshaped for 3D conv [B, C, D, W, H]:\", x.shape)\n        dim3d = (\"NCDHW\", \"OIDHW\", \"NCDHW\")\n        x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME', dimension_numbers=dim3d)(x)\n        x = nn.relu(x)\n        print(\"After 3D Conv #1:\", x.shape)\n        x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME', dimension_numbers=dim3d)(x)\n        x = nn.relu(x)\n        print(\"After 3D Conv #2:\", x.shape)\n        x = nn.ConvTranspose(features=32, kernel_size=(1, 4, 4), strides=(1, 4, 4), padding='SAME', dimension_numbers=dim3d)(x)\n        x = nn.relu(x)\n        print(\"After 3D Transposed Conv #1:\", x.shape)\n        x = nn.ConvTranspose(features=16, kernel_size=(1, 8, 8), strides=(1, 8, 8), padding='SAME', dimension_numbers=dim3d)(x)\n        x = nn.relu(x)\n        print(\"After 3D Transposed Conv #2:\", x.shape)\n        x = nn.Conv(features=self.out_channel, kernel_size=(1, 1, 1), padding='SAME', dimension_numbers=dim3d)(x)\n        x = jax.nn.sigmoid(x)\n        print(\"Final output shape:\", x.shape)\n        return x\n\ndef compute_dice_loss(pred, labels, eps=1e-8):\n    numerator = 2 * jnp.sum(pred * labels)\n    denominator = jnp.sum(pred) + jnp.sum(labels) + eps\n    return numerator / denominator\n\nbackbone = DummyResNet18()\nmodel = MedCNN(backbone=backbone)\n\nclass TrainState(train_state.TrainState):\n    pass\n\nrng = jax.random.PRNGKey(0)\nparams = model.init(rng, ct_images)\ntx = optax.adam(learning_rate=0.01)\nstate = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\nepochs = 5\nfor epoch in range(epochs):\n    def loss_fn(params):\n        pred = model.apply(params, ct_images)\n        loss = compute_dice_loss(pred, segmentation_masks)\n        return loss, pred\n    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    print(f\"Loss at epoch {epoch}: {loss}\")",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nimport optax\nfrom flax.training import train_state\n\nbatch = 5\nnum_slices = 10\nchannels = 3\nwidth = 256\nheight = 256\n\nkey = jax.random.PRNGKey(42)\nkey, subkey1, subkey2 = jax.random.split(key, 3)\n\nct_images = jax.random.normal(subkey1, (batch, num_slices, channels, width, height))\nsegmentation_masks = (jax.random.normal(subkey2, (batch, num_slices, 1, width, height)) > 0).astype(jnp.float32)\n\nprint(\"CT images (train examples) shape:\", ct_images.shape)\nprint(\"Segmentation binary masks (labels) shape:\", segmentation_masks.shape)\n\nclass DummyResNet18(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=512, kernel_size=(7, 7), strides=(32, 32), padding='SAME')(x)\n        x = nn.relu(x)\n        return x\n\nclass MedCNN(nn.Module):\n    backbone: nn.Module\n    out_channel: int = 1\n\n    @nn.compact\n    def __call__(self, x):\n        b, d, c, w, h = x.shape\n        print(\"Input shape [B, D, C, W, H]:\", (b, d, c, w, h))\n        x = x.reshape((b * d, c, w, h))\n        features = self.backbone(x)\n        print(\"Backbone (ResNet) output shape [B*D, C, W, H]:\", features.shape)\n        _, new_c, new_w, new_h = features.shape\n        x = features.reshape((b, d, new_c, new_w, new_h))\n        x = jnp.transpose(x, (0, 2, 1, 3, 4))\n        print(\"Reshaped for 3D conv [B, C, D, W, H]:\", x.shape)\n        x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME')(x)\n        x = nn.relu(x)\n        print(\"After 3D Conv #1:\", x.shape)\n        x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME')(x)\n        x = nn.relu(x)\n        print(\"After 3D Conv #2:\", x.shape)\n        x = nn.ConvTranspose(features=32, kernel_size=(1, 4, 4), strides=(1, 4, 4), padding='SAME')(x)\n        x = nn.relu(x)\n        print(\"After 3D Transposed Conv #1:\", x.shape)\n        x = nn.ConvTranspose(features=16, kernel_size=(1, 8, 8), strides=(1, 8, 8), padding='SAME')(x)\n        x = nn.relu(x)\n        print(\"After 3D Transposed Conv #2:\", x.shape)\n        x = nn.Conv(features=self.out_channel, kernel_size=(1, 1, 1), padding='SAME')(x)\n        x = jax.nn.sigmoid(x)\n        print(\"Final output shape:\", x.shape)\n        return x\n\ndef compute_dice_loss(pred, labels, eps=1e-8):\n    numerator = 2 * jnp.sum(pred * labels)\n    denominator = jnp.sum(pred) + jnp.sum(labels) + eps\n    return numerator / denominator\n\nbackbone = DummyResNet18()\nmodel = MedCNN(backbone=backbone)\n\nclass TrainState(train_state.TrainState):\n    pass\n\nrng = jax.random.PRNGKey(0)\nparams = model.init(rng, ct_images)\ntx = optax.adam(learning_rate=0.01)\nstate = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\nepochs = 5\nfor epoch in range(epochs):\n    def loss_fn(params):\n        pred = model.apply(params, ct_images)\n        loss = compute_dice_loss(pred, segmentation_masks)\n        return loss, pred\n    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    print(f\"Loss at epoch {epoch}: {loss}\")",
        "Errors": [
          {
            "Error_Code": "x = nn.Conv(features=512, kernel_size=(7, 7), strides=(32, 32), padding='SAME', dimension_numbers=('NCHW', 'OIHW', 'NCHW'))(x) x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME', dimension_numbers=dim3d)(x)",
            "Error": "TypeError: Conv.__init__() got an unexpected keyword argument 'dimension_numbers'",
            "Fix_info": "Remove dimension_numbers.",
            "Fixed_Code": "x = nn.Conv(features=512, kernel_size=(7, 7), strides=(32, 32), padding='SAME')(x) x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME')(x)"
          },
          {
            "Error_Code": "(loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)",
            "Error": "XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 83886080000 bytes.",
            "Fix_info": "Reduce batch size.",
            "Fixed_Code": "batch = 5"
          }
        ]
      },
      {
        "Example_id": "m5",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Generate synthetic sequential data\ntorch.manual_seed(42)\nsequence_length = 10\nnum_samples = 100\n\n# Create a sine wave dataset\nX = torch.linspace(0, 4 * 3.14159, steps=num_samples).unsqueeze(1)\ny = torch.sin(X)\n\n# Prepare data for RNN\ndef create_in_out_sequences(data, seq_length):\n    in_seq = []\n    out_seq = []\n    for i in range(len(data) - seq_length):\n        in_seq.append(data[i:i + seq_length])\n        out_seq.append(data[i + seq_length])\n    return torch.stack(in_seq), torch.stack(out_seq)\n\nX_seq, y_seq = create_in_out_sequences(y, sequence_length)\n\n# Define the RNN Model\nclass RNNModel(nn.Module):\n    def __init__(self):\n        super(RNNModel, self).__init__()\n        self.rnn = nn.RNN(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n        self.fc = nn.Linear(50, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out, _ = self.rnn(x)\n        out = self.fc(out[:, -1, :])  # Use the last output of the RNN\n        return out\n\n# Initialize the model, loss function, and optimizer\nmodel = RNNModel()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 500\nfor epoch in range(epochs):\n    for sequences, labels in zip(X_seq, y_seq):\n        sequences = sequences.unsqueeze(0)  # Add batch dimension\n        labels = labels.unsqueeze(0)  # Add batch dimension\n\n        # Forward pass\n        outputs = model(sequences)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Testing on new data\nX_test = torch.linspace(4 * 3.14159, 5 * 3.14159, steps=10).unsqueeze(1)\n\n# Reshape to (batch_size, sequence_length, input_size)\nX_test = X_test.unsqueeze(0)  # Add batch dimension, shape becomes (1, 10, 1)\n\nwith torch.no_grad():\n    predictions = model(X_test)\n    print(f\"Predictions for new sequence: {predictions.tolist()}\")",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nimport optax\nimport numpy as np\n\nsequence_length = 10\nnum_samples = 100\n\nX = jnp.linspace(0, 4 * jnp.pi, num=num_samples)[:, None]\ny = jnp.sin(X)\n\ndef create_in_out_sequences(data, seq_length):\n    in_seq = []\n    out_seq = []\n    for i in range(len(data) - seq_length):\n        in_seq.append(data[i:i + seq_length])\n        out_seq.append(data[i + seq_length])\n    return jnp.stack(in_seq), jnp.stack(out_seq)\n\nX_seq, y_seq = create_in_out_sequences(y, sequence_length)\n\nclass RNNCell(nn.Module):\n    hidden_size: int = 50\n\n    @nn.compact\n    def __call__(self, carry, x):\n        new_h = nn.tanh(\n            nn.Dense(self.hidden_size, name=\"ih\")(x) +\n            nn.Dense(self.hidden_size, use_bias=False, name=\"hh\")(carry)\n        )\n        return new_h, new_h\n\nclass RNNModel(nn.Module):\n    hidden_size: int = 50\n\n    @nn.compact\n    def __call__(self, x):\n        batch_size = x.shape[0]\n        init_carry = jnp.zeros((batch_size, self.hidden_size))\n        rnn_scan = nn.scan(\n            RNNCell,\n            in_axes=1,\n            out_axes=1,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False},\n        )(hidden_size=self.hidden_size)\n        carry, ys = rnn_scan(init_carry, x)\n        last_output = ys[:, -1, :]\n        output = nn.Dense(1)(last_output)\n        return output\n\nmodel = RNNModel()\nrng = jax.random.PRNGKey(42)\nsample_input = jnp.ones((1, sequence_length, 1))\nparams = model.init(rng, sample_input)\n\noptimizer = optax.adam(learning_rate=0.001)\nopt_state = optimizer.init(params)\n\ndef loss_fn(params, x, y):\n    preds = model.apply(params, x)\n    return jnp.mean((preds - y) ** 2)\n\n@jax.jit\ndef train_step(params, opt_state, x, y):\n    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\nepochs = 500\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    for seq, label in zip(X_seq, y_seq):\n        seq = seq[None, :, :]\n        label = label[None, :]\n        params, opt_state, loss = train_step(params, opt_state, seq, label)\n        epoch_loss += loss\n    epoch_loss /= len(X_seq)\n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n\nX_test = jnp.linspace(4 * jnp.pi, 5 * jnp.pi, num=10)[:, None]\nX_test = X_test[None, :, :]\npredictions = model.apply(params, X_test)\nprint(\"Predictions for new sequence:\", predictions)",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nimport optax\nimport numpy as np\n\nsequence_length = 10\nnum_samples = 100\n\nX = jnp.linspace(0, 4 * jnp.pi, num=num_samples)[:, None]\ny = jnp.sin(X)\n\ndef create_in_out_sequences(data, seq_length):\n    in_seq = []\n    out_seq = []\n    for i in range(len(data) - seq_length):\n        in_seq.append(data[i:i + seq_length])\n        out_seq.append(data[i + seq_length])\n    return jnp.stack(in_seq), jnp.stack(out_seq)\n\nX_seq, y_seq = create_in_out_sequences(y, sequence_length)\n\nclass RNNCell(nn.Module):\n    hidden_size: int = 50\n\n    @nn.compact\n    def __call__(self, carry, x):\n        new_h = nn.tanh(\n            nn.Dense(self.hidden_size, name=\"ih\")(x) +\n            nn.Dense(self.hidden_size, use_bias=False, name=\"hh\")(carry)\n        )\n        return new_h, new_h\n\nclass RNNModel(nn.Module):\n    hidden_size: int = 50\n\n    @nn.compact\n    def __call__(self, x):\n        batch_size = x.shape[0]\n        init_carry = jnp.zeros((batch_size, self.hidden_size))\n        rnn_scan = nn.scan(\n            RNNCell,\n            in_axes=1,\n            out_axes=1,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False},\n        )(hidden_size=self.hidden_size)\n        carry, ys = rnn_scan(init_carry, x)\n        last_output = ys[:, -1, :]\n        output = nn.Dense(1)(last_output)\n        return output\n\nmodel = RNNModel()\nrng = jax.random.PRNGKey(42)\nsample_input = jnp.ones((1, sequence_length, 1))\nparams = model.init(rng, sample_input)\n\noptimizer = optax.adam(learning_rate=0.001)\nopt_state = optimizer.init(params)\n\ndef loss_fn(params, x, y):\n    preds = model.apply(params, x)\n    return jnp.mean((preds - y) ** 2)\n\n@jax.jit\ndef train_step(params, opt_state, x, y):\n    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\nepochs = 500\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    for seq, label in zip(X_seq, y_seq):\n        seq = seq[None, :, :]\n        label = label[None, :]\n        params, opt_state, loss = train_step(params, opt_state, seq, label)\n        epoch_loss += loss\n    epoch_loss /= len(X_seq)\n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n\nX_test = jnp.linspace(4 * jnp.pi, 5 * jnp.pi, num=10)[:, None]\nX_test = X_test[None, :, :]\npredictions = model.apply(params, X_test)\nprint(\"Predictions for new sequence:\", predictions)",
        "Errors": []
      },
      {
        "Example_id": "m6",
        "Input_Code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load CIFAR-10 dataset with data augmentation\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n    transforms.RandomCrop(32, padding=4), # Randomly crop the image with padding\n    transforms.ToTensor(),                # Convert the image to a tensor\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize with mean and std\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Display a batch of augmented images\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Get some random training images\ndata_iter = iter(train_loader)\nimages, labels = next(data_iter)\n\n# Show images\nimshow(torchvision.utils.make_grid(images))",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\ndef random_flip(image, key):\n    do_flip = jax.random.uniform(key) > 0.5\n    return jnp.where(do_flip, jnp.flip(image, axis=1), image)\n\ndef random_crop(image, key, crop_size=32, padding=4):\n    image_padded = jnp.pad(image, ((padding, padding), (padding, padding), (0, 0)), mode='reflect')\n    max_offset = padding * 2\n    offset_x = jax.random.randint(key, (), 0, max_offset + 1)\n    offset_y = jax.random.randint(key, (), 0, max_offset + 1)\n    cropped = image_padded[offset_x:offset_x+crop_size, offset_y:offset_y+crop_size, :]\n    return cropped\n\ndef normalize(image):\n    return (image - 0.5 ​​​​​​return (image - 0.5) / 0.5\n\ndef augment_image(image, key):\n    key_flip, key_crop = jax.random.split(key)\n    image = random_flip(image, key_flip)\n    image = random_crop(image, key_crop, crop_size=32, padding=4)\n    image = normalize(image)\n    return image\n\ndef imshow_grid(images):\n    images = images * 0.5 + 0.5\n    batch, h, w, c = images.shape\n    grid_cols = 8\n    grid_rows = int(np.ceil(batch / grid_cols))\n    grid = np.zeros((grid_rows * h, grid_cols * w, c))\n    for idx, image in enumerate(images):\n        row = idx // grid_cols\n        col = idx % grid_cols\n        grid[row*h:(row+1)*h, col*w:(col+1)*w, :] = image\n    plt.figure(figsize=(grid_cols, grid_rows))\n    plt.imshow(grid)\n    plt.axis('off')\n    plt.show()\n\ndef load_dataset(split, batch_size=64):\n    ds = tfds.load('cifar10', split=split, as_supervised=True)\n\n    def preprocess(image, label):\n        image = tf.cast(image, tf.float32) / 255.0\n        return image, label\n\n    ds = ds.map(preprocess)\n    ds = ds.cache()\n    if split == 'train':\n        ds = ds.shuffle(10000)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(1)\n    return tfds.as_numpy(ds)\n\ntrain_ds = load_dataset('train', batch_size=64)\ntest_ds = load_dataset('test', batch_size=64)\n\nbatch = next(iter(train_ds))\nimages, labels = batch\naugmented_images = []\nfor i, image in enumerate(images):\n    key = jax.random.PRNGKey(i)\n    aug_img = augment_image(jnp.array(image), key)\n    augmented_images.append(np.array(aug_img))\naugmented_images = np.stack(augmented_images, axis=0)\n\nimshow_grid(augmented_images)",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\ndef random_flip(image, key):\n    do_flip = jax.random.uniform(key) > 0.5\n    return jnp.where(do_flip, jnp.flip(image, axis=1), image)\n\ndef random_crop(image, key, crop_size=32, padding=4):\n    image_padded = jnp.pad(image, ((padding, padding), (padding, padding), (0, 0)), mode='reflect')\n    max_offset = padding * 2\n    offset_x = jax.random.randint(key, (), 0, max_offset + 1)\n    offset_y = jax.random.randint(key, (), 0, max_offset + 1)\n    cropped = image_padded[offset_x:offset_x+crop_size, offset_y:offset_y+crop_size, :]\n    return cropped\n\ndef normalize(image):\n    return (image - 0.5) / 0.5\n\ndef augment_image(image, key):\n    key_flip, key_crop = jax.random.split(key)\n    image = random_flip(image, key_flip)\n    image = random_crop(image, key_crop, crop_size=32, padding=4)\n    image = normalize(image)\n    return image\n\ndef imshow_grid(images):\n    images = images * 0.5 + 0.5\n    batch, h, w, c = images.shape\n    grid_cols = 8\n    grid_rows = int(np.ceil(batch / grid_cols))\n    grid = np.zeros((grid_rows * h, grid_cols * w, c))\n    for idx, image in enumerate(images):\n        row = idx // grid_cols\n        col = idx % grid_cols\n        grid[row*h:(row+1)*h, col*w:(col+1)*w, :] = image\n    plt.figure(figsize=(grid_cols, grid_rows))\n    plt.imshow(grid)\n    plt.axis('off')\n    plt.show()\n\ndef load_dataset(split, batch_size=64):\n    ds = tfds.load('cifar10', split=split, as_supervised=True)\n\n    def preprocess(image, label):\n        image = tf.cast(image, tf.float32) / 255.0\n        return image, label\n\n    ds = ds.map(preprocess)\n    ds = ds.cache()\n    if split == 'train':\n        ds = ds.shuffle(10000)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(1)\n    return tfds.as_numpy(ds)\n\ntrain_ds = load_dataset('train', batch_size=64)\ntest_ds = load_dataset('test', batch_size=64)\n\nbatch = next(iter(train_ds))\nimages, labels = batch\naugmented_images = []\nfor i, image in enumerate(images):\n    key = jax.random.PRNGKey(i)\n    aug_img = augment_image(jnp.array(image), key)\n    augmented_images.append(np.array(aug_img))\naugmented_images = np.stack(augmented_images, axis=0)\n\nimshow_grid(augmented_images)",
        "Errors": []
      },
      {
        "Example_id": "m7",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport time\n\n# Load MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Define a simple neural network model\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(28*28, 128)  # Input: 28x28 pixels, Output: 128 neurons\n        self.fc2 = nn.Linear(128, 10)  # Output: 10 classes (digits 0-9)\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)  # Flatten the input\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop with benchmarking\nepochs = 5\nfor epoch in range(epochs):\n    start_time = time.time()  # Start time for training\n    for images, labels in train_loader:\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    end_time = time.time()  # End time for training\n    training_time = end_time - start_time\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}, Time: {training_time:.4f}s\")\n\n# Evaluate the model on the test set and benchmark the accuracy\ncorrect = 0\ntotal = 0\nstart_time = time.time()  # Start time for testing\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nend_time = time.time()  # End time for testing\ntesting_time = end_time - start_time\naccuracy = 100 * correct / total\nprint(f\"Test Accuracy: {accuracy:.2f}%, Testing Time: {testing_time:.4f}s\")",
        "LLM_strong_output": "import time\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport optax\nfrom jax.example_libraries import stax\nfrom jax.example_libraries.stax import Dense, Relu, Flatten\n\ndef preprocess(example):\n    image = np.array(example['image'], dtype=np.float32) / 255.0\n    image = (image - 0.5) / 0.5\n    label = example['label']\n    return image, label\n\ndef dataset_to_batches(ds, batch_size):\n    ds = tfds.as_numpy(ds)\n    images, labels = [], []\n    for example in ds:\n        img, lab = preprocess(example)\n        images.append(img)\n        labels.append(lab)\n    images = np.stack(images)\n    labels = np.array(labels)\n    num_batches = images.shape[0] // batch_size\n    batches = []\n    for i in range(num_batches):\n        batch_images = images[i*batch_size:(i+1)*batch_size]\n        batch_labels = labels[i*batch_size:(i+1)*batch_size]\n        batches.append((batch_images, batch_labels))\n    return batches\n\nbatch_size = 64\ntrain_ds = tfds.load('mnist', split='train', shuffle_files=True)\ntest_ds = tfds.load('mnist', split='test', shuffle_files=False)\n\ntrain_batches = dataset_to_batches(train_ds, batch_size)\ntest_batches = dataset_to_batches(test_ds, batch_size)\n\ninit_random_params, predict = stax.serial(\n    Flatten,\n    Dense(128),\n    Relu,\n    Dense(10)\n)\n\nrng = jax.random.PRNGKey(0)\n_, params = init_random_params(rng, (-1, 28, 28, 1))\n\ndef loss_fn(params, batch):\n    images, labels = batch\n    logits = predict(params, images)\n    one_hot = jax.nn.one_hot(labels, num_classes=10)\n    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n    return loss\n\noptimizer = optax.sgd(learning_rate=0.01)\nopt_state = optimizer.init(params)\n\n@jax.jit\ndef train_step(params, opt_state, batch):\n    loss, grads = jax.value_and_grad(loss_fn)(params, batch)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\nepochs = 5\nfor epoch in range(epochs):\n    start_time = time.time()\n    for batch in train_batches:\n        params, opt_state, loss = train_step(params, opt_state, batch)\n    end_time = time.time()\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}, Time: {end_time - start_time:.4f}s\")\n\ncorrect = 0\ntotal = 0\nstart_time = time.time()\nfor batch in test_batches:\n    images, labels = batch\n    logits = predict(params, images)\n    predictions = jnp.argmax(logits, axis=1)\n    correct += int(jnp.sum(predictions == labels))\n    total += images.shape[0]\nend_time = time.time()\naccuracy = 100 * correct / total\nprint(f\"Test Accuracy: {accuracy:.2f}%, Testing Time: {end_time - start_time:.4f}s\")",
        "LLM_fix_strong_output": "import time\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport optax\nfrom jax.example_libraries import stax\nfrom jax.example_libraries.stax import Dense, Relu, Flatten\n\ndef preprocess(example):\n    image = np.array(example['image'], dtype=np.float32) / 255.0\n    image = (image - 0.5) / 0.5\n    label = example['label']\n    return image, label\n\ndef dataset_to_batches(ds, batch_size):\n    ds = tfds.as_numpy(ds)\n    images, labels = [], []\n    for example in ds:\n        img, lab = preprocess(example)\n        images.append(img)\n        labels.append(lab)\n    images = np.stack(images)\n    labels = np.array(labels)\n    num_batches = images.shape[0] // batch_size\n    batches = []\n    for i in range(num_batches):\n        batch_images = images[i*batch_size:(i+1)*batch_size]\n        batch_labels = labels[i*batch_size:(i+1)*batch_size]\n        batches.append((batch_images, batch_labels))\n    return batches\n\nbatch_size = 64\ntrain_ds = tfds.load('mnist', split='train', shuffle_files=True)\ntest_ds = tfds.load('mnist', split='test', shuffle_files=False)\n\ntrain_batches = dataset_to_batches(train_ds, batch_size)\ntest_batches = dataset_to_batches(test_ds, batch_size)\n\ninit_random_params, predict = stax.serial(\n    Flatten,\n    Dense(128),\n    Relu,\n    Dense(10)\n)\n\nrng = jax.random.PRNGKey(0)\n_, params = init_random_params(rng, (-1, 28, 28, 1))\n\ndef loss_fn(params, batch):\n    images, labels = batch\n    logits = predict(params, images)\n    one_hot = jax.nn.one_hot(labels, num_classes=10)\n    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n    return loss\n\noptimizer = optax.sgd(learning_rate=0.01)\nopt_state = optimizer.init(params)\n\n@jax.jit\ndef train_step(params, opt_state, batch):\n    loss, grads = jax.value_and_grad(loss_fn)(params, batch)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\nepochs = 5\nfor epoch in range(epochs):\n    start_time = time.time()\n    for batch in train_batches:\n        params, opt_state, loss = train_step(params, opt_state, batch)\n    end_time = time.time()\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}, Time: {end_time - start_time:.4f}s\")\n\ncorrect = 0\ntotal = 0\nstart_time = time.time()\nfor batch in test_batches:\n    images, labels = batch\n    logits = predict(params, images)\n    predictions = jnp.argmax(logits, axis=1)\n    correct += int(jnp.sum(predictions == labels))\n    total += images.shape[0]\nend_time = time.time()\naccuracy = 100 * correct / total\nprint(f\"Test Accuracy: {accuracy:.2f}%, Testing Time: {end_time - start_time:.4f}s\")",
        "Errors": []
      },
      {
        "Example_id": "m8",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Define an Autoencoder model\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # Downsample to 14x14\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)  # Downsample to 7x7\n        )\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()  # To keep pixel values between 0 and 1\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n# Initialize the model, loss function, and optimizer\nmodel = Autoencoder()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 10\nfor epoch in range(epochs):\n    for images, _ in train_loader:\n        # Forward pass\n        reconstructed = model(images)\n        loss = criterion(reconstructed, images)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nfrom jax import random, jit, value_and_grad\nimport optax\nimport flax.linen as nn\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\n\ndef preprocess(example):\n    image = example['image']\n    image = tf.cast(image, tf.float32) / 255.0\n    image = (image - 0.5) / 0.5\n    if image.shape.ndims == 2:\n        image = tf.expand_dims(image, axis=-1)\n    return {'image': image}\n\nbatch_size = 64\ntrain_ds = tfds.load('mnist', split='train', shuffle_files=True)\ntrain_ds = train_ds.map(preprocess)\ntrain_ds = train_ds.shuffle(1024).batch(batch_size).prefetch(1)\n\nclass Autoencoder(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n        x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n        x = nn.ConvTranspose(features=32, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.ConvTranspose(features=1, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)\n        x = nn.sigmoid(x)\n        return x\n\nrng = random.PRNGKey(0)\ndummy_input = jnp.ones([1, 28, 28, 1])\nmodel = Autoencoder()\nparams = model.init(rng, dummy_input)\n\noptimizer = optax.adam(learning_rate=0.001)\nopt_state = optimizer.init(params)\n\ndef mse_loss(params, batch):\n    recon = model.apply(params, batch)\n    return jnp.mean((recon - batch) ** 2)\n\n@jit\ndef train_step(params, opt_state, images):\n    loss, grads = value_and_grad(mse_loss)(params, images)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\nepochs = 10\nfor epoch in range(epochs):\n    for batch in tfds.as_numpy(train_ds):\n        images = batch['image']\n        params, opt_state, loss = train_step(params, opt_state, images)\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nfrom jax import random, jit, value_and_grad\nimport optax\nimport flax.linen as nn\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\n\ndef preprocess(example):\n    image = example['image']\n    image = tf.cast(image, tf.float32) / 255.0\n    image = (image - 0.5) / 0.5\n    if image.shape.ndims == 2:\n        image = tf.expand_dims(image, axis=-1)\n    return {'image': image}\n\nbatch_size = 64\ntrain_ds = tfds.load('mnist', split='train', shuffle_files=True)\ntrain_ds = train_ds.map(preprocess)\ntrain_ds = train_ds.shuffle(1024).batch(batch_size).prefetch(1)\n\nclass Autoencoder(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n        x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n        x = nn.ConvTranspose(features=32, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.ConvTranspose(features=1, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)\n        x = nn.sigmoid(x)\n        return x\n\nrng = random.PRNGKey(0)\ndummy_input = jnp.ones([1, 28, 28, 1])\nmodel = Autoencoder()\nparams = model.init(rng, dummy_input)\n\noptimizer = optax.adam(learning_rate=0.001)\nopt_state = optimizer.init(params)\n\ndef mse_loss(params, batch):\n    recon = model.apply(params, batch)\n    return jnp.mean((recon - batch) ** 2)\n\n@jit\ndef train_step(params, opt_state, images):\n    loss, grads = value_and_grad(mse_loss)(params, images)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\nepochs = 10\nfor epoch in range(epochs):\n    for batch in tfds.as_numpy(train_ds):\n        images = batch['image']\n        params, opt_state, loss = train_step(params, opt_state, images)\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")",
        "Errors": []
      },
      {
        "Example_id": "m1",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n# Generate synthetic sequential data\ntorch.manual_seed(42)\nsequence_length = 10\nnum_samples = 100\n\n# Create a sine wave dataset\nX = torch.linspace(0, 4 * 3.14159, steps=num_samples).unsqueeze(1)\ny = torch.sin(X)\n\n# Prepare data for LSTM\ndef create_in_out_sequences(data, seq_length):\n    in_seq = []\n    out_seq = []\n    for i in range(len(data) - seq_length):\n        in_seq.append(data[i:i + seq_length])\n        out_seq.append(data[i + seq_length])\n    return torch.stack(in_seq), torch.stack(out_seq)\n\nX_seq, y_seq = create_in_out_sequences(y, sequence_length)\n\nclass CustomLSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_units):\n        super().__init__()\n        weights_biases_init = lambda : (nn.Parameter(torch.randn(input_dim, hidden_units)), nn.Parameter(torch.randn(hidden_units, hidden_units)), nn.Parameter(torch.zeros(hidden_units)))\n        self.input_dim = input_dim\n        self.hidden_units = hidden_units\n        self.Wxi, self.Whi, self.bi = weights_biases_init()\n        self.Wxf, self.Whf, self.bf = weights_biases_init()\n        self.Wxo, self.Who, self.bo = weights_biases_init()\n        self.Wxc, self.Whc, self.bc = weights_biases_init()\n        self.fc = nn.Linear(hidden_units, 1)\n\n    def forward(self, inputs, H_C=None):\n        batch_size, seq_len, _ = inputs.shape\n        if not H_C:\n            H = torch.randn(batch_size, self.hidden_units)\n            C = torch.randn(batch_size, self.hidden_units)\n        else:\n            H, C = H_C\n\n        all_hidden_states = []\n        for t in range(seq_len):\n            X_t = inputs[:, t, :]\n            I_t = torch.sigmoid(torch.matmul(X_t, self.Wxi) + torch.matmul(H, self.Whi) + self.bi)\n            F_t = torch.sigmoid(torch.matmul(X_t, self.Wxf) + torch.matmul(H, self.Whf) + self.bf)\n            O_t = torch.sigmoid(torch.matmul(X_t, self.Wxo) + torch.matmul(H, self.Who) + self.bo)\n            C_tilde = torch.tanh(torch.matmul(X_t, self.Wxc) + torch.matmul(H, self.Whc) + self.bc)\n            C = F_t * C + I_t * C_tilde\n            H = O_t * torch.tanh(C)\n            all_hidden_states.append(H.unsqueeze(1))\n\n        outputs = torch.cat(all_hidden_states, dim=1)\n        pred = self.fc(outputs)\n        return pred, (H, C)\n\n# Define the LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n        self.fc = nn.Linear(50, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Initialize the model, loss function, and optimizer\nmodel_custom = CustomLSTMModel(1, 50)\nmodel_inbuilt = LSTMModel()\ncriterion = nn.MSELoss()\noptimizer_custom = optim.Adam(model_custom.parameters(), lr=0.01)\noptimizer_inbuilt = optim.Adam(model_inbuilt.parameters(), lr=0.01)\n\n# Training loop for the custom model\nepochs = 500\nfor epoch in range(epochs):\n    state = None\n    pred, state = model_custom(X_seq, state)\n    loss = criterion(pred[:, -1, :], y_seq)\n    optimizer_custom.zero_grad()\n    loss.backward()\n    optimizer_custom.step()\n    if (epoch + 1) % 50 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Training loop for the inbuilt model\nepochs = 500\nfor epoch in range(epochs):\n    pred = model_inbuilt(X_seq)\n    loss = criterion(pred, y_seq)\n    optimizer_inbuilt.zero_grad()\n    loss.backward()\n    optimizer_inbuilt.step()\n    if (epoch + 1) % 50 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\n# Testing on new data\ntest_steps = 100\nX_test = torch.linspace(0, 5 * 3.14159, steps=test_steps).unsqueeze(1)\ny_test = torch.sin(X_test)\n\nX_test_seq, _ = create_in_out_sequences(y_test, sequence_length)\n\nwith torch.no_grad():\n    pred_custom, _ = model_custom(X_test_seq)\n    pred_inbuilt = model_inbuilt(X_test_seq)\npred_custom = torch.flatten(pred_custom[:, -1, :])\npred_inbuilt = pred_inbuilt.squeeze()\nprint(f\"Predictions with Custom Model for new sequence: {pred_custom.tolist()}\")\nprint(f\"Predictions with In-Built Model: {pred_inbuilt.tolist()}\")\n\n#Plot the predictions\nplt.figure()\nplt.plot(pred_custom, label=\"custom model\")\nplt.plot(pred_inbuilt, label=\"inbuilt model\")\nplt.legend()\nplt.show()",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\nimport matplotlib.pyplot as plt\n\nfrom flax import linen as nn\nfrom flax.training import train_state\n\nnp.random.seed(42)\nsequence_length = 10\nnum_samples = 100\n\nX_vals = np.linspace(0, 4 * np.pi, num_samples).reshape(-1, 1)\ny_vals = np.sin(X_vals)\n\ndef create_in_out_sequences(data, seq_length):\n    in_seq = []\n    out_seq = []\n    for i in range(len(data) - seq_length):\n        in_seq.append(data[i:i + seq_length])\n        out_seq.append(data[i + seq_length])\n    return jnp.array(in_seq), jnp.array(out_seq)\n\nX_seq, y_seq = create_in_out_sequences(y_vals, sequence_length)\n\nclass CustomLSTMModel(nn.Module):\n    input_dim: int\n    hidden_units: int\n\n    @nn.compact\n    def __call__(self, inputs, H_C=None):\n        batch_size, seq_len, _ = inputs.shape\n        if H_C is None:\n            key = self.make_rng(\"lstm\")\n            key_H, key_C = jax.random.split(key)\n            H = jax.random.normal(key_H, (batch_size, self.hidden_units))\n            C = jax.random.normal(key_C, (batch_size, self.hidden_units))\n        else:\n            H, C = H_C\n\n        Wxi = self.param(\"Wxi\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Whi = self.param(\"Whi\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bi = self.param(\"bi\", nn.initializers.zeros, (self.hidden_units,))\n\n        Wxf = self.param(\"Wxf\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Whf = self.param(\"Whf\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bf = self.param(\"bf\", nn.initializers.zeros, (self.hidden_units,))\n\n        Wxo = self.param(\"Wxo\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Who = self.param(\"Who\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bo = self.param(\"bo\", nn.initializers.zeros, (self.hidden_units,))\n\n        Wxc = self.param(\"Wxc\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Whc = self.param(\"Whc\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bc = self.param(\"bc\", nn.initializers.zeros, (self.hidden_units,))\n\n        dense = nn.Dense(features=1)\n\n        outputs = []\n        for t in range(seq_len):\n            X_t = inputs[:, t, :]\n            I_t = jax.nn.sigmoid(jnp.dot(X_t, Wxi) + jnp.dot(H, Whi) + bi)\n            F_t = jax.nn.sigmoid(jnp.dot(X_t, Wxf) + jnp.dot(H, Whf) + bf)\n            O_t = jax.nn.sigmoid(jnp.dot(X_t, Wxo) + jnp.dot(H, Who) + bo)\n            C_tilde = jnp.tanh(jnp.dot(X_t, Wxc) + jnp.dot(H, Whc) + bc)\n            C = F_t * C + I_t * C_tilde\n            H = O_t * jnp.tanh(C)\n            outputs.append(H)\n        outputs = jnp.stack(outputs, axis=1)\n        pred = dense(outputs)\n        return pred, (H, C)\n\nclass LSTMModel(nn.Module):\n    hidden_size: int = 50\n\n    @nn.compact\n    def __call__(self, inputs):\n        batch_size, seq_len, input_dim = inputs.shape\n        carry = nn.LSTMCell.initialize_carry(self.make_rng(\"lstm\"), (batch_size, input_dim), self.hidden_size)\n        lstm_cell = nn.LSTMCell()\n        outputs = []\n        for t in range(seq_len):\n            carry, y = lstm_cell(carry, inputs[:, t, :])\n            outputs.append(y)\n        outputs = jnp.stack(outputs, axis=1)\n        dense = nn.Dense(features=1)\n        out = dense(outputs[:, -1, :])\n        return out\n\nclass TrainState(train_state.TrainState):\n    pass\n\ndef loss_fn_custom(params, rng, batch_x, batch_y):\n    pred, _ = custom_model.apply({\"params\": params}, batch_x, rngs={\"lstm\": rng})\n    pred_last = pred[:, -1, 0]\n    loss = jnp.mean((pred_last - batch_y.squeeze()) ** 2)\n    return loss\n\ndef loss_fn_builtin(params, rng, batch_x, batch_y):\n    pred = lstm_model.apply({\"params\": params}, batch_x, rngs={\"lstm\": rng})\n    pred = pred.squeeze()\n    loss = jnp.mean((pred - batch_y.squeeze()) ** 2)\n    return loss\n\n@jax.jit\ndef train_step_custom(state, batch_x, batch_y, rng):\n    loss, grads = jax.value_and_grad(loss_fn_custom)(state.params, rng, batch_x, batch_y)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\n@jax.jit\ndef train_step_builtin(state, batch_x, batch_y, rng):\n    loss, grads = jax.value_and_grad(loss_fn_builtin)(state.params, rng, batch_x, batch_y)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\ncustom_model = CustomLSTMModel(input_dim=1, hidden_units=50)\nlstm_model = LSTMModel(hidden_size=50)\n\ndummy_input_custom = jnp.ones((X_seq.shape[0], sequence_length, 1))\ndummy_input_builtin = jnp.ones((X_seq.shape[0], sequence_length, 1))\n\nrng_custom = jax.random.PRNGKey(0)\nrng_builtin = jax.random.PRNGKey(1)\n\nparams_custom = custom_model.init({\"params\": rng_custom, \"lstm\": rng_custom}, dummy_input_custom)[\"params\"]\nparams_builtin = lstm_model.init({\"params\": rng_builtin, \"lstm\": rng_builtin}, dummy_input_builtin)[\"params\"]\n\noptimizer = optax.adam(0.01)\nstate_custom = TrainState.create(apply_fn=custom_model.apply, params=params_custom, tx=optimizer)\nstate_builtin = TrainState.create(apply_fn=lstm_model.apply, params=params_builtin, tx=optimizer)\n\nepochs = 500\nrng = jax.random.PRNGKey(42)\nfor epoch in range(epochs):\n    rng, step_rng = jax.random.split(rng)\n    state_custom, loss_value = train_step_custom(state_custom, X_seq, y_seq, step_rng)\n    if (epoch + 1) % 50 == 0:\n        print(f\"[Custom LSTM] Epoch [{epoch + 1}/{epochs}], Loss: {loss_value:.4f}\")\n\nrng = jax.random.PRNGKey(100)\nfor epoch in range(epochs):\n    rng, step_rng = jax.random.split(rng)\n    state_builtin, loss_value = train_step_builtin(state_builtin, X_seq, y_seq, step_rng)\n    if (epoch + 1) % 50 == 0:\n        print(f\"[Built-In LSTM] Epoch [{epoch + 1}/{epochs}], Loss: {loss_value:.4f}\")\n\ntest_steps = 100\nX_test = np.linspace(0, 5 * np.pi, test_steps).reshape(-1, 1)\ny_test = np.sin(X_test)\nX_test_seq, _ = create_in_out_sequences(y_test, sequence_length)\n\nrng, custom_test_rng = jax.random.split(rng)\npred_custom, _ = custom_model.apply({\"params\": state_custom.params}, X_test_seq, rngs={\"lstm\": custom_test_rng})\npred_custom = pred_custom[:, -1, 0]\n\nrng, builtin_test_rng = jax.random.split(rng)\npred_builtin = lstm_model.apply({\"params\": state_builtin.params}, X_test_seq, rngs={\"lstm\": builtin_test_rng})\npred_builtin = pred_builtin.squeeze()\n\nprint(\"Predictions with Custom Model for new sequence:\")\nprint(np.array(pred_custom))\nprint(\"\\nPredictions with Built-In Model:\")\nprint(np.array(pred_builtin))\n\nplt.figure(figsize=(8, 4))\nplt.plot(pred_custom, label=\"Custom LSTM Model\")\nplt.plot(pred_builtin, label=\"Built-In LSTM Model\")\nplt.legend()\nplt.title(\"Predictions on New Sine Wave Sequence\")\nplt.xlabel(\"Time step\")\nplt.ylabel(\"Predicted value\")\nplt.show()",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\nimport matplotlib.pyplot as plt\n\nfrom flax import linen as nn\nfrom flax.training import train_state\n\n# ----------------------------\n# Data Preparation\n# ----------------------------\n# Set random seed for numpy (JAX’s PRNG is used in the models)\nnp.random.seed(42)\nsequence_length = 10\nnum_samples = 100\n\n# Create a sine wave dataset\nX_vals = np.linspace(0, 4 * np.pi, num_samples).reshape(-1, 1)\ny_vals = np.sin(X_vals)\n\ndef create_in_out_sequences(data, seq_length):\n    in_seq = []\n    out_seq = []\n    # data is assumed to be of shape (N, 1)\n    for i in range(len(data) - seq_length):\n        in_seq.append(data[i:i + seq_length])\n        out_seq.append(data[i + seq_length])\n    return jnp.array(in_seq), jnp.array(out_seq)\n\nX_seq, y_seq = create_in_out_sequences(y_vals, sequence_length)  # shapes: (num_samples-seq_length, seq_length, 1) and (num_samples-seq_length, 1)\n\n# ----------------------------\n# Custom LSTM Model (Hand-Coded)\n# ----------------------------\nclass CustomLSTMModel(nn.Module):\n    input_dim: int\n    hidden_units: int\n\n    @nn.compact\n    def __call__(self, inputs, H_C=None):\n        \"\"\"\n        inputs: shape (batch, seq_len, input_dim)\n        H_C: tuple of (H, C); if None, initialize with random normal values using the \"lstm\" RNG.\n        Returns:\n          pred: predictions of shape (batch, seq_len, 1)\n          (H, C): the final hidden and cell states.\n        \"\"\"\n        batch_size, seq_len, _ = inputs.shape\n        if H_C is None:\n            # Use the \"lstm\" rng to generate initial states.\n            key = self.make_rng(\"lstm\")\n            key_H, key_C = jax.random.split(key)\n            H = jax.random.normal(key_H, (batch_size, self.hidden_units))\n            C = jax.random.normal(key_C, (batch_size, self.hidden_units))\n        else:\n            H, C = H_C\n\n        # Initialize gate parameters. (Each gate uses a weight matrix for input and hidden state, plus a bias.)\n        Wxi = self.param(\"Wxi\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Whi = self.param(\"Whi\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bi  = self.param(\"bi\", nn.initializers.zeros, (self.hidden_units,))\n\n        Wxf = self.param(\"Wxf\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Whf = self.param(\"Whf\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bf  = self.param(\"bf\", nn.initializers.zeros, (self.hidden_units,))\n\n        Wxo = self.param(\"Wxo\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Who = self.param(\"Who\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bo  = self.param(\"bo\", nn.initializers.zeros, (self.hidden_units,))\n\n        Wxc = self.param(\"Wxc\", nn.initializers.normal(), (self.input_dim, self.hidden_units))\n        Whc = self.param(\"Whc\", nn.initializers.normal(), (self.hidden_units, self.hidden_units))\n        bc  = self.param(\"bc\", nn.initializers.zeros, (self.hidden_units,))\n\n        dense = nn.Dense(features=1)\n\n        outputs = []\n        for t in range(seq_len):\n            X_t = inputs[:, t, :]  # (batch, input_dim)\n            I_t = jax.nn.sigmoid(jnp.dot(X_t, Wxi) + jnp.dot(H, Whi) + bi)\n            F_t = jax.nn.sigmoid(jnp.dot(X_t, Wxf) + jnp.dot(H, Whf) + bf)\n            O_t = jax.nn.sigmoid(jnp.dot(X_t, Wxo) + jnp.dot(H, Who) + bo)\n            C_tilde = jnp.tanh(jnp.dot(X_t, Wxc) + jnp.dot(H, Whc) + bc)\n            C = F_t * C + I_t * C_tilde\n            H = O_t * jnp.tanh(C)\n            outputs.append(H)\n        outputs = jnp.stack(outputs, axis=1)  # shape (batch, seq_len, hidden_units)\n        pred = dense(outputs)  # shape (batch, seq_len, 1)\n        return pred, (H, C)\n\n# ----------------------------\n# Built-In LSTM Model using Flax's LSTMCell\n# ----------------------------\nclass LSTMModel(nn.Module):\n    hidden_size: int = 50\n\n    @nn.compact\n    def __call__(self, inputs):\n        \"\"\"\n        inputs: shape (batch, seq_len, input_dim) where input_dim is assumed to be 1.\n        The LSTM cell is applied over the time axis; the final hidden state is passed through a Dense layer.\n        Returns:\n          out: predictions of shape (batch, 1)\n        \"\"\"\n        batch_size, seq_len, input_dim = inputs.shape\n        # Initialize carry for the LSTM cell.\n        rng = self.make_rng(\"lstm\")\n        key1, key2 = jax.random.split(rng)\n        carry = ( ## THIS IS WHERE THE FIX 2 IS\n            jax.random.normal(key1, (batch_size, self.hidden_size)),  # Hidden state\n            jax.random.normal(key2, (batch_size, self.hidden_size))   # Cell state\n        ) ## END FIX 2\n        # carry = nn.LSTMCell.initialize_carry(self.make_rng(\"lstm\"), (batch_size, input_dim), self.hidden_size) ## THIS IS WHERE THE FIX 1 IS\n        lstm_cell = nn.LSTMCell(features=self.hidden_size) ## THIS IS WHERE THE FIX 3 IS\n        # lstm_cell = nn.LSTMCell()\n        outputs = []\n        for t in range(seq_len):\n            carry, y = lstm_cell(carry, inputs[:, t, :])\n            outputs.append(y)\n        outputs = jnp.stack(outputs, axis=1)  # shape (batch, seq_len, hidden_size)\n        dense = nn.Dense(features=1)\n        out = dense(outputs[:, -1, :])  # use the last output for prediction\n        return out\n\n# ----------------------------\n# Training Utilities\n# ----------------------------\n# A simple TrainState to hold parameters and the optimizer state.\nclass TrainState(train_state.TrainState):\n    pass\n\n# Loss functions\n\ndef loss_fn_custom(params, rng, batch_x, batch_y):\n    # Model returns a sequence of predictions; we use the last time-step.\n    pred, _ = custom_model.apply({\"params\": params}, batch_x, rngs={\"lstm\": rng})\n    pred_last = pred[:, -1, 0]  # shape (batch,)\n    loss = jnp.mean((pred_last - batch_y.squeeze()) ** 2)\n    return loss\n\ndef loss_fn_builtin(params, rng, batch_x, batch_y):\n    pred = lstm_model.apply({\"params\": params}, batch_x, rngs={\"lstm\": rng})\n    pred = pred.squeeze()  # shape (batch,)\n    loss = jnp.mean((pred - batch_y.squeeze()) ** 2)\n    return loss\n\n# Training steps (jitted)\n@jax.jit\ndef train_step_custom(state, batch_x, batch_y, rng):\n    loss, grads = jax.value_and_grad(loss_fn_custom)(state.params, rng, batch_x, batch_y)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\n@jax.jit\ndef train_step_builtin(state, batch_x, batch_y, rng):\n    loss, grads = jax.value_and_grad(loss_fn_builtin)(state.params, rng, batch_x, batch_y)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\n# ----------------------------\n# Initialize Models and Optimizers\n# ----------------------------\n# Create instances of both models.\ncustom_model = CustomLSTMModel(input_dim=1, hidden_units=50)\nlstm_model = LSTMModel(hidden_size=50)\n\n# Initialize parameters by “calling” the model once with a sample input.\ndummy_input_custom = jnp.ones((X_seq.shape[0], sequence_length, 1))\ndummy_input_builtin = jnp.ones((X_seq.shape[0], sequence_length, 1))\n\nrng_custom = jax.random.PRNGKey(0)\nrng_builtin = jax.random.PRNGKey(1)\n\nparams_custom = custom_model.init({\"params\": rng_custom, \"lstm\": rng_custom}, dummy_input_custom)[\"params\"]\nparams_builtin = lstm_model.init({\"params\": rng_builtin, \"lstm\": rng_builtin}, dummy_input_builtin)[\"params\"]\n\n# Create training states with Adam optimizer (learning rate = 0.01).\noptimizer = optax.adam(0.01)\nstate_custom = TrainState.create(apply_fn=custom_model.apply, params=params_custom, tx=optimizer)\nstate_builtin = TrainState.create(apply_fn=lstm_model.apply, params=params_builtin, tx=optimizer)\n\n# ----------------------------\n# Training Loop for Custom LSTM Model\n# ----------------------------\nepochs = 500\nrng = jax.random.PRNGKey(42)\nfor epoch in range(epochs):\n    rng, step_rng = jax.random.split(rng)\n    state_custom, loss_value = train_step_custom(state_custom, X_seq, y_seq, step_rng)\n    if (epoch + 1) % 50 == 0:\n        print(f\"[Custom LSTM] Epoch [{epoch + 1}/{epochs}], Loss: {loss_value:.4f}\")\n\n# ----------------------------\n# Training Loop for Built-In LSTM Model\n# ----------------------------\nrng = jax.random.PRNGKey(100)\nfor epoch in range(epochs):\n    rng, step_rng = jax.random.split(rng)\n    state_builtin, loss_value = train_step_builtin(state_builtin, X_seq, y_seq, step_rng)\n    if (epoch + 1) % 50 == 0:\n        print(f\"[Built-In LSTM] Epoch [{epoch + 1}/{epochs}], Loss: {loss_value:.4f}\")\n\n# ----------------------------\n# Testing on New Data\n# ----------------------------\ntest_steps = 100  # should be greater than sequence_length\nX_test = np.linspace(0, 5 * np.pi, test_steps).reshape(-1, 1)\ny_test = np.sin(X_test)\nX_test_seq, _ = create_in_out_sequences(y_test, sequence_length)  # (test_steps-seq_length, seq_length, 1)\n\n# Get predictions from both models.\nrng, custom_test_rng = jax.random.split(rng)\npred_custom, _ = custom_model.apply({\"params\": state_custom.params}, X_test_seq, rngs={\"lstm\": custom_test_rng})\n# Use the last time-step predictions (flattened)\npred_custom = pred_custom[:, -1, 0]\n\nrng, builtin_test_rng = jax.random.split(rng)\npred_builtin = lstm_model.apply({\"params\": state_builtin.params}, X_test_seq, rngs={\"lstm\": builtin_test_rng})\npred_builtin = pred_builtin.squeeze()\n\nprint(\"Predictions with Custom Model for new sequence:\")\nprint(np.array(pred_custom))\nprint(\"\\nPredictions with Built-In Model:\")\nprint(np.array(pred_builtin))\n\n# ----------------------------\n# Plot the predictions\n# ----------------------------\nplt.figure(figsize=(8, 4))\nplt.plot(pred_custom, label=\"Custom LSTM Model\")\nplt.plot(pred_builtin, label=\"Built-In LSTM Model\")\nplt.legend()\nplt.title(\"Predictions on New Sine Wave Sequence\")\nplt.xlabel(\"Time step\")\nplt.ylabel(\"Predicted value\")\nplt.show()\n",
        "Errors": [
          {
            "Error_Code": "carry = nn.LSTMCell.initialize_carry(self.make_rng('lstm'), (batch_size,), self.hidden_size)",
            "Error": "TypeError: 'int' object is not subscriptable",
            "Fix_info": "Change input_shape to (batch_size,), which correctly specifies the batch dimension. The feature dimension is not needed here, as nn.LSTMCell handles the input dimension (input_dim=1) via the input tensor inputs[:, t, :].",
            "Fixed_Code": ""
          },
          {
            "Error_Code": "carry = nn.LSTMCell.initialize_carry(self.make_rng('lstm'), (batch_size, input_dim), self.hidden_size)",
            "Error": "TypeError: 'int' object is not subscriptable",
            "Fix_info": "Replace nn.LSTMCell.initialize_carry with manual initialization of the LSTM carry state. The carry is a tuple (h, c) of hidden and cell states, each of shape (batch_size, hidden_size). Then use jax.random.normal to initialize these states, mirroring the behavior of initialize_carry.",
            "Fixed_Code": "carry = (jax.random.normal(key1, (batch_size, self.hidden_size)), jax.random.normal(key2, (batch_size, self.hidden_size)))"
          },
          {
            "Error_Code": "lstm_cell = nn.LSTMCell()",
            "Error": "TypeError: LSTMCell.__init__() missing 1 required positional argument: 'features'",
            "Fix_info": "Add features=self.hidden_size to nn.LSTMCell initialization.",
            "Fixed_Code": "lstm_cell = nn.LSTMCell(features=self.hidden_size)"
          }
        ]
      },
      {
        "Example_id": "h9",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Define a simple model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.fc = nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.fc(x)\n\n# Generate synthetic data\nX = torch.randn(1000, 10)\ny = torch.randn(1000, 1)\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Initialize model, loss function, and optimizer\nmodel = SimpleModel().cuda()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Enable mixed precision training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\nepochs = 5\nfor epoch in range(epochs):\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass under autocast\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n        # Backward pass with scaled gradients\n        optimizer.zero_grad()\n        # TODO: Set scaler\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n\n# Test the model on new data\nX_test = torch.randn(5, 10).cuda()\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    predictions = model(X_test)\n    print(\"Predictions:\", predictions)\n\n# Test the model on new data\nX_test = torch.randn(5, 10).cuda()\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    predictions = model(X_test)\n    print(\"Predictions:\", predictions)",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nfrom jax import random, value_and_grad, jit\nimport optax\nimport numpy as np\n\ndef init_params(key, in_dim=10, out_dim=1, dtype=jnp.float16):\n    k1, k2 = random.split(key)\n    w = random.normal(k1, (in_dim, out_dim), dtype=dtype)\n    b = jnp.zeros((out_dim,), dtype=dtype)\n    return {\"w\": w, \"b\": b}\n\ndef forward(params, x):\n    x = x.astype(jnp.float16)\n    y_hat = x @ params[\"w\"] + params[\"b\"]\n    return y_hat\n\ndef make_dataset(n_samples=1000, in_dim=10, out_dim=1, batch_size=32, key=0):\n    key = random.PRNGKey(key)\n    X = random.normal(key, (n_samples, in_dim), dtype=jnp.float32)\n    y = random.normal(key, (n_samples, out_dim), dtype=jnp.float32)\n    X, y = np.array(X), np.array(y)\n\n    def loader():\n        idxs = np.random.permutation(len(X))\n        for i in range(0, len(X), batch_size):\n            batch_idx = idxs[i : i + batch_size]\n            yield X[batch_idx], y[batch_idx]\n\n    return loader\n\ndataloader = make_dataset()\n\ndef mse(preds, targets):\n    return jnp.mean((preds - targets) ** 2)\n\nclass GradScaler:\n    def __init__(self, init_scale=2.**15, growth_factor=2., backoff_factor=0.5, growth_interval=2000):\n        self.scale = init_scale\n        self.growth_factor = growth_factor\n        self.backoff_factor = backoff_factor\n        self.growth_interval = growth_interval\n        self._skip = 0\n\n    def unscale_and_check(self, grads):\n        inv_scale = 1. / self.scale\n        grads = jax.tree_map(lambda g: g * inv_scale, grads)\n\n        def _has_inf_or_nan(x):\n            return jnp.any(~jnp.isfinite(x))\n\n        overflow = jax.tree_util.tree_reduce(\n            lambda a, b: jnp.logical_or(a, _has_inf_or_nan(b)), grads, False\n        )\n        return grads, overflow\n\n    def update(self, overflow):\n        if overflow:\n            self.scale = max(self.scale * self.backoff_factor, 1.)\n            self._skip = 0\n        else:\n            self._skip += 1\n            if self._skip % self.growth_interval == 0:\n                self.scale *= self.growth_factor\n\nlr = 1e-3\noptimizer = optax.adam(lr)\nkey = random.PRNGKey(42)\nparams = init_params(key)\nopt_state = optimizer.init(params)\nscaler = GradScaler()\n\n@jit\ndef train_step(params, opt_state, x, y, scale):\n    def loss_fn(p, bx, by):\n        preds = forward(p, bx)\n        return mse(preds.astype(jnp.float32), by.astype(jnp.float32))\n\n    scaled_loss_fn = lambda p: loss_fn(p, x, y) * scale\n    loss_val, grads = value_and_grad(scaled_loss_fn)(params)\n    return loss_val, grads\n\nepochs = 5\nfor epoch in range(epochs):\n    for xb, yb in dataloader():\n        xb = jnp.asarray(xb)\n        yb = jnp.asarray(yb)\n        loss_val, grads = train_step(params, opt_state, xb, yb, scaler.scale)\n        grads, found_inf = scaler.unscale_and_check(grads)\n        if not found_inf:\n            updates, opt_state = optimizer.update(grads, opt_state, params)\n            params = optax.apply_updates(params, updates)\n        scaler.update(found_inf)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {(loss_val / scaler.scale):.4f}\")\n\nkey_pred = random.PRNGKey(2025)\nX_test = random.normal(key_pred, (5, 10), dtype=jnp.float32)\npredictions = forward(params, X_test).astype(jnp.float32)\nprint(\"Predictions:\", np.array(predictions))",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nfrom jax import random, value_and_grad, jit\nimport optax\nimport numpy as np\n\ndef init_params(key, in_dim=10, out_dim=1, dtype=jnp.float16):\n    k1, k2 = random.split(key)\n    w = random.normal(k1, (in_dim, out_dim), dtype=dtype)\n    b = jnp.zeros((out_dim,), dtype=dtype)\n    return {\"w\": w, \"b\": b}\n\ndef forward(params, x):\n    x = x.astype(jnp.float16)\n    y_hat = x @ params[\"w\"] + params[\"b\"]\n    return y_hat\n\ndef make_dataset(n_samples=1000, in_dim=10, out_dim=1, batch_size=32, key=0):\n    key = random.PRNGKey(key)\n    X = random.normal(key, (n_samples, in_dim), dtype=jnp.float32)\n    y = random.normal(key, (n_samples, out_dim), dtype=jnp.float32)\n    X, y = np.array(X), np.array(y)\n\n    def loader():\n        idxs = np.random.permutation(len(X))\n        for i in range(0, len(X), batch_size):\n            batch_idx = idxs[i : i + batch_size]\n            yield X[batch_idx], y[batch_idx]\n\n    return loader\n\ndataloader = make_dataset()\n\ndef mse(preds, targets):\n    return jnp.mean((preds - targets) ** 2)\n\nclass GradScaler:\n    def __init__(self, init_scale=2.**15, growth_factor=2., backoff_factor=0.5, growth_interval=2000):\n        self.scale = init_scale\n        self.growth_factor = growth_factor\n        self.backoff_factor = backoff_factor\n        self.growth_interval = growth_interval\n        self._skip = 0\n\n    def unscale_and_check(self, grads):\n        inv_scale = 1. / self.scale\n        grads = jax.tree_map(lambda g: g * inv_scale, grads)\n\n        def _has_inf_or_nan(x):\n            return jnp.any(~jnp.isfinite(x))\n\n        overflow = jax.tree_util.tree_reduce(\n            lambda a, b: jnp.logical_or(a, _has_inf_or_nan(b)), grads, False\n        )\n        return grads, overflow\n\n    def update(self, overflow):\n        if overflow:\n            self.scale = max(self.scale * self.backoff_factor, 1.)\n            self._skip = 0\n        else:\n            self._skip += 1\n            if self._skip % self.growth_interval == 0:\n                self.scale *= self.growth_factor\n\nlr = 1e-3\noptimizer = optax.adam(lr)\nkey = random.PRNGKey(42)\nparams = init_params(key)\nopt_state = optimizer.init(params)\nscaler = GradScaler()\n\n@jit\ndef train_step(params, opt_state, x, y, scale):\n    def loss_fn(p, bx, by):\n        preds = forward(p, bx)\n        return mse(preds.astype(jnp.float32), by.astype(jnp.float32))\n\n    scaled_loss_fn = lambda p: loss_fn(p, x, y) * scale\n    loss_val, grads = value_and_grad(scaled_loss_fn)(params)\n    return loss_val, grads\n\nepochs = 5\nfor epoch in range(epochs):\n    for xb, yb in dataloader():\n        xb = jnp.asarray(xb)\n        yb = jnp.asarray(yb)\n        loss_val, grads = train_step(params, opt_state, xb, yb, scaler.scale)\n        grads, found_inf = scaler.unscale_and_check(grads)\n        if not found_inf:\n            updates, opt_state = optimizer.update(grads, opt_state, params)\n            params = optax.apply_updates(params, updates)\n        scaler.update(found_inf)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {(loss_val / scaler.scale):.4f}\")\n\nkey_pred = random.PRNGKey(2025)\nX_test = random.normal(key_pred, (5, 10), dtype=jnp.float32)\npredictions = forward(params, X_test).astype(jnp.float32)\nprint(\"Predictions:\", np.array(predictions))",
        "Errors": []
      },
      {
        "Example_id": "h10",
        "Input_Code": "import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load a pre-trained ResNet model\nmodel = models.resnet18(pretrained=True)\nmodel.eval()\n\n# Define variables to capture gradients and activations\ngradients = None\nactivations = None\n\n# Define hooks to capture gradients and activations\ndef save_gradients(module, grad_in, grad_out):\n    global gradients\n    gradients = grad_out[0]\n\ndef save_activations(module, input, output):\n    global activations\n    activations = output\n\n# Attach hooks to the target layer\ntarget_layer = model.layer4[1].conv2\ntarget_layer.register_backward_hook(save_gradients)\ntarget_layer.register_forward_hook(save_gradations)\n\n# Fetch a sample image from torchvision datasets\ndataset = datasets.FakeData(transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n]))\nimage, _ = dataset[0]\nimage = transforms.ToPILImage()(image)\n\n# Preprocess the image for the model\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\ninput_tensor = preprocess(image).unsqueeze(0)\n\n# Perform a forward pass\noutput = model(input_tensor)\npredicted_class = output.argmax(dim=1).item()\n\n# Perform a backward pass for the predicted class\nmodel.zero_grad()\noutput[0, predicted_class].backward()\n\n# Generate Grad-CAM heatmap\nweights = gradients.mean(dim=[2, 3], keepdim=True)\nheatmap = (weights * activations).sum(dim=1).squeeze().relu()\n\n# Normalize the heatmap and overlay it on the original image\nheatmap = heatmap / heatmap.max()\nheatmap = transforms.ToPILImage()(heatmap.cpu())\nheatmap = heatmap.resize(image.size, resample=Image.BILINEAR)\n\n# Display the image with the Grad-CAM heatmap\nplt.imshow(image)\nplt.imshow(heatmap, alpha=0.5, cmap='jet')\nplt.title(f\"Predicted Class: {predicted_class}\")\nplt.axis('off')\nplt.show()",
        "LLM_strong_output": "import jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nimport numpy as np\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom jax import random\n\n# Define a simplified ResNet-like model in Flax\nclass ResNet(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(64, (7, 7), strides=(2, 2), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, (3, 3), strides=(2, 2), padding='SAME')\n        x = nn.Conv(512, (3, 3), padding='SAME', name='layer4_conv2')(x)\n        self.sow('intermediates', 'activations', x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, (7, 7), padding='SAME')\n        x = x.reshape((x.shape[0], -1))\n        x = nn.Dense(1000, kernel_init=nn.initializers.normal(stddev=0.01))(x)\n        return x\n\n# Function to compute gradients\ndef compute_gradients(params, inputs, target_class):\n    def forward_fn(inputs):\n        model = ResNet()\n        logits, state = model.apply({'params': params}, inputs, mutable=['intermediates'])\n        return logits[0, target_class], state['intermediates']['activations']\n    \n    grad_fn = jax.grad(forward_fn, has_aux=True)\n    (grads, activations) = grad_fn(inputs)\n    return grads, activations\n\ndef main():\n    key = random.PRNGKey(0)\n    dataset = datasets.FakeData(transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]))\n    image, _ = dataset[0]\n    image_pil = transforms.ToPILImage()(image)\n\n    preprocess = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    input_tensor = preprocess(image_pil).unsqueeze(0)\n    input_tensor = jnp.array(input_tensor)\n\n    model = ResNet()\n    params = model.init(key, input_tensor)['params']\n\n    logits, state = model.apply({'params': params}, input_tensor, mutable=['intermediates'])\n    predicted_class = jnp.argmax(logits, axis=1).item()\n    activations = state['intermediates']['activations']\n\n    gradients, _ = compute_gradients(params, input_tensor, predicted_class)\n\n    weights = jnp.mean(gradients, axis=(2, 3), keepdims=True)\n    heatmap = jnp.sum(weights * activations[0], axis=1).squeeze()\n    heatmap = jax.nn.relu(heatmap)\n\n    heatmap = heatmap / jnp.max(heatmap + 1e-8)\n    heatmap_np = np.array(heatmap)\n    heatmap_pil = Image.fromarray((heatmap_np * 255).astype(np.uint8)).resize(image_pil.size, resample=Image.BILINEAR)\n\n    plt.imshow(image_pil)\n    plt.imshow(heatmap_pil, alpha=0.5, cmap='jet')\n    plt.title(f\"Predicted Class: {predicted_class}\")\n    plt.axis('off')\n    plt.savefig('gradcam_output.png')\n\nif __name__ == \"__main__\":\n    main()",
        "LLM_fix_strong_output": "import jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nimport numpy as np\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom jax import random\n\nclass ResNet(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(64, (7, 7), strides=(2, 2), padding='SAME')(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, (3, 3), strides=(2, 2), padding='SAME')\n        x = nn.Conv(512, (3, 3), padding='SAME', name='layer4_conv2')(x)\n        self.sow('intermediates', 'activations', x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, (7, 7), padding='SAME')\n        x = x.reshape((x.shape[0], -1))\n        x = nn.Dense(1000, kernel_init=nn.initializers.normal(stddev=0.01))(x)\n        return x\n\ndef compute_gradients(params, inputs, target_class):\n    def forward_fn(inputs):\n        model = ResNet()\n        logits, state = model.apply({'params': params}, inputs, mutable=['intermediates'])\n        return logits[0, target_class], state['intermediates']['activations']\n    \n    grad_fn = jax.grad(forward_fn, has_aux=True)\n    (grads, activations) = grad_fn(inputs)\n    return grads, activations\n\ndef main():\n    key = random.PRNGKey(0)\n    dataset = datasets.FakeData(transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]))\n    image, _ = dataset[0]\n    image_pil = transforms.ToPILImage()(image)\n\n    preprocess = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    input_tensor = preprocess(image_pil).unsqueeze(0)\n    input_tensor = jnp.array(input_tensor)\n\n    model = ResNet()\n    params = model.init(key, input_tensor)['params']\n\n    logits, state = model.apply({'params': params}, input_tensor, mutable=['intermediates'])\n    predicted_class = jnp.argmax(logits, axis=1).item()\n    activations = state['intermediates']['activations']\n\n    gradients, _ = compute_gradients(params, input_tensor, predicted_class)\n\n    weights = jnp.mean(gradients, axis=(2, 3), keepdims=True)\n    heatmap = jnp.sum(weights * activations[0], axis=1).squeeze()\n    heatmap = jax.nn.relu(heatmap)\n\n    heatmap = heatmap / jnp.max(heatmap + 1e-8)\n    heatmap_np = np.array(heatmap)\n    heatmap_pil = Image.fromarray((heatmap_np * 255).astype(np.uint8)).resize(image_pil.size, resample=Image.BILINEAR)\n\n    plt.imshow(image_pil)\n    plt.imshow(heatmap_pil, alpha=0.5, cmap='jet')\n    plt.title(f\"Predicted Class: {predicted_class}\")\n    plt.axis('off')\n    plt.savefig('gradcam_output.png')\n\nif __name__ == \"__main__\":\n    main()",
        "Errors": [
          {
            "Error_Code": "features = model.extract_features({'params': params}, x) variables = model.init(key, dummy_input)",
            "Error": "ScopeCollectionNotFound: Tried to access 'mean' from collection 'batch_stats' in '/BatchNorm_0' but the collection is empty.",
            "Fix_info": "Load the pre-trained flaxmodels.ResNet18 model with its full state (including params and batch_stats) using the flaxmodels API, and pass both to model.apply.",
            "Fixed_Code": ""
          },
          {
            "Error_Code": "params = model.param_dict",
            "Error": "AttributeError: 'ResNet' object has no attribute 'param_dict'.",
            "Fix_info": "Use model.init with a dummy input to provide both params and batch_stats.",
            "Fixed_Code": "params = variables['params']"
          },
          {
            "Error_Code": "variables = model.init(key, dummy_input, mutable=['batch_stats']) params = variables['params']",
            "Error": "ScopeCollectionNotFound: Tried to access 'kernel' from collection 'params' in '/Conv_0' but the collection is empty.",
            "Fix_info": "Remove the model.init and load the pre-trained parameters and batch_stats directly from the flaxmodels.ResNet18 model's pretrained state using param_dict.",
            "Fixed_Code": "variables = model.init(key, dummy_input, train=False) params = variables['params'] batch_stats = variables.get('batch_stats', {})"
          },
          {
            "Error_Code": "predicted_class = int(jnp.argmax(logits, axis=-1)[0])",
            "Error": "TypeError: argmax requires ndarray or scalar arguments, got <class 'tuple'> at position 0",
            "Fix_info": "Update the forward pass to handle the tuple output from model.apply.",
            "Fixed_Code": "outputs = model.apply({'params': params, 'batch_stats': batch_stats}, input_tensor, mutable=['batch_stats'], train=False) logits = outputs[0] predicted_class = int(jnp.argmax(logits, axis=-1)[0])"
          },
          {
            "Error_Code": "target_activation = feature_extractor(params, batch_stats, input_tensor)",
            "Error": "TypeError: feature_extractor() takes 2 positional arguments but 3 were given",
            "Fix_info": "Modify the feature_extractor function to accept batch_stats.",
            "Fixed_Code": "def feature_extractor(params, batch_stats, x):"
          },
          {
            "Error_Code": "features = model.extract_features({'params': params, 'batch_stats': batch_stats}, x, mutable=['batch_stats'])[0]",
            "Error": "AttributeError: 'ResNet' object has no attribute 'extract_features'.",
            "Fix_info": "PLEASE NOTE THAT THIS DOES NOT HAVE A FIX YET",
            "Fixed_Code": ""
          },
          {
            "Error_Code": "variables = model.init(rng, jnp.zeros((1, 3, 224, 224)))",
            "Error": "TypeError: sub got incompatible shapes for broadcasting: (1, 3, 224, 224), (1, 1, 1, 3)",
            "Fix_info": "flaxmodels ImageNet ResNet-18 expects channel-last inputs — shape (B, H, W, 3). Apply the Fixed_Code.",
            "Fixed_Code": "variables = model.init(rng, jnp.zeros((1, 224, 224, 3))) x = jnp.asarray(img_tensor.unsqueeze(0).permute(0, 2, 3, 1).numpy())"
          },
          {
            "Error_Code": "logits, activations = forward_apply(params, x, train=False, capture_fn=capture_conv2) logits, intermediates = model.apply({'params': params}, x,",
            "Error": "TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].",
            "Fix_info": "Mark the train flag as static in the jit decorator to remove the trace error.",
            "Fixed_Code": ""
          },
          {
            "Error_Code": "logits, activations = forward_apply(params, x, train=False, capture_fn=capture_conv2)",
            "Error": "ScopeCollectionNotFound: Tried to access 'mean' from collection 'batch_stats' in '/BatchNorm_0' but the collection is empty.",
            "Fix_info": "Keep the batch_stats from model.init and pass them into every model.apply call.",
            "Fixed_Code": "params = variables['params'] batch_stats = variables['batch_stats'] def forward_apply(params, batch_stats, x, train, capture_fn): logits, inter = model.apply({'params': params, 'batch_stats': batch_stats}"
          },
          {
            "Error_Code": "act_tree = intermediates[target_path] logits, activations = forward_apply(params, x, train=False, capture_fn=capture_conv2)",
            "Error": "KeyError: ('layer4_1', 'Conv_1')",
            "Fix_info": "flaxmodels.ResNet18 uses a different layer path naming convention, and the capture_intermediates=True mechanism is not exposing the expected 512-channel convolutional layer.",
            "Fixed_Code": "NO FIX"
          },
          {
            "Error_Code": "x = nn.Dense(1000)(x)",
            "Error": "ZeroDivisionError: division by zero",
            "Fix_info": "Ensure the input to nn.Dense has a valid shape by adjusting the pooling or convolution layers.",
            "Fixed_Code": "x = nn.avg_pool(x, (3, 3), strides=(2, 2), padding='SAME') x = nn.avg_pool(x, (7, 7), padding='SAME') x = nn.Dense(1000, kernel_init=nn.initializers.normal(stddev=0.01))(x)"
          },
          {
            "Error_Code": "heatmap = jnp.sum(weights * activations, axis=1).squeeze()",
            "Error": "TypeError: unsupported operand type(s) for *: 'ArrayImpl' and 'tuple'",
            "Fix_info": "Extract the correct JAX array from the activations tuple before performing the multiplication.",
            "Fixed_Code": "heatmap = jnp.sum(weights * activations[0], axis=1).squeeze()"
          }
        ]
      }
    ]
  }
