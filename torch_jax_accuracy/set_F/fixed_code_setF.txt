# -------------------------------
# Example ID = "h6"
# -------------------------------

import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.core import freeze, unfreeze
from flax.training import train_state
import optax
import numpy as np
import os  # THIS IS WHERE THE FIX IS: Added import for os to handle absolute paths

def one_hot(labels, num_classes):
    return jnp.eye(num_classes, dtype=jnp.float32)[labels]

def cross_entropy_with_integer_labels(logits, labels):
    """Equivalent to nn.CrossEntropyLoss in PyTorch (averaged over batch)."""
    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()

class LanguageModel(nn.Module):
    vocab_size:  int          # ← dataclass fields
    embed_size:  int
    hidden_size: int
    num_layers:  int = 1
    @nn.compact
    def __call__(self, x, *, train: bool = True):
        emb = nn.Embed(self.vocab_size, self.embed_size)(x)        # (B,T,E)

        batch_size = x.shape[0] ## THIS IS FIX 1
        rng = jax.random.PRNGKey(0)
        key1, key2 = jax.random.split(rng)
        carry = (
            jax.random.normal(key1, (batch_size, self.hidden_size)),  # Hidden state
            jax.random.normal(key2, (batch_size, self.hidden_size))   # Cell state
        )
        # carry = nn.LSTMCell.initialize_carry(##END OF FIX 1
        #     jax.random.PRNGKey(0),
        #     (batch_size,), self.hidden_size
        # )
        lstm_cell = nn.scan(
            nn.LSTMCell,## THIS IS WHERE THE FIX IS
            variable_broadcast='params',
            split_rngs={'params': False},
            in_axes=1, out_axes=1, length=x.shape[1])(features=self.hidden_size, name='lstm') ## THIS IS WHERE THE FIX IS
        carry, lstm_out = lstm_cell(carry, emb)
        last_h = lstm_out[:, -1, :]
        logits = nn.Dense(self.vocab_size)(last_h)
        probs  = nn.softmax(logits)
        return logits, probs
# tiny weight quantiser 
def quantize_params(params):
    """Post‑training symmetric int8 weight quantisation (per‑tensor)."""
    qparams = {}
    for k, v in params.items():
        if isinstance(v, dict):          # recurse into sub‑dicts
            qparams[k] = quantize_params(v)
        else:
            scale = jnp.max(jnp.abs(v)) / 127.  # symmetric int8
            qparams[k] = {
                'int8': jnp.round(v / scale).astype(jnp.int8),
                'scale': scale.astype(jnp.float32)
            }
    return qparams

def dequantize_params(qparams):
    params = {}
    for k, v in qparams.items():
        if isinstance(v, dict) and 'int8' not in v:
            params[k] = dequantize_params(v)
        else:
            params[k] = v['int8'].astype(jnp.float32) * v['scale']
    return params

seed = 42
rng = jax.random.PRNGKey(seed)

vocab_size  = 50
seq_length  = 10
batch_size  = 32
embed_size  = 64
hidden_size = 128
num_layers  = 2
epochs      = 5
lr          = 1e-3

# Synthetic data (match PyTorch’s manual_seed(42) outcome for repeatability)
np.random.seed(seed)
X_train = np.random.randint(0, vocab_size, (batch_size, seq_length))
y_train = np.random.randint(0, vocab_size, (batch_size,))

## THIS IS FIX 2
model = LanguageModel(
    vocab_size=vocab_size,
    embed_size=embed_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
)

def create_train_state(rng):
    params = model.init(rng, jnp.ones((1, seq_length), dtype=jnp.int32))[ 'params']
    optimiser = optax.adam(lr)
    return train_state.TrainState.create(apply_fn=model.apply,
                                         params=params,
                                         tx=optimiser)

state = create_train_state(rng)

@jax.jit
def train_step(state, batch_x, batch_y):
    def loss_fn(params):
        logits, _ = state.apply_fn({'params': params},
                                   batch_x, train=True)
        loss = cross_entropy_with_integer_labels(logits, batch_y)
        return loss
    grad_fn = jax.value_and_grad(loss_fn)
    loss, grads = grad_fn(state.params)
    new_state  = state.apply_gradients(grads=grads)
    return new_state, loss

training loop --
for epoch in range(epochs):
    state, loss = train_step(state,
                             jnp.array(X_train, dtype=jnp.int32),
                             jnp.array(y_train, dtype=jnp.int32))
    print(f"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item():.4f}")
“dynamic” quantisation --
quantised_params = quantize_params(state.params)
flax.serialization:
import orbax.checkpoint as ocp         # optional, but very handy

checkpointer = ocp.PyTreeCheckpointer() ## THIS IS WHERE THE FIX IS
save_path = os.path.abspath("quantised_language_model")
# save_path = "quantised_language_model"
# To reload later:
restored_qparams = checkpointer.restore(save_path)      # dict with int8 + scale
restored_params  = freeze(dequantize_params(restored_qparams))
inference / test ----
model_apply = jax.jit(model.apply)

test_input = np.random.randint(0, vocab_size, (1, seq_length))
_, probs = model_apply({'params': restored_params},
                       jnp.array(test_input, dtype=jnp.int32),
                       train=False)

prediction = int(jnp.argmax(probs, axis=1)[0])
print(f"Prediction for input {test_input.tolist()}: {prediction}")


# -------------------------------
# Example ID = "m4"
# -------------------------------

import jax
import jax.numpy as jnp
from flax import linen as nn
import optax
from flax.training import train_state

# Synthetic Data Generation
batch = 5 ## THIS IS WHERE FIX 2 IS
num_slices = 10
channels = 3
width = 256
height = 256

# Create PRNG keys
key = jax.random.PRNGKey(42)
key, subkey1, subkey2 = jax.random.split(key, 3)

# Generate synthetic CT images and binary segmentation masks
ct_images = jax.random.normal(subkey1, (batch, num_slices, channels, width, height))
segmentation_masks = (jax.random.normal(subkey2, (batch, num_slices, 1, width, height)) > 0).astype(jnp.float32)

print("CT images (train examples) shape:", ct_images.shape)
print("Segmentation binary masks (labels) shape:", segmentation_masks.shape)

# Dummy ResNet18 Backbone Definition
class DummyResNet18(nn.Module):
    """
    Mimics a truncated ResNet18:
    Given an input of shape [B*D, 3, 256, 256],
    returns a feature map of shape [B*D, 512, new_w, new_h],
    where new_w and new_h are ~8 (downsampling via stride=32).
    """
    @nn.compact
    def __call__(self, x):
        # Using a single convolution with stride 32 to simulate the downsampling:
        x = nn.Conv(features=512,
                    kernel_size=(7, 7),
                    strides=(32, 32),
                    padding='SAME')(x)
        x = nn.relu(x)
        return x

# MedCNN Model Definition in Flax
class MedCNN(nn.Module):
    backbone: nn.Module
    out_channel: int = 1

    @nn.compact
    def __call__(self, x):
        # x shape: [B, D, C, W, H]
        b, d, c, w, h = x.shape
        print("Input shape [B, D, C, W, H]:", (b, d, c, w, h))

        # Reshape for backbone (2D convolutions): [B*D, C, W, H]
        x = x.reshape((b * d, c, w, h))
        features = self.backbone(x)
        print("Backbone (ResNet) output shape [B*D, C, W, H]:", features.shape)

        # Get new dimensions and reshape back: [B, D, new_c, new_w, new_h]
        _, new_c, new_w, new_h = features.shape
        x = features.reshape((b, d, new_c, new_w, new_h))
        # Permute to [B, new_c, D, new_w, new_h] for 3D convolutions
        x = jnp.transpose(x, (0, 2, 1, 3, 4))
        print("Reshaped for 3D conv [B, C, D, W, H]:", x.shape)

        # Define dimension numbers for 3D convolutions (NCDHW ordering)
        dim3d = ("NCDHW", "OIDHW", "NCDHW")

        # Downsampling 3D convolutions:
        x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME')(x) ## THIS IS WHERE FIX 1 IS
        x = nn.relu(x)
        print("After 3D Conv #1:", x.shape)

        x = nn.Conv(features=64, kernel_size=(3, 3, 3), padding='SAME')(x) ## THIS IS WHERE FIX 1 IS
        x = nn.relu(x)
        print("After 3D Conv #2:", x.shape)

        # Upsampling 3D transposed convolutions:
        x = nn.ConvTranspose(features=32, kernel_size=(1, 4, 4), strides=(1, 4, 4), 
                             padding='SAME')(x) ## THIS IS WHERE FIX 1 IS
        x = nn.relu(x)
        print("After 3D Transposed Conv #1:", x.shape)

        x = nn.ConvTranspose(features=16, kernel_size=(1, 8, 8), strides=(1, 8, 8),
                             padding='SAME')(x) ## THIS IS WHERE FIX 1 IS
        x = nn.relu(x)
        print("After 3D Transposed Conv #2:", x.shape)

        # Final segmentation layer (from 16 to 1 channel)
        x = nn.Conv(features=self.out_channel, kernel_size=(1, 1, 1), padding='SAME')(x) ## THIS IS WHERE FIX 1 IS
        x = jax.nn.sigmoid(x)
        print("Final output shape:", x.shape)
        return x

# Dice Loss Function
def compute_dice_loss(pred, labels, eps=1e-8):
    """
    Args:
      pred: [B, D, 1, W, H]
      labels: [B, D, 1, W, H]

    Returns:
      Dice coefficient (scalar)
    """
    numerator = 2 * jnp.sum(pred * labels)
    denominator = jnp.sum(pred) + jnp.sum(labels) + eps
    return numerator / denominator

# Instantiate the backbone and MedCNN model.
backbone = DummyResNet18()
model = MedCNN(backbone=backbone)

# Create a training state using Flax's TrainState and Optax's Adam optimizer.
class TrainState(train_state.TrainState):
    pass

# Initialize model parameters using the synthetic data.
rng = jax.random.PRNGKey(0)
params = model.init(rng, ct_images)
tx = optax.adam(learning_rate=0.01)
state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)

epochs = 5
for epoch in range(epochs):
    def loss_fn(params):
        pred = model.apply(params, ct_images)
        loss = compute_dice_loss(pred, segmentation_masks)
        return loss, pred

    # Compute loss and gradients.
    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
    # Update the model parameters.
    state = state.apply_gradients(grads=grads)
    print(f"Loss at epoch {epoch}: {loss}")


# -------------------------------
# Example ID = "m5"
# -------------------------------
import jax
import jax.numpy as jnp
from flax import linen as nn
import optax
import numpy as np

sequence_length = 10
num_samples = 100

# Create a sine wave dataset (using 4 * pi as the end point)
X = jnp.linspace(0, 4 * jnp.pi, num=num_samples)[:, None]
y = jnp.sin(X)

def create_in_out_sequences(data, seq_length):
    in_seq = []
    out_seq = []
    for i in range(len(data) - seq_length):
        in_seq.append(data[i:i + seq_length])
        out_seq.append(data[i + seq_length])
    return jnp.stack(in_seq), jnp.stack(out_seq)

X_seq, y_seq = create_in_out_sequences(y, sequence_length)

class RNNCell(nn.Module):
    hidden_size: int = 50

    @nn.compact
    def __call__(self, carry, x):
        # carry: previous hidden state, shape (batch, hidden_size)
        # x: current input, shape (batch, input_size)
        # Compute new hidden state: tanh(W_ih*x + W_hh*carry + b)
        new_h = nn.tanh(
            nn.Dense(self.hidden_size, name="ih")(x) +
            nn.Dense(self.hidden_size, use_bias=False, name="hh")(carry)
        )
        return new_h, new_h  # returning new state as both carry and output

class RNNModel(nn.Module):
    hidden_size: int = 50

    @nn.compact
    def __call__(self, x):
        # x shape: (batch, seq_length, input_size)
        batch_size = x.shape[0]
        init_carry = jnp.zeros((batch_size, self.hidden_size))
        # Instead of instantiating RNNCell, pass the class to nn.scan
        rnn_scan = nn.scan(
            RNNCell,  # pass the class instead of an instance
            in_axes=1,
            out_axes=1,
            variable_broadcast="params",
            split_rngs={"params": False},
        )(hidden_size=self.hidden_size)  # now provide the argument for hidden_size
        carry, ys = rnn_scan(init_carry, x)
        # Use the output at the final time step
        last_output = ys[:, -1, :]
        output = nn.Dense(1)(last_output)
        return output

# Initialize the Model and Optimizer
model = RNNModel()
rng = jax.random.PRNGKey(42)
# Sample input with shape (batch=1, seq_length, input_size)
sample_input = jnp.ones((1, sequence_length, 1))
params = model.init(rng, sample_input)

# Set up the Adam optimizer from Optax
optimizer = optax.adam(learning_rate=0.001)
opt_state = optimizer.init(params)

def loss_fn(params, x, y):
    preds = model.apply(params, x)
    return jnp.mean((preds - y) ** 2)

@jax.jit
def train_step(params, opt_state, x, y):
    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss

epochs = 500
for epoch in range(epochs):
    epoch_loss = 0.0
    # Loop over each sample (each sample has shape (sequence_length, 1))
    for seq, label in zip(X_seq, y_seq):
        # Add a batch dimension: new shape becomes (1, sequence_length, 1)
        seq = seq[None, :, :]
        label = label[None, :]
        params, opt_state, loss = train_step(params, opt_state, seq, label)
        epoch_loss += loss
    epoch_loss /= len(X_seq)
    if (epoch + 1) % 5 == 0 or epoch == 0:
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}")

# Testing on New Data
# Create new test data (from 4*pi to 5*pi)
X_test = jnp.linspace(4 * jnp.pi, 5 * jnp.pi, num=10)[:, None]
X_test = X_test[None, :, :]  # Add batch dimension: shape becomes (1, 10, 1)
predictions = model.apply(params, X_test)
print("Predictions for new sequence:", predictions)

# -------------------------------
# Example ID = "m6"
# -------------------------------
## 
import jax
import jax.numpy as jnp
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds

# Augmentation Functions in JAX
def random_flip(image, key):
    """Randomly flip the image horizontally with probability 0.5."""
    do_flip = jax.random.uniform(key) > 0.5
    return jnp.where(do_flip, jnp.flip(image, axis=1), image)

def random_crop(image, key, crop_size=32, padding=4):
    """Pad the image then randomly crop a patch of size (crop_size x crop_size)."""
    # Pad with 4 pixels on each side using reflection (mimics PyTorch padding behavior)
    image_padded = jnp.pad(image, ((padding, padding), (padding, padding), (0, 0)), mode='reflect')
    # The padded image has size 32+8=40; choose a random crop location in the padded image.
    max_offset = padding * 2  # 8
    offset_x = jax.random.randint(key, (), 0, max_offset + 1)
    offset_y = jax.random.randint(key, (), 0, max_offset + 1)
    cropped = image_padded[offset_x:offset_x+crop_size, offset_y:offset_y+crop_size, :]
    return cropped

def normalize(image):
    """Normalize image with mean=0.5 and std=0.5 (per channel)."""
    return (image - 0.5) / 0.5

def augment_image(image, key):
    """Apply the series of augmentations to a single image."""
    key_flip, key_crop = jax.random.split(key)
    image = random_flip(image, key_flip)
    image = random_crop(image, key_crop, crop_size=32, padding=4)
    image = normalize(image)
    return image

# Utility Function for Plotting
def imshow_grid(images):
    """Display a grid of images. Assumes images shape is (batch, H, W, C)."""
    # Unnormalize to bring pixel values back to [0, 1] for display
    images = images * 0.5 + 0.5
    batch, h, w, c = images.shape
    grid_cols = 8  # adjust as needed
    grid_rows = int(np.ceil(batch / grid_cols))
    grid = np.zeros((grid_rows * h, grid_cols * w, c))
    for idx, image in enumerate(images):
        row = idx // grid_cols
        col = idx % grid_cols
        grid[row*h:(row+1)*h, col*w:(col+1)*w, :] = image
    plt.figure(figsize=(grid_cols, grid_rows))
    plt.imshow(grid)
    plt.axis('off')
    plt.show()

# Dataset Loading with tfds
def load_dataset(split, batch_size=64):
    """
    Load CIFAR-10 from TensorFlow Datasets.
    The images are scaled to [0,1] and returned along with their labels.
    """
    ds = tfds.load('cifar10', split=split, as_supervised=True)

    def preprocess(image, label):
        image = tf.cast(image, tf.float32) / 255.0  # scale image to [0, 1]
        return image, label

    ds = ds.map(preprocess)
    ds = ds.cache()
    if split == 'train':
        ds = ds.shuffle(10000)
    ds = ds.batch(batch_size)
    ds = ds.prefetch(1)
    return tfds.as_numpy(ds)

# Load training and test datasets
train_ds = load_dataset('train', batch_size=64)
test_ds = load_dataset('test', batch_size=64)

# Process One Batch of Training Images

# Get one batch of training images and labels
batch = next(iter(train_ds))
images, labels = batch  # images shape: (64, 32, 32, 3)

# Apply augmentation to each image in the batch using separate random keys.
augmented_images = []
for i, image in enumerate(images):
    key = jax.random.PRNGKey(i)  # In practice, manage keys more carefully
    aug_img = augment_image(jnp.array(image), key)
    augmented_images.append(np.array(aug_img))
augmented_images = np.stack(augmented_images, axis=0)

imshow_grid(augmented_images)


# -------------------------------
# Example ID = "m7"
# -------------------------------
import time
import jax
import jax.numpy as jnp
import numpy as np
import tensorflow_datasets as tfds
import optax
from jax.example_libraries import stax
from jax.example_libraries.stax import Dense, Relu, Flatten

def preprocess(example):
    # Convert image to float32, scale to [0,1] then normalize to [-1,1] (like (x-0.5)/0.5)
    image = np.array(example['image'], dtype=np.float32) / 255.0
    image = (image - 0.5) / 0.5
    label = example['label']
    return image, label

def dataset_to_batches(ds, batch_size):
    # Convert the TFDS dataset to numpy arrays and create batches.
    ds = tfds.as_numpy(ds)
    images, labels = [], []
    for example in ds:
        img, lab = preprocess(example)
        images.append(img)
        labels.append(lab)
    images = np.stack(images)
    labels = np.array(labels)
    num_batches = images.shape[0] // batch_size
    batches = []
    for i in range(num_batches):
        batch_images = images[i*batch_size:(i+1)*batch_size]
        batch_labels = labels[i*batch_size:(i+1)*batch_size]
        batches.append((batch_images, batch_labels))
    return batches

batch_size = 64
train_ds = tfds.load('mnist', split='train', shuffle_files=True)
test_ds  = tfds.load('mnist', split='test',  shuffle_files=False)

train_batches = dataset_to_batches(train_ds, batch_size)
test_batches  = dataset_to_batches(test_ds, batch_size)

# Model Definition using stax
# Define a simple neural network that mirrors the PyTorch model:
# - Flatten the input (28x28 or 28x28x1)
# - Dense layer with 128 units and ReLU activation
# - Dense layer with 10 outputs (for the 10 classes)
init_random_params, predict = stax.serial(
    Flatten,
    Dense(128),
    Relu,
    Dense(10)
)

# Initialize model parameters. The expected input shape is (batch, 28, 28, 1).
rng = jax.random.PRNGKey(0)
_, params = init_random_params(rng, (-1, 28, 28, 1))

# Define the cross-entropy loss function. Note that optax.softmax_cross_entropy
# expects logits and one-hot encoded labels.
def loss_fn(params, batch):
    images, labels = batch
    logits = predict(params, images)
    one_hot = jax.nn.one_hot(labels, num_classes=10)
    loss = optax.softmax_cross_entropy(logits, one_hot).mean()
    return loss

# Use SGD optimizer with learning rate 0.01
optimizer = optax.sgd(learning_rate=0.01)
opt_state = optimizer.init(params)

# Define a single training step with JIT compilation.
@jax.jit
def train_step(params, opt_state, batch):
    loss, grads = jax.value_and_grad(loss_fn)(params, batch)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss

# Training Loop with Benchmarking
epochs = 5
for epoch in range(epochs):
    start_time = time.time()
    for batch in train_batches:
        params, opt_state, loss = train_step(params, opt_state, batch)
    end_time = time.time()
    print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}, Time: {end_time - start_time:.4f}s")

correct = 0
total = 0
start_time = time.time()
for batch in test_batches:
    images, labels = batch
    logits = predict(params, images)
    predictions = jnp.argmax(logits, axis=1)
    correct += int(jnp.sum(predictions == labels))
    total += images.shape[0]
end_time = time.time()
accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%, Testing Time: {end_time - start_time:.4f}s")

# -------------------------------
# Example ID = "m8"
# -------------------------------
import jax
import jax.numpy as jnp
from jax import random, jit, value_and_grad
import optax
import flax.linen as nn
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np

# Data Loading and Preprocessing
# The original PyTorch code applies ToTensor and Normalize((0.5,), (0.5,)).
# When MNIST images (originally uint8 in [0, 255]) are converted to float and normalized,
# the transformation is: x -> (x/255 - 0.5)/0.5 = 2*x/255 - 1.
# We'll do the same here.

def preprocess(example):
    image = example['image']  # shape (28, 28, 1) or (28, 28)
    # Convert to float32 in [0,1]
    image = tf.cast(image, tf.float32) / 255.0
    # Normalize to [-1, 1] as in PyTorch
    image = (image - 0.5) / 0.5
    # Ensure image has channel dimension (H, W, C)
    if image.shape.ndims == 2:
        image = tf.expand_dims(image, axis=-1)
    return {'image': image}

batch_size = 64
# Load and preprocess the training set
train_ds = tfds.load('mnist', split='train', shuffle_files=True)
train_ds = train_ds.map(preprocess)
train_ds = train_ds.shuffle(1024).batch(batch_size).prefetch(1)
# (For testing, a similar pipeline can be built from the 'test' split)

# Model Definition using Flax
class Autoencoder(nn.Module):
    @nn.compact
    def __call__(self, x):
        # Encoder
        x = nn.Conv(features=32, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')  # 28->14
        x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')  # 14->7

        # Decoder
        x = nn.ConvTranspose(features=32, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.ConvTranspose(features=1, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)
        x = nn.sigmoid(x)  # Keep output values between 0 and 1
        return x

# Initialize Model and Optimizer
rng = random.PRNGKey(0)
# Dummy input with shape [batch, height, width, channels]; note that Flax defaults to NHWC.
dummy_input = jnp.ones([1, 28, 28, 1])
model = Autoencoder()
params = model.init(rng, dummy_input)

# Set up the optimizer (Adam with learning rate 0.001)
optimizer = optax.adam(learning_rate=0.001)
opt_state = optimizer.init(params)

# Loss and Training Step
def mse_loss(params, batch):
    """Compute mean squared error between the reconstruction and input."""
    recon = model.apply(params, batch)
    return jnp.mean((recon - batch) ** 2)

@jit
def train_step(params, opt_state, images):
    loss, grads = value_and_grad(mse_loss)(params, images)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss

# Training Loop
epochs = 10
for epoch in range(epochs):
    # Iterate over the TFDS training dataset (convert batches to numpy arrays)
    for batch in tfds.as_numpy(train_ds):
        images = batch['image']  # shape: [batch, 28, 28, 1]
        params, opt_state, loss = train_step(params, opt_state, images)
    print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}")

# -------------------------------
# Example ID = "m1"
# -------------------------------

import jax
import jax.numpy as jnp
import numpy as np
import optax
import matplotlib.pyplot as plt

from flax import linen as nn
from flax.training import train_state

# Data Preparation
# Set random seed for numpy (JAX’s PRNG is used in the models)
np.random.seed(42)
sequence_length = 10
num_samples = 100

# Create a sine wave dataset
X_vals = np.linspace(0, 4 * np.pi, num_samples).reshape(-1, 1)
y_vals = np.sin(X_vals)

def create_in_out_sequences(data, seq_length):
    in_seq = []
    out_seq = []
    # data is assumed to be of shape (N, 1)
    for i in range(len(data) - seq_length):
        in_seq.append(data[i:i + seq_length])
        out_seq.append(data[i + seq_length])
    return jnp.array(in_seq), jnp.array(out_seq)

X_seq, y_seq = create_in_out_sequences(y_vals, sequence_length)  # shapes: (num_samples-seq_length, seq_length, 1) and (num_samples-seq_length, 1)

# Custom LSTM Model (Hand-Coded)
class CustomLSTMModel(nn.Module):
    input_dim: int
    hidden_units: int

    @nn.compact
    def __call__(self, inputs, H_C=None):
        """
        inputs: shape (batch, seq_len, input_dim)
        H_C: tuple of (H, C); if None, initialize with random normal values using the "lstm" RNG.
        Returns:
          pred: predictions of shape (batch, seq_len, 1)
          (H, C): the final hidden and cell states.
        """
        batch_size, seq_len, _ = inputs.shape
        if H_C is None:
            # Use the "lstm" rng to generate initial states.
            key = self.make_rng("lstm")
            key_H, key_C = jax.random.split(key)
            H = jax.random.normal(key_H, (batch_size, self.hidden_units))
            C = jax.random.normal(key_C, (batch_size, self.hidden_units))
        else:
            H, C = H_C

        # Initialize gate parameters. (Each gate uses a weight matrix for input and hidden state, plus a bias.)
        Wxi = self.param("Wxi", nn.initializers.normal(), (self.input_dim, self.hidden_units))
        Whi = self.param("Whi", nn.initializers.normal(), (self.hidden_units, self.hidden_units))
        bi  = self.param("bi", nn.initializers.zeros, (self.hidden_units,))

        Wxf = self.param("Wxf", nn.initializers.normal(), (self.input_dim, self.hidden_units))
        Whf = self.param("Whf", nn.initializers.normal(), (self.hidden_units, self.hidden_units))
        bf  = self.param("bf", nn.initializers.zeros, (self.hidden_units,))

        Wxo = self.param("Wxo", nn.initializers.normal(), (self.input_dim, self.hidden_units))
        Who = self.param("Who", nn.initializers.normal(), (self.hidden_units, self.hidden_units))
        bo  = self.param("bo", nn.initializers.zeros, (self.hidden_units,))

        Wxc = self.param("Wxc", nn.initializers.normal(), (self.input_dim, self.hidden_units))
        Whc = self.param("Whc", nn.initializers.normal(), (self.hidden_units, self.hidden_units))
        bc  = self.param("bc", nn.initializers.zeros, (self.hidden_units,))

        dense = nn.Dense(features=1)

        outputs = []
        for t in range(seq_len):
            X_t = inputs[:, t, :]  # (batch, input_dim)
            I_t = jax.nn.sigmoid(jnp.dot(X_t, Wxi) + jnp.dot(H, Whi) + bi)
            F_t = jax.nn.sigmoid(jnp.dot(X_t, Wxf) + jnp.dot(H, Whf) + bf)
            O_t = jax.nn.sigmoid(jnp.dot(X_t, Wxo) + jnp.dot(H, Who) + bo)
            C_tilde = jnp.tanh(jnp.dot(X_t, Wxc) + jnp.dot(H, Whc) + bc)
            C = F_t * C + I_t * C_tilde
            H = O_t * jnp.tanh(C)
            outputs.append(H)
        outputs = jnp.stack(outputs, axis=1)  # shape (batch, seq_len, hidden_units)
        pred = dense(outputs)  # shape (batch, seq_len, 1)
        return pred, (H, C)

# Built-In LSTM Model using Flax's LSTMCell
class LSTMModel(nn.Module):
    hidden_size: int = 50

    @nn.compact
    def __call__(self, inputs):
        """
        inputs: shape (batch, seq_len, input_dim) where input_dim is assumed to be 1.
        The LSTM cell is applied over the time axis; the final hidden state is passed through a Dense layer.
        Returns:
          out: predictions of shape (batch, 1)
        """
        batch_size, seq_len, input_dim = inputs.shape
        # Initialize carry for the LSTM cell.
        rng = self.make_rng("lstm")
        key1, key2 = jax.random.split(rng)
        carry = ( ## THIS IS WHERE THE FIX 2 IS
            jax.random.normal(key1, (batch_size, self.hidden_size)),  # Hidden state
            jax.random.normal(key2, (batch_size, self.hidden_size))   # Cell state
        ) ## END FIX 2
        # carry = nn.LSTMCell.initialize_carry(self.make_rng("lstm"), (batch_size, input_dim), self.hidden_size) ## THIS IS WHERE THE FIX 1 IS
        lstm_cell = nn.LSTMCell(features=self.hidden_size) ## THIS IS WHERE THE FIX 3 IS
        # lstm_cell = nn.LSTMCell()
        outputs = []
        for t in range(seq_len):
            carry, y = lstm_cell(carry, inputs[:, t, :])
            outputs.append(y)
        outputs = jnp.stack(outputs, axis=1)  # shape (batch, seq_len, hidden_size)
        dense = nn.Dense(features=1)
        out = dense(outputs[:, -1, :])  # use the last output for prediction
        return out

# Training Utilities
# A simple TrainState to hold parameters and the optimizer state.
class TrainState(train_state.TrainState):
    pass

# Loss functions
def loss_fn_custom(params, rng, batch_x, batch_y):
    # Model returns a sequence of predictions; we use the last time-step.
    pred, _ = custom_model.apply({"params": params}, batch_x, rngs={"lstm": rng})
    pred_last = pred[:, -1, 0]  # shape (batch,)
    loss = jnp.mean((pred_last - batch_y.squeeze()) ** 2)
    return loss

def loss_fn_builtin(params, rng, batch_x, batch_y):
    pred = lstm_model.apply({"params": params}, batch_x, rngs={"lstm": rng})
    pred = pred.squeeze()  # shape (batch,)
    loss = jnp.mean((pred - batch_y.squeeze()) ** 2)
    return loss

# Training steps (jitted)
@jax.jit
def train_step_custom(state, batch_x, batch_y, rng):
    loss, grads = jax.value_and_grad(loss_fn_custom)(state.params, rng, batch_x, batch_y)
    state = state.apply_gradients(grads=grads)
    return state, loss

@jax.jit
def train_step_builtin(state, batch_x, batch_y, rng):
    loss, grads = jax.value_and_grad(loss_fn_builtin)(state.params, rng, batch_x, batch_y)
    state = state.apply_gradients(grads=grads)
    return state, loss

# Initialize Models and Optimizers
# Create instances of both models.
custom_model = CustomLSTMModel(input_dim=1, hidden_units=50)
lstm_model = LSTMModel(hidden_size=50)

# Initialize parameters by “calling” the model once with a sample input.
dummy_input_custom = jnp.ones((X_seq.shape[0], sequence_length, 1))
dummy_input_builtin = jnp.ones((X_seq.shape[0], sequence_length, 1))

rng_custom = jax.random.PRNGKey(0)
rng_builtin = jax.random.PRNGKey(1)

params_custom = custom_model.init({"params": rng_custom, "lstm": rng_custom}, dummy_input_custom)["params"]
params_builtin = lstm_model.init({"params": rng_builtin, "lstm": rng_builtin}, dummy_input_builtin)["params"]

# Create training states with Adam optimizer (learning rate = 0.01).
optimizer = optax.adam(0.01)
state_custom = TrainState.create(apply_fn=custom_model.apply, params=params_custom, tx=optimizer)
state_builtin = TrainState.create(apply_fn=lstm_model.apply, params=params_builtin, tx=optimizer)

# Training Loop for Custom LSTM Model
epochs = 500
rng = jax.random.PRNGKey(42)
for epoch in range(epochs):
    rng, step_rng = jax.random.split(rng)
    state_custom, loss_value = train_step_custom(state_custom, X_seq, y_seq, step_rng)
    if (epoch + 1) % 50 == 0:
        print(f"[Custom LSTM] Epoch [{epoch + 1}/{epochs}], Loss: {loss_value:.4f}")

# Training Loop for Built-In LSTM Model
rng = jax.random.PRNGKey(100)
for epoch in range(epochs):
    rng, step_rng = jax.random.split(rng)
    state_builtin, loss_value = train_step_builtin(state_builtin, X_seq, y_seq, step_rng)
    if (epoch + 1) % 50 == 0:
        print(f"[Built-In LSTM] Epoch [{epoch + 1}/{epochs}], Loss: {loss_value:.4f}")

# Testing on New Data
test_steps = 100  # should be greater than sequence_length
X_test = np.linspace(0, 5 * np.pi, test_steps).reshape(-1, 1)
y_test = np.sin(X_test)
X_test_seq, _ = create_in_out_sequences(y_test, sequence_length)  # (test_steps-seq_length, seq_length, 1)

# Get predictions from both models.
rng, custom_test_rng = jax.random.split(rng)
pred_custom, _ = custom_model.apply({"params": state_custom.params}, X_test_seq, rngs={"lstm": custom_test_rng})
# Use the last time-step predictions (flattened)
pred_custom = pred_custom[:, -1, 0]

rng, builtin_test_rng = jax.random.split(rng)
pred_builtin = lstm_model.apply({"params": state_builtin.params}, X_test_seq, rngs={"lstm": builtin_test_rng})
pred_builtin = pred_builtin.squeeze()

print("Predictions with Custom Model for new sequence:")
print(np.array(pred_custom))
print("\nPredictions with Built-In Model:")
print(np.array(pred_builtin))

# Plot the predictions
plt.figure(figsize=(8, 4))
plt.plot(pred_custom, label="Custom LSTM Model")
plt.plot(pred_builtin, label="Built-In LSTM Model")
plt.legend()
plt.title("Predictions on New Sine Wave Sequence")
plt.xlabel("Time step")
plt.ylabel("Predicted value")
plt.show()

# -------------------------------
# Example ID = "h9"
# -------------------------------

# Mixed‑precision training in JAX
import jax
import jax.numpy as jnp
from jax import random, value_and_grad, jit
import optax
import numpy as np

# Model definition (single fully‑connected layer, like nn.Linear)

def init_params(key, in_dim=10, out_dim=1, dtype=jnp.float16):
    """Return a pytree with weights and bias in fp16."""
    k1, k2 = random.split(key)
    w = random.normal(k1, (in_dim, out_dim), dtype=dtype)
    b = jnp.zeros((out_dim,), dtype=dtype)
    return {"w": w, "b": b}

def forward(params, x):
    """fp16 matmul, result promoted to fp16, stays fp16."""
    x = x.astype(jnp.float16)
    y_hat = x @ params["w"] + params["b"]
    return y_hat

# Synthetic data and DataLoader helper

def make_dataset(n_samples=1000, in_dim=10, out_dim=1, batch_size=32, key=0):
    key = random.PRNGKey(key)
    X = random.normal(key, (n_samples, in_dim), dtype=jnp.float32)
    y = random.normal(key, (n_samples, out_dim), dtype=jnp.float32)

    # Convert to numpy for simple slicing in a Python loop
    X, y = np.array(X), np.array(y)

    def loader():
        idxs = np.random.permutation(len(X))
        for i in range(0, len(X), batch_size):
            batch_idx = idxs[i : i + batch_size]
            yield X[batch_idx], y[batch_idx]

    return loader

dataloader = make_dataset()


# Loss, mixed‑precision helpers, and GradScaler analogue
def mse(preds, targets):
    return jnp.mean((preds - targets) ** 2)

class GradScaler:
    """
    A minimal dynamic‑loss‑scaling utility akin to torch.cuda.amp.GradScaler.
    Keeps everything in float32 outside the forward pass.
    """
    def __init__(self,
                 init_scale=2.**15,
                 growth_factor=2.,
                 backoff_factor=0.5,
                 growth_interval=2000):
        self.scale = init_scale
        self.growth_factor = growth_factor
        self.backoff_factor = backoff_factor
        self.growth_interval = growth_interval
        self._skip = 0  # counts consecutive non‑overflow steps

    def unscale_and_check(self, grads):
        """Divide grads by scale and test for NaN/Inf overflow."""
        inv_scale = 1. / self.scale
        grads = jax.tree_map(lambda g: g * inv_scale, grads)

        def _has_inf_or_nan(x):
            return jnp.any(~jnp.isfinite(x))

        overflow = jax.tree_util.tree_reduce(
            lambda a, b: jnp.logical_or(a, _has_inf_or_nan(b)), grads, False
        )
        return grads, overflow

    def update(self, overflow):
        """Update scale according to overflow detection."""
        if overflow:
            self.scale = max(self.scale * self.backoff_factor, 1.)
            self._skip = 0
        else:
            self._skip += 1
            if self._skip % self.growth_interval == 0:
                self.scale *= self.growth_factor

# Optimizer (Adam) and training step

lr = 1e-3
optimizer = optax.adam(lr)
key = random.PRNGKey(42)
params = init_params(key)
opt_state = optimizer.init(params)
scaler = GradScaler()

@jit
def train_step(params, opt_state, x, y, scale):
    """
    A single mixed‑precision update step:
    - forward pass in fp16
    - loss scaled to fp32 * scale
    - gradients unscaled before the Adam update
    """
    def loss_fn(p, bx, by):
        preds = forward(p, bx)
        return mse(preds.astype(jnp.float32), by.astype(jnp.float32))

    scaled_loss_fn = lambda p: loss_fn(p, x, y) * scale
    loss_val, grads = value_and_grad(scaled_loss_fn)(params)
    return loss_val, grads

# Training loop
epochs = 5
for epoch in range(epochs):
    for xb, yb in dataloader():
        # Convert NumPy batches to jax arrays on device
        xb = jnp.asarray(xb)
        yb = jnp.asarray(yb)

        # Forward / backward with scaling
        loss_val, grads = train_step(params, opt_state, xb, yb, scaler.scale)

        # Unscale & check overflow
        grads, found_inf = scaler.unscale_and_check(grads)

        # If no overflow, update params
        if not found_inf:
            updates, opt_state = optimizer.update(grads, opt_state, params)
            params = optax.apply_updates(params, updates)

        # Adjust dynamic scale
        scaler.update(found_inf)

    print(f"Epoch {epoch+1}/{epochs}, Loss: {(loss_val / scaler.scale):.4f}")


# Inference (autocast fp16 forward, no grad)

key_pred = random.PRNGKey(2025)
X_test = random.normal(key_pred, (5, 10), dtype=jnp.float32)
predictions = forward(params, X_test).astype(jnp.float32)   # cast back for readability
print("Predictions:", np.array(predictions))


# -------------------------------
# Example ID = "h10"
# ------------------------------- 
import jax
import jax.numpy as jnp
import flax.linen as nn
import numpy as np
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from PIL import Image
import matplotlib.pyplot as plt
from jax import random

# Define a simplified ResNet-like model in Flax
class ResNet(nn.Module):
    @nn.compact
    def __call__(self, x):
        x = nn.Conv(64, (7, 7), strides=(2, 2), padding='SAME')(x)
        x = nn.relu(x)
        # x = nn.avg_pool(x, (3, 3), strides=(2, 2))
        x = nn.avg_pool(x, (3, 3), strides=(2, 2), padding='SAME') 
        x = nn.Conv(512, (3, 3), padding='SAME', name='layer4_conv2')(x)  # Target layer
        self.sow('intermediates', 'activations', x)  # Capture activations
        x = nn.relu(x)
        # x = nn.avg_pool(x, (7, 7))
        x = nn.avg_pool(x, (7, 7), padding='SAME')
        x = x.reshape((x.shape[0], -1))
        # x = nn.Dense(1000, kernel_init=nn.initializers.he_normal())(x)
        x = nn.Dense(1000, kernel_init=nn.initializers.normal(stddev=0.01))(x)
        return x

# Function to compute gradients
def compute_gradients(params, inputs, target_class):
    def forward_fn(inputs):
        model = ResNet()
        logits, state = model.apply({'params': params}, inputs, mutable=['intermediates'])
        return logits[0, target_class], state['intermediates']['activations']
    
    grad_fn = jax.grad(forward_fn, has_aux=True)
    (grads, activations) = grad_fn(inputs)
    return grads, activations

def main():
    # Initialize random key
    key = random.PRNGKey(0)

    # Load and preprocess image
    dataset = datasets.FakeData(transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ]))
    image, _ = dataset[0]
    image_pil = transforms.ToPILImage()(image)

    preprocess = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    input_tensor = preprocess(image_pil).unsqueeze(0)
    input_tensor = jnp.array(input_tensor)

    # Initialize model
    model = ResNet()
    params = model.init(key, input_tensor)['params']

    # Forward pass
    logits, state = model.apply({'params': params}, input_tensor, mutable=['intermediates'])
    predicted_class = jnp.argmax(logits, axis=1).item()
    activations = state['intermediates']['activations']

    # Backward pass for Grad-CAM
    gradients, _ = compute_gradients(params, input_tensor, predicted_class)

    # Generate Grad-CAM heatmap
    weights = jnp.mean(gradients, axis=(2, 3), keepdims=True)
    heatmap = jnp.sum(weights * activations[0], axis=1).squeeze()
    heatmap = jax.nn.relu(heatmap)

    # Normalize and convert heatmap
    heatmap = heatmap / jnp.max(heatmap + 1e-8)
    heatmap_np = np.array(heatmap)
    heatmap_pil = Image.fromarray((heatmap_np * 255).astype(np.uint8)).resize(image_pil.size, resample=Image.BILINEAR)

    # Display image with heatmap
    plt.imshow(image_pil)
    plt.imshow(heatmap_pil, alpha=0.5, cmap='jet')
    plt.title(f"Predicted Class: {predicted_class}")
    plt.axis('off')
    plt.savefig('gradcam_output.png')

if __name__ == "__main__":
    main()
