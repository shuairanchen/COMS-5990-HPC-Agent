{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6afd06db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15514,
     "status": "ok",
     "timestamp": 1743286174785,
     "user": {
      "displayName": "Shuairan Chen",
      "userId": "14778649985650964664"
     },
     "user_tz": 300
    },
    "id": "6afd06db",
    "outputId": "c40765eb-4d66-436e-8b4d-ca34fe928fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] - Loss: 35.5304\n",
      "Epoch [20/100] - Loss: 34.7664\n",
      "Epoch [30/100] - Loss: 33.6247\n",
      "Epoch [40/100] - Loss: 30.9979\n",
      "Epoch [50/100] - Loss: 27.3896\n",
      "Epoch [60/100] - Loss: 24.1525\n",
      "Epoch [70/100] - Loss: 21.2032\n",
      "Epoch [80/100] - Loss: 18.6953\n",
      "Epoch [90/100] - Loss: 16.5154\n",
      "Epoch [100/100] - Loss: 14.5447\n",
      "Input: [[3, 18, 4, 11, 8, 17, 12, 7, 18, 1]], Output: [13, 13, 2, 2, 2, 12, 12, 7, 7, 12, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "#Input\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "# Define the Decoder with Attention\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, src_seq_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.attention = nn.Linear(hidden_dim + embed_dim, src_seq_length)\n",
    "        self.attention_combine = nn.Linear(hidden_dim + embed_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, hidden, cell):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(torch.cat((embedded.squeeze(1), hidden[-1]), dim=1)), dim=1)\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        # Combine context and embedded input\n",
    "        combined = torch.cat((embedded.squeeze(1), context_vector.squeeze(1)), dim=1)\n",
    "        combined = torch.tanh(self.attention_combine(combined)).unsqueeze(1)\n",
    "\n",
    "        # LSTM and output\n",
    "        lstm_out, (hidden, cell) = self.lstm(combined, (hidden, cell))\n",
    "        output = self.fc_out(lstm_out.squeeze(1))\n",
    "        return output, hidden, cell\n",
    "\n",
    "# Define synthetic training data\n",
    "torch.manual_seed(42)\n",
    "src_vocab_size = 20\n",
    "tgt_vocab_size = 20\n",
    "src_seq_length = 10\n",
    "tgt_seq_length = 12\n",
    "batch_size = 16\n",
    "\n",
    "src_data = torch.randint(0, src_vocab_size, (batch_size, src_seq_length))\n",
    "tgt_data = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_length))\n",
    "\n",
    "# Initialize models, loss function, and optimizer\n",
    "input_dim = src_vocab_size\n",
    "output_dim = tgt_vocab_size\n",
    "embed_dim = 32\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "\n",
    "encoder = Encoder(input_dim, embed_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(output_dim, embed_dim, hidden_dim, num_layers, src_seq_length)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    encoder_outputs, (hidden, cell) = encoder(src_data)\n",
    "    loss = 0\n",
    "    decoder_input = torch.zeros(batch_size, dtype=torch.long)  # Start token\n",
    "\n",
    "    for t in range(tgt_seq_length):\n",
    "        output, hidden, cell = decoder(decoder_input, encoder_outputs, hidden, cell)\n",
    "        loss += criterion(output, tgt_data[:, t])\n",
    "        decoder_input = tgt_data[:, t]  # Teacher forcing\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the sequence-to-sequence model with new input\n",
    "test_input = torch.randint(0, src_vocab_size, (1, src_seq_length))\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = encoder(test_input)\n",
    "    decoder_input = torch.zeros(1, dtype=torch.long)  # Start token\n",
    "    output_sequence = []\n",
    "\n",
    "    for _ in range(tgt_seq_length):\n",
    "        output, hidden, cell = decoder(decoder_input, encoder_outputs, hidden, cell)\n",
    "        predicted = output.argmax(1)\n",
    "        output_sequence.append(predicted.item())\n",
    "        decoder_input = predicted\n",
    "\n",
    "    print(f\"Input: {test_input.tolist()}, Output: {output_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98dcb7c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 8526,
     "status": "error",
     "timestamp": 1743286190393,
     "user": {
      "displayName": "Shuairan Chen",
      "userId": "14778649985650964664"
     },
     "user_tz": 300
    },
    "id": "98dcb7c2",
    "outputId": "41d759d4-08d9-4d2d-84e4-035c233ace7c"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 191\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39marray(test_input)\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_sequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 149\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(output_dim\u001b[38;5;241m=\u001b[39moutput_dim, embed_dim\u001b[38;5;241m=\u001b[39membed_dim, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim,\n\u001b[0;32m    146\u001b[0m                   num_layers\u001b[38;5;241m=\u001b[39mnum_layers, src_seq_length\u001b[38;5;241m=\u001b[39msrc_seq_length)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Initialize model parameters.\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m encoder_vars \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m encoder_params \u001b[38;5;241m=\u001b[39m encoder_vars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    151\u001b[0m encoder_outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mapply({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder_params}, src_data)\n",
      "    \u001b[1;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m, in \u001b[0;36mEncoder.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Process through a stack of LSTM layers\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Initialize carry (cell, hidden) for this layer\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     initial_state \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTMCell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_carry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     outputs, final_state \u001b[38;5;241m=\u001b[39m LSTMLayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_layer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)(outputs, initial_state)\n\u001b[0;32m     47\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(final_state)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\flax\\linen\\recurrent.py:193\u001b[0m, in \u001b[0;36mLSTMCell.initialize_carry\u001b[1;34m(self, rng, input_shape)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;129m@nowrap\u001b[39m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_carry\u001b[39m(\n\u001b[0;32m    183\u001b[0m   \u001b[38;5;28mself\u001b[39m, rng: PRNGKey, input_shape: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Array, Array]:\n\u001b[0;32m    185\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the RNN cell carry.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    An initialized carry for the given RNN cell.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m   batch_dims \u001b[38;5;241m=\u001b[39m \u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    194\u001b[0m   key1, key2 \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[0;32m    195\u001b[0m   mem_shape \u001b[38;5;241m=\u001b[39m batch_dims \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Strong LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    hidden_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, initial_state):\n",
    "        # x shape: (batch, seq_length, features)\n",
    "        lstm_cell = nn.LSTMCell()\n",
    "        def step_fn(carry, x_t):\n",
    "            new_carry, y = lstm_cell(carry, x_t)\n",
    "            return new_carry, y\n",
    "        final_state, outputs = nn.scan(\n",
    "            step_fn,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1,\n",
    "            out_axes=1,\n",
    "        )(initial_state, x)\n",
    "        return outputs, final_state\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    input_dim: int        # vocabulary size of source\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x shape: (batch, src_seq_length) of token IDs\n",
    "        emb = nn.Embed(num_embeddings=self.input_dim, features=self.embed_dim)(x)\n",
    "        # emb shape: (batch, src_seq_length, embed_dim)\n",
    "        batch_size = emb.shape[0]\n",
    "        outputs = emb\n",
    "        states = []\n",
    "        # Process through a stack of LSTM layers\n",
    "        for i in range(self.num_layers):\n",
    "            # Initialize carry (cell, hidden) for this layer\n",
    "            initial_state = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,), self.hidden_dim)\n",
    "            outputs, final_state = LSTMLayer(self.hidden_dim, name=f\"lstm_layer_{i}\")(outputs, initial_state)\n",
    "            states.append(final_state)\n",
    "        # Collect final states from each layer.\n",
    "        hidden = jnp.stack([s[1] for s in states], axis=0)  # shape: (num_layers, batch, hidden_dim)\n",
    "        cell   = jnp.stack([s[0] for s in states], axis=0)\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    output_dim: int       # vocabulary size of target\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    src_seq_length: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, encoder_outputs, hidden, cell):\n",
    "        # x shape: (batch,) token IDs; add a time dimension.\n",
    "        x = x[:, None]  # shape becomes (batch, 1)\n",
    "        embedded = nn.Embed(num_embeddings=self.output_dim, features=self.embed_dim)(x)\n",
    "        # embedded shape: (batch, 1, embed_dim)\n",
    "        embedded_squeezed = jnp.squeeze(embedded, axis=1)  # (batch, embed_dim)\n",
    "        # Attention: combine embedded input with last-layer hidden state.\n",
    "        last_hidden = hidden[-1]  # (batch, hidden_dim)\n",
    "        attn_input = jnp.concatenate([embedded_squeezed, last_hidden], axis=1)  # (batch, embed_dim+hidden_dim)\n",
    "        # Map to raw attention scores (one score per encoder time step).\n",
    "        attn_scores = nn.Dense(self.src_seq_length)(attn_input)  # (batch, src_seq_length)\n",
    "        attention_weights = jax.nn.softmax(attn_scores, axis=1)   # (batch, src_seq_length)\n",
    "        # Compute context vector as weighted sum over encoder outputs.\n",
    "        # encoder_outputs shape: (batch, src_seq_length, hidden_dim)\n",
    "        attention_weights_exp = attention_weights[:, None, :]  # (batch, 1, src_seq_length)\n",
    "        context_vector = jnp.matmul(attention_weights_exp, encoder_outputs)  # (batch, 1, hidden_dim)\n",
    "        context_vector = jnp.squeeze(context_vector, axis=1)  # (batch, hidden_dim)\n",
    "        # Combine context vector and embedded input.\n",
    "        combined = jnp.concatenate([embedded_squeezed, context_vector], axis=1)  # (batch, embed_dim+hidden_dim)\n",
    "        combined = nn.Dense(self.embed_dim)(combined)\n",
    "        combined = jnp.tanh(combined)\n",
    "        combined = combined[:, None, :]  # (batch, 1, embed_dim)\n",
    "        # Pass through a one-step multi-layer LSTM.\n",
    "        new_hidden = []\n",
    "        new_cell = []\n",
    "        x_t = jnp.squeeze(combined, axis=1)  # (batch, embed_dim)\n",
    "        for i in range(self.num_layers):\n",
    "            lstm_cell = nn.LSTMCell(name=f\"decoder_lstm_cell_{i}\")\n",
    "            state = (cell[i], hidden[i])\n",
    "            new_state, y = lstm_cell(state, x_t)\n",
    "            new_cell.append(new_state[0])\n",
    "            new_hidden.append(new_state[1])\n",
    "            x_t = y  # output becomes input for the next layer\n",
    "        new_hidden = jnp.stack(new_hidden, axis=0)\n",
    "        new_cell = jnp.stack(new_cell, axis=0)\n",
    "        # Map final LSTM output to target vocabulary logits.\n",
    "        output = nn.Dense(self.output_dim)(y)  # (batch, output_dim)\n",
    "        return output, new_hidden, new_cell\n",
    "\n",
    "def get_data(key):\n",
    "    src_vocab_size = 20\n",
    "    tgt_vocab_size = 20\n",
    "    src_seq_length = 10\n",
    "    tgt_seq_length = 12\n",
    "    batch_size = 16\n",
    "    key, subkey = jax.random.split(key)\n",
    "    src_data = jax.random.randint(subkey, shape=(batch_size, src_seq_length), minval=0, maxval=src_vocab_size)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    tgt_data = jax.random.randint(subkey, shape=(batch_size, tgt_seq_length), minval=0, maxval=tgt_vocab_size)\n",
    "    return src_data, tgt_data, src_vocab_size, tgt_vocab_size, src_seq_length, tgt_seq_length, batch_size\n",
    "\n",
    "def cross_entropy_loss(logits, targets):\n",
    "    # logits shape: (batch, num_classes); targets shape: (batch,)\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    one_hot = jax.nn.one_hot(targets, logits.shape[-1])\n",
    "    loss = -jnp.sum(one_hot * log_probs, axis=-1)\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def loss_fn(encoder_params, decoder_params, src_data, tgt_data, encoder, decoder, tgt_seq_length):\n",
    "    # Run encoder.\n",
    "    enc_vars = {'params': encoder_params}\n",
    "    encoder_outputs, (hidden, cell) = encoder.apply(enc_vars, src_data)\n",
    "    loss = 0.0\n",
    "    # Start token (assumed 0) for the decoder.\n",
    "    decoder_input = jnp.zeros((src_data.shape[0],), dtype=jnp.int32)\n",
    "    dec_vars = {'params': decoder_params}\n",
    "    for t in range(tgt_seq_length):\n",
    "        logits, hidden, cell = decoder.apply(dec_vars, decoder_input, encoder_outputs, hidden, cell)\n",
    "        loss += cross_entropy_loss(logits, tgt_data[:, t])\n",
    "        # Teacher forcing: next input is current target.\n",
    "        decoder_input = tgt_data[:, t]\n",
    "    return loss / tgt_seq_length\n",
    "\n",
    "def main():\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    src_data, tgt_data, src_vocab_size, tgt_vocab_size, src_seq_length, tgt_seq_length, batch_size = get_data(key)\n",
    "\n",
    "    input_dim = src_vocab_size      # Source vocabulary size\n",
    "    output_dim = tgt_vocab_size     # Target vocabulary size\n",
    "    embed_dim = 32\n",
    "    hidden_dim = 64\n",
    "    num_layers = 2\n",
    "\n",
    "    encoder = Encoder(input_dim=input_dim, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "    decoder = Decoder(output_dim=output_dim, embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                      num_layers=num_layers, src_seq_length=src_seq_length)\n",
    "\n",
    "    # Initialize model parameters.\n",
    "    encoder_vars = encoder.init(key, src_data)\n",
    "    encoder_params = encoder_vars['params']\n",
    "    encoder_outputs, (hidden, cell) = encoder.apply({'params': encoder_params}, src_data)\n",
    "    dummy_decoder_input = jnp.zeros((batch_size,), dtype=jnp.int32)\n",
    "    decoder_vars = decoder.init(key, dummy_decoder_input, encoder_outputs, hidden, cell)\n",
    "    decoder_params = decoder_vars['params']\n",
    "\n",
    "    # Combine parameters and set up the optimizer.\n",
    "    params = {'encoder': encoder_params, 'decoder': decoder_params}\n",
    "    optimizer = optax.adam(learning_rate=0.001)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(params, opt_state, src_data, tgt_data):\n",
    "        def loss_wrapper(params):\n",
    "            return loss_fn(params['encoder'], params['decoder'], src_data, tgt_data, encoder, decoder, tgt_seq_length)\n",
    "        loss_val, grads = jax.value_and_grad(loss_wrapper)(params)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss_val\n",
    "\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        params, opt_state, loss_val = train_step(params, opt_state, src_data, tgt_data)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss_val:.4f}\")\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    test_input = jax.random.randint(subkey, shape=(1, src_seq_length), minval=0, maxval=src_vocab_size)\n",
    "    enc_vars = {'params': params['encoder']}\n",
    "    encoder_outputs, (hidden, cell) = encoder.apply(enc_vars, test_input)\n",
    "    decoder_input = jnp.zeros((1,), dtype=jnp.int32)  # Start token\n",
    "    output_sequence = []\n",
    "    dec_vars = {'params': params['decoder']}\n",
    "    for _ in range(tgt_seq_length):\n",
    "        logits, hidden, cell = decoder.apply(dec_vars, decoder_input, encoder_outputs, hidden, cell)\n",
    "        predicted = jnp.argmax(logits, axis=-1)\n",
    "        output_sequence.append(int(predicted[0]))\n",
    "        decoder_input = predicted\n",
    "    print(f\"Input: {np.array(test_input).tolist()}, Output: {output_sequence}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2c42c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "error",
     "timestamp": 1743286237846,
     "user": {
      "displayName": "Shuairan Chen",
      "userId": "14778649985650964664"
     },
     "user_tz": 300
    },
    "id": "1ec2c42c",
    "outputId": "26ca609a-0430-4bf9-b3cc-adcc4e4c15d4"
   },
   "outputs": [],
   "source": [
    "#Weak LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Define the decoder module with attention mechanism\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def __call__(self, decoder_input, encoder_outputs, hidden_state, cell_state):\n",
    "        # Compute the attention scores\n",
    "        attention_scores = jnp.dot(encoder_outputs, hidden_state)  # MODIFIED: Ensure hidden_state is used appropriately\n",
    "        attention_weights = nn.softmax(attention_scores)\n",
    "        context_vector = jnp.dot(attention_weights, encoder_outputs)  # Compute the context vector\n",
    "\n",
    "        # Update hidden state (dummy example, the actual implementation may vary)\n",
    "        hidden_state = self.update_hidden_state(hidden_state, context_vector)\n",
    "\n",
    "        # Generate output (dummy generation logic)\n",
    "        output = nn.Dense(self.vocab_size)(context_vector)  # Define your output layer here\n",
    "\n",
    "        return output, hidden_state, cell_state\n",
    "\n",
    "    def update_hidden_state(self, hidden_state, context_vector):\n",
    "        # Dummy update function for hidden state\n",
    "        return hidden_state + context_vector  # Replace with actual update logic\n",
    "\n",
    "def main():\n",
    "    # Example parameters\n",
    "    vocab_size = 10000\n",
    "    hidden_size = 256\n",
    "    tgt_seq_length = 10\n",
    "\n",
    "    # Initialize decoder and states\n",
    "    decoder = Decoder(vocab_size=vocab_size, hidden_size=hidden_size)\n",
    "    hidden_state = jnp.zeros((1, hidden_size))\n",
    "    cell_state = jnp.zeros((1, hidden_size))\n",
    "    decoder_input = jnp.zeros((1, vocab_size))  # Adjust input dimensions accordingly\n",
    "    encoder_outputs = jnp.zeros((1, tgt_seq_length, hidden_size))  # Example encoder output\n",
    "\n",
    "    output_sequence = []\n",
    "\n",
    "    # Decoding process\n",
    "    for _ in range(tgt_seq_length):\n",
    "        output, hidden_state, cell_state = decoder(decoder_input, encoder_outputs, hidden_state, cell_state)  # MODIFIED: Updated to pass hidden_state\n",
    "        predicted = jnp.argmax(output, axis=1)\n",
    "        output_sequence.append(predicted.item())\n",
    "\n",
    "        # Ensure decoder_input shape matches the required input shape for the attention function\n",
    "        decoder_input = jax.nn.one_hot(predicted, vocab_size)  # MODIFIED: Convert predicted index to one-hot encoding\n",
    "\n",
    "    print(f\"Input: {jnp.zeros((1, vocab_size)).tolist()}, Output: {output_sequence}\")  # Placeholder for input\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23764fb2",
   "metadata": {
    "id": "23764fb2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error Code:\n",
    "File: <ipython-input-1-ffef6510e4ae>, line 17 attention_scores = jnp.dot(encoder_outputs, hidden_state) ... context_vector = jnp.dot(attention_weights, encoder_outputs)\n",
    "\n",
    "\n",
    "Error:\n",
    "dot_general requires contracting dimensions to have the same shape, got (256,) and (1,).\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "The error occurs because the dimensions in the dot product don't align. The encoder_outputs has shape (batch, seq_len, hidden_size), and hidden_state is (batch, hidden_size). Using jnp.dot here is incorrect. Instead, use einsum to correctly compute attention scores between each encoder output and the hidden state. Similarly, adjust the context vector computation to sum over the sequence dimension.\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "attention_scores = jnp.einsum('bsh,bh->bs', encoder_outputs, hidden_state)\n",
    "context_vector = jnp.einsum('bs,bsh->bh', attention_weights, encoder_outputs)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "output = nn.Dense(self.vocab_size)(context_vector)\n",
    "\n",
    "\n",
    "Error:\n",
    "raised in the init method of Dense\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "The error occurs because Flax modules require parameter initialization through a proper module structure. The Decoder's call method needs the @nn.compact decorator to create submodules (like Dense) inline. Also, ensure the Decoder's init calls its parent's init.\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@nn.compact  # MODIFIED: Add this decorator\n",
    "def __call__(self, decoder_input, encoder_outputs, hidden_state, cell_state):\n",
    "    # ... (existing code)\n",
    "    output = nn.Dense(self.vocab_size)(context_vector)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "Error:\n",
    "In Flax Linen, the __init__ method should not be directly overridden to initialize parameters\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Declare module parameters using class attributes\n",
    "Define each sublayer in the setup() method\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class Decoder(nn.Module):\n",
    "    output_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    src_seq_length: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.output_dim, features=self.embed_dim)\n",
    "        self.attention = nn.Dense(self.src_seq_length)\n",
    "        self.attention_combine = nn.Dense(self.embed_dim)\n",
    "        self.lstm = nn.OptimizedLSTMCell() \n",
    "        self.fc_out = nn.Dense(self.output_dim)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def __call__(self, decoder_input, encoder_outputs, hidden_state, cell_state):\n",
    "\n",
    "\n",
    "Error:\n",
    "The JAX code directly treats decoder_input as a vector and uses one-hot encoding, which is inconsistent with the original code logic\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Define the embedding layer in setup().\n",
    "In __call__, first embed the decoder_input (token index, shape [batch] or [batch, 1]) to get the embedding vector, which is then used for subsequent calculations\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def __call__(self, decoder_input, encoder_outputs, hidden_state, cell_state):\n",
    "    # decoder_input is a token index, the shape is (batch,) or (batch, 1)\n",
    "    embedded = self.embedding(decoder_input)  # Output shape: (batch, embed_dim) or (batch, 1, embed_dim)\n",
    "    if embedded.ndim == 3:\n",
    "        embedded = embedded.squeeze(1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "attention_scores = jnp.einsum('bsh,bh->bs', encoder_outputs, hidden_state)\n",
    "attention_weights = nn.softmax(attention_scores)\n",
    "context_vector = jnp.einsum('bs,bsh->bh', attention_weights, encoder_outputs)\n",
    "\n",
    "\n",
    "Error:\n",
    "The JAX code only uses hidden_state to participate in the calculation, without using the embedded information of the decoder.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Concatenate the current decoder's embedded embedded with hidden_state, and pass it to the self.attention linear layer to calculate the attention score\n",
    "Use jax.nn.softmax to calculate the attention weight, and calculate the context vector based on the weight and encoder_outputs\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "# Concatenate the current embedding and the previous hidden state (assuming the shape of hidden_state is (batch, hidden_dim))\n",
    "concat_input = jnp.concatenate([embedded, hidden_state], axis=-1) # Shape (batch, embed_dim + hidden_dim)\n",
    "attention_scores = self.attention(concat_input) # Output shape (batch, src_seq_length)\n",
    "attention_weights = jax.nn.softmax(attention_scores, axis=-1)\n",
    "context_vector = jnp.einsum('bs,bsh->bh', attention_weights, encoder_outputs) # Get (batch, hidden_dim)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# Update hidden state (dummy example, the actual implementation may vary)\n",
    "hidden_state = self.update_hidden_state(hidden_state, context_vector)\n",
    "\n",
    "# Generate output (dummy generation logic)\n",
    "output = nn.Dense(self.vocab_size)(context_vector)  # Define your output layer here\n",
    "\n",
    "return output, hidden_state, cell_state\n",
    "\n",
    "\n",
    "Error:\n",
    "After obtaining the context vector, the AX code concatenates it with the embedded input, expands the dimension through a fusion layer and tanh activation, then feeds it into an LSTM for state update, and then generates the output using a fully connected layer.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Concatenate embedded and context_vector, then pass self.attention_combine and tanh activation\n",
    "Expand the fused vector into the sequence dimension and input it into LSTMCell for state update\n",
    "Use the updated hidden state to get the output through self.fc_out\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "# Fusion current embedding and context vector\n",
    "combined = jnp.concatenate([embedded, context_vector], axis=-1) # (batch, embed_dim + hidden_dim)\n",
    "combined = jax.nn.tanh(self.attention_combine(combined))\n",
    "combined = combined[:, None, :]\n",
    "\n",
    "combined = combined.squeeze(1) # (batch, embed_dim)\n",
    "(new_hidden_state, new_cell_state), _ = self.lstm((hidden_state, cell_state), combined)\n",
    "output = self.fc_out(new_hidden_state) # (batch, output_dim)\n",
    "\n",
    "return output, new_hidden_state, new_cell_state\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "decoder = Decoder(vocab_size=vocab_size, hidden_size=hidden_size)\n",
    "\n",
    "\n",
    "Error:\n",
    "__init__() got an unexpected keyword argument 'vocab_size'\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the input parameters of Decoder\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "vocab_size = 10000\n",
    "embed_dim = 32\n",
    "hidden_dim = 256\n",
    "num_layers = 1 \n",
    "src_seq_length = 10\n",
    "\n",
    "decoder = Decoder(output_dim=output_dim, embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                      num_layers=num_layers, src_seq_length=src_seq_length)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "self.lstm = nn.OptimizedLSTMCell()\n",
    "\n",
    "\n",
    "Error:\n",
    "The hidden layer dimension parameter was not passed in when the LSTM cell was initialized\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Pass in features=self.hidden_dim during initialization\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "self.lstm = nn.OptimizedLSTMCell(features=self.hidden_dim)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "(new_hidden_state, new_cell_state), _ = self.lstm((hidden_state, cell_state), combined)\n",
    "\n",
    "\n",
    "Error:\n",
    "The input here is (hidden_state, cell_state), which does not match the state order of the LSTM cell\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Adjust the state order of the incoming LSTM cell and receive the return value\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "(new_cell_state, new_hidden_state), _ = self.lstm((cell_state, hidden_state), combined)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "decoder_input = jnp.zeros((1, vocab_size))\n",
    "\n",
    "\n",
    "Error:\n",
    "In the PyTorch code, the token index is passed to the decoder (the shape is (batch,) or (batch, 1)), and the one-hot vector is passed to jax, the shape is (1, vocab_size)\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Define decoder_input as an integer token index\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "decoder_input = jnp.array([0])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "hidden_state = jnp.zeros((1, hidden_size))\n",
    "cell_state = jnp.zeros((1, hidden_size))\n",
    "decoder_input = jnp.array([0])\n",
    "encoder_outputs = jnp.zeros((1, tgt_seq_length, hidden_size))\n",
    "\n",
    "Error:\n",
    "name 'hidden_size' is not defined\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Should use src_seq_length and hidden_dim\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "hidden_state = jnp.zeros((1, hidden_dim))\n",
    "cell_state = jnp.zeros((1, hidden_dim))\n",
    "decoder_input = jnp.array([0])\n",
    "encoder_outputs = jnp.zeros((1, src_seq_length, hidden_dim))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "combined = jax.nn.tanh(self.attention_combine(combined))\n",
    "combined = combined[:, None, :]\n",
    "combined = combined.squeeze(1) \n",
    "\n",
    "\n",
    "Error:\n",
    "Adding a dimension and then squeezing it out immediately is unnecessary for the input LSTM cell and may cause shape confusion\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Directly keep the shape of combined as (batch, embed_dim)\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "combined = jax.nn.tanh(self.attention_combine(combined))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "decoder = Decoder(output_dim=output_dim, embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                  num_layers=num_layers, src_seq_length=src_seq_length)\n",
    "\n",
    "\n",
    "Error:\n",
    "output_dim is undefined\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Replace output_dim with vocab_size\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "decoder = Decoder(output_dim=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                  num_layers=num_layers, src_seq_length=src_seq_length)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "for _ in range(tgt_seq_length):\n",
    "\n",
    "\n",
    "Error:\n",
    "tgt_seq_length is not defined in main()\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "When using the target sequence length, refer to tgt_seq_length defined in the PyTorch code\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "tgt_seq_length = 12\n",
    "for _ in range(tgt_seq_length):\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "decoder_input = jax.nn.one_hot(predicted, vocab_size)\n",
    "\n",
    "\n",
    "Error:\n",
    "Input dimensions do not match\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "When decoding, the predicted token index is used directly without converting to one-hot encoding\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "decoder_input = predicted\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "for _ in range(tgt_seq_length):\n",
    "    output, hidden_state, cell_state = decoder(decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "\n",
    "\n",
    "Error:\n",
    "\"Decoder\" object has no attribute \"embedding\". If \"embedding\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Define a random number generator (PRNG key)\n",
    "Call decoder.init to initialize the model parameters and save the returned variable dictionary\n",
    "In the decoding loop, use decoder.apply(variables, ...) to call the model instead of calling the module object directly\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "variables = decoder.init(rng, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "for _ in range(tgt_seq_length):\n",
    "    output, hidden_state, cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "output_sequence = []\n",
    "\n",
    "# Decoding process\n",
    "tgt_seq_length = 12\n",
    "\n",
    "\n",
    "Error:\n",
    "name 'rng' is not defined\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Initialize parameter variables using jax\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "rng = jax.random.PRNGKey(0)\n",
    "output_sequence = []\n",
    "\n",
    "# Decoding process\n",
    "tgt_seq_length = 12\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "for _ in range(tgt_seq_length):\n",
    "    output, hidden_state, cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    predicted = jnp.argmax(output, axis=1)\n",
    "    output_sequence.append(int(predicted.item()))\n",
    "    decoder_input = predicted\n",
    "\n",
    "\n",
    "Error:\n",
    "The kernel appears to have died. It will restart automatically.\n",
    "Calling decoder.apply(...) directly in each loop may cause repeated tracing and compilation, which may consume a lot of memory or cause runtime problems, eventually leading to kernel crashes.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Encapsulate the decoding step into a separate function and JIT-compile it using jax.jit\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@jax.jit\n",
    "def decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs):\n",
    "    output, new_hidden_state, new_cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    predicted = jnp.argmax(output, axis=1)\n",
    "    return predicted, new_hidden_state, new_cell_state\n",
    "    \n",
    "for _ in range(tgt_seq_length):\n",
    "    predicted, hidden_state, cell_state = decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs)\n",
    "    output_sequence.append(int(predicted.item()))\n",
    "    decoder_input = predicted\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "@jax.jit\n",
    "def decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs):\n",
    "    output, new_hidden_state, new_cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    predicted = jnp.argmax(output, axis=1)\n",
    "    return predicted, new_hidden_state, new_cell_state\n",
    "\n",
    "\n",
    "Error:\n",
    "A JIT-compiled decode_step whose parameters are not marked as static each time it is called in a loop may cause JAX to repeatedly trace and recompile, consuming large amounts of memory or causing unexpected errors\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Mark unchanged parameters as static so that the JIT only compiles the dynamic part\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@jax.jit(static_argnums=(3,4))\n",
    "def decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs):\n",
    "    output, new_hidden_state, new_cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    predicted = jnp.argmax(output, axis=1)\n",
    "    return predicted, new_hidden_state, new_cell_state\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "@jax.jit(static_argnums=(3,4))\n",
    "\n",
    "\n",
    "Error:\n",
    "Mark model parameters and encoder_outputs as static parameters via static_argnums, causing JAX to try to hash these objects during tracing\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Remove static_argnums parameter so all inputs are passed as dynamic arguments\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@jax.jit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "self.lstm = nn.OptimizedLSTMCell(features=self.hidden_dim)\n",
    "\n",
    "\n",
    "Error:\n",
    "nn.OptimizedLSTMCell is not available or deprecated in Flax's Linen API\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Replace nn.OptimizedLSTMCell with nn.LSTMCell and pass the same features parameter\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "self.lstm = nn.LSTMCell(features=self.hidden_dim)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "(new_cell_state, new_hidden_state), _ = self.lstm((cell_state, hidden_state), combined)\n",
    "output = self.fc_out(new_hidden_state)\n",
    "\n",
    "\n",
    "Error:\n",
    "When calling LSTMCell, a tuple is returned:\n",
    "The first return value is the new state (carry), which is usually structured as (new_cell_state, new_hidden_state)\n",
    "The second return value is the output of the current time step\n",
    "The current code incorrectly uses the second return value as \"ignore\" and directly uses the new hidden state (the part removed from carry) as the output, which is inconsistent with the logic of taking lstm_out and then fully connecting in PyTorch\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "When unpacking, get the carry and output at the same time\n",
    "Use the output value to pass into the fully connected layer to get the final output\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "carry, lstm_output = self.lstm((cell_state, hidden_state), combined)\n",
    "new_cell_state, new_hidden_state = carry\n",
    "output = self.fc_out(lstm_output)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# The JAX code only has the Decoder part, but no corresponding Encoder\n",
    "\n",
    "\n",
    "Error:\n",
    "The sequence-to-sequence model requires two parts: Encoder and Decoder. The lack of Encoder makes the overall model incomplete and cannot complete the end-to-end task\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add a Flax-based Encoder module\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class Encoder(nn.Module):\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.input_dim, features=self.embed_dim)\n",
    "        self.lstm = nn.LSTMCell(features=self.hidden_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x: (batch, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch, seq_length, embed_dim)\n",
    "        batch, seq_length, _ = embedded.shape\n",
    "        cell_state = jnp.zeros((batch, self.hidden_dim))\n",
    "        hidden_state = jnp.zeros((batch, self.hidden_dim))\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            (cell_state, hidden_state), lstm_output = self.lstm((cell_state, hidden_state), embedded[:, t, :])\n",
    "            outputs.append(lstm_output)\n",
    "        outputs = jnp.stack(outputs, axis=1)  # (batch, seq_length, hidden_dim)\n",
    "        return outputs, (hidden_state, cell_state)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "class Encoder(nn.Module):\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.input_dim, features=self.embed_dim)\n",
    "        self.lstm = nn.LSTMCell(features=self.hidden_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x: (batch, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch, seq_length, embed_dim)\n",
    "        batch, seq_length, _ = embedded.shape\n",
    "        cell_state = jnp.zeros((batch, self.hidden_dim))\n",
    "        hidden_state = jnp.zeros((batch, self.hidden_dim))\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            (cell_state, hidden_state), lstm_output = self.lstm((cell_state, hidden_state), embedded[:, t, :])\n",
    "            outputs.append(lstm_output)\n",
    "        outputs = jnp.stack(outputs, axis=1)  # (batch, seq_length, hidden_dim)\n",
    "        return outputs, (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "Error:\n",
    "The Encoder in the PyTorch code uses the num_layers parameter to build a multi-layer LSTM, while the JAX code only creates a single-layer LSTMCell\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add num_layers parameter to Encoder and construct a LSTMCell list in setup()\n",
    "Update each layer in turn for each time step in __call__\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class Encoder(nn.Module):\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.input_dim, features=self.embed_dim)\n",
    "        self.lstm_cells = [nn.LSTMCell(features=self.hidden_dim) for _ in range(self.num_layers)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x: (batch, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch, seq_length, embed_dim)\n",
    "        batch, seq_length, _ = embedded.shape\n",
    "\n",
    "        hidden_states = [jnp.zeros((batch, self.hidden_dim)) for _ in range(self.num_layers)]\n",
    "        cell_states = [jnp.zeros((batch, self.hidden_dim)) for _ in range(self.num_layers)]\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = embedded[:, t, :]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                (cell_states[i], hidden_states[i]), x_t = cell((cell_states[i], hidden_states[i]), x_t)\n",
    "            outputs.append(x_t)\n",
    "        outputs = jnp.stack(outputs, axis=1)  # (batch, seq_length, hidden_dim)\n",
    "\n",
    "        hidden_states = jnp.stack(hidden_states, axis=0)\n",
    "        cell_states = jnp.stack(cell_states, axis=0)\n",
    "        return outputs, (hidden_states, cell_states)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "class Decoder(nn.Module):\n",
    "    output_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    src_seq_length: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.output_dim, features=self.embed_dim)\n",
    "        self.attention = nn.Dense(self.src_seq_length)\n",
    "        self.attention_combine = nn.Dense(self.embed_dim)\n",
    "        self.lstm = nn.LSTMCell(features=self.hidden_dim)\n",
    "        self.fc_out = nn.Dense(self.output_dim)\n",
    "\n",
    "    def __call__(self, decoder_input, encoder_outputs, hidden_state, cell_state):\n",
    "        # decoder_input is a token index, the shape is (batch,) or (batch, 1)\n",
    "        embedded = self.embedding(decoder_input)  # Output shape: (batch, embed_dim) or (batch, 1, embed_dim)\n",
    "        if embedded.ndim == 3:\n",
    "            embedded = embedded.squeeze(1)\n",
    "        # Compute the attention scores\n",
    "        # Concatenate the current embedding and the previous hidden state (assuming the shape of hidden_state is (batch, hidden_dim))\n",
    "        concat_input = jnp.concatenate([embedded, hidden_state], axis=-1) # Shape (batch, embed_dim + hidden_dim)\n",
    "        attention_scores = self.attention(concat_input) # Output shape (batch, src_seq_length)\n",
    "        attention_weights = jax.nn.softmax(attention_scores, axis=-1)\n",
    "        context_vector = jnp.einsum('bs,bsh->bh', attention_weights, encoder_outputs) # Get (batch, hidden_dim)\n",
    "\n",
    "        # Fusion current embedding and context vector\n",
    "        combined = jnp.concatenate([embedded, context_vector], axis=-1) # (batch, embed_dim + hidden_dim)\n",
    "        combined = jax.nn.tanh(self.attention_combine(combined))\n",
    "        \n",
    "        carry, lstm_output = self.lstm((cell_state, hidden_state), combined)\n",
    "        new_cell_state, new_hidden_state = carry\n",
    "        output = self.fc_out(lstm_output)\n",
    "\n",
    "        return output, new_hidden_state, new_cell_state\n",
    "\n",
    "    def update_hidden_state(self, hidden_state, context_vector):\n",
    "        # Dummy update function for hidden state\n",
    "        return hidden_state + context_vector  # Replace with actual update logic\n",
    "\n",
    "\n",
    "Error:\n",
    "Only a single-layer LSTMCell is created in the Decoder, while the Decoder in the PyTorch code uses multiple layers of LSTM\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify setup() to use a list to generate multiple LSTMCells, and update each layer in turn in __call__\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class Decoder(nn.Module):\n",
    "    output_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    src_seq_length: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.output_dim, features=self.embed_dim)\n",
    "        self.attention = nn.Dense(self.src_seq_length)\n",
    "        self.attention_combine = nn.Dense(self.embed_dim)\n",
    "        self.lstm_cells = [nn.LSTMCell(features=self.hidden_dim) for _ in range(self.num_layers)]\n",
    "        self.fc_out = nn.Dense(self.output_dim)\n",
    "\n",
    "    def __call__(self, decoder_input, encoder_outputs, hidden_state, cell_state):\n",
    "        # decoder_input: (batch,) 或 (batch, 1)\n",
    "        embedded = self.embedding(decoder_input)  # (batch, embed_dim) 或 (batch, 1, embed_dim)\n",
    "        if embedded.ndim == 3:\n",
    "            embedded = embedded.squeeze(1)  # (batch, embed_dim)\n",
    "\n",
    "        concat_input = jnp.concatenate([embedded, hidden_state[-1]], axis=-1)  # (batch, embed_dim + hidden_dim)\n",
    "        attention_scores = self.attention(concat_input)  # (batch, src_seq_length)\n",
    "        attention_weights = jax.nn.softmax(attention_scores, axis=-1)\n",
    "        context_vector = jnp.einsum('bs,bsh->bh', attention_weights, encoder_outputs)  # (batch, hidden_dim)\n",
    "\n",
    "        combined = jnp.concatenate([embedded, context_vector], axis=-1)  # (batch, embed_dim + hidden_dim)\n",
    "        combined = jax.nn.tanh(self.attention_combine(combined))  # (batch, embed_dim)\n",
    "        \n",
    "        new_hidden_states = []\n",
    "        new_cell_states = []\n",
    "        x = combined\n",
    "\n",
    "        for i, cell in enumerate(self.lstm_cells):\n",
    "            (new_cell, new_hidden), x = cell((cell_state[i], hidden_state[i]), x)\n",
    "            new_hidden_states.append(new_hidden)\n",
    "            new_cell_states.append(new_cell)\n",
    "        new_hidden_states = jnp.stack(new_hidden_states, axis=0)  # (num_layers, batch, hidden_dim)\n",
    "        new_cell_states = jnp.stack(new_cell_states, axis=0)      # (num_layers, batch, hidden_dim)\n",
    "        output = self.fc_out(x)  # (batch, output_dim)\n",
    "        return output, new_hidden_states, new_cell_states\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def main():\n",
    "    # Example parameters\n",
    "    vocab_size = 10000\n",
    "    embed_dim = 32\n",
    "    hidden_dim = 256\n",
    "    num_layers = 1 \n",
    "    src_seq_length = 10\n",
    "\n",
    "    # Initialize decoder and states\n",
    "    decoder = Decoder(output_dim=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                      num_layers=num_layers, src_seq_length=src_seq_length)\n",
    "\n",
    "    hidden_state = jnp.zeros((1, hidden_dim))\n",
    "    cell_state = jnp.zeros((1, hidden_dim))\n",
    "    decoder_input = jnp.array([0])\n",
    "    encoder_outputs = jnp.zeros((1, src_seq_length, hidden_dim))\n",
    "    \n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    output_sequence = []\n",
    "\n",
    "    # Decoding process\n",
    "    tgt_seq_length = 12\n",
    "    variables = decoder.init(rng, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    \n",
    "    @jax.jit\n",
    "    def decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs):\n",
    "        output, new_hidden_state, new_cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "        predicted = jnp.argmax(output, axis=1)\n",
    "        return predicted, new_hidden_state, new_cell_state\n",
    "\n",
    "    # Decoding process\n",
    "    for _ in range(tgt_seq_length):\n",
    "        predicted, hidden_state, cell_state = decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs)\n",
    "        output_sequence.append(int(predicted.item()))\n",
    "        decoder_input = predicted\n",
    "\n",
    "    print(f\"Input: {jnp.zeros((1, vocab_size)).tolist()}, Output: {output_sequence}\")  # Placeholder for input\n",
    "\n",
    "\n",
    "Error:\n",
    "The required parameters are missing, the Encoder is not called, and a randomly generated test_input is not used to get the encoder_outputs and status through the Encoder and then pass them to the Decoder\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add the corresponding parameters in pytorch and call Encoder. In the test phase, a test input should be generated first, encoder_outputs and initial state should be obtained through Encoder, and then Decoder should be called for decoding, and finally the actual input and output should be printed\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def main():\n",
    "    # Example parameters\n",
    "    src_vocab_size = 20\n",
    "    tgt_vocab_size = 20\n",
    "    src_seq_length = 10\n",
    "    tgt_seq_length = 12\n",
    "    batch_size = 1 \n",
    "    embed_dim = 32\n",
    "    hidden_dim = 64\n",
    "    num_layers = 2\n",
    "\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    \n",
    "    encoder = Encoder(input_dim=src_vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "    decoder = Decoder(output_dim=tgt_vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                      num_layers=num_layers, src_seq_length=src_seq_length)\n",
    "    \n",
    "    test_input = jax.random.randint(rng, (1, src_seq_length), 0, src_vocab_size)\n",
    "    encoder_variables = encoder.init(rng, test_input)\n",
    "    encoder_outputs, (enc_hidden, enc_cell) = encoder.apply(encoder_variables, test_input)\n",
    "    \n",
    "    hidden_state = jnp.zeros((num_layers, 1, hidden_dim))\n",
    "    cell_state = jnp.zeros((num_layers, 1, hidden_dim))\n",
    "    \n",
    "    decoder_input = jnp.array([0])  \n",
    "    decoder_variables = decoder.init(rng, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    \n",
    "    output_sequence = []\n",
    "    \n",
    "    @jax.jit\n",
    "    def decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs):\n",
    "        output, new_hidden_state, new_cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "        predicted = jnp.argmax(output, axis=-1)\n",
    "        return predicted, new_hidden_state, new_cell_state\n",
    "    \n",
    "    for _ in range(tgt_seq_length):\n",
    "        predicted, hidden_state, cell_state = decode_step(decoder_input, hidden_state, cell_state, decoder_variables, encoder_outputs)\n",
    "        output_sequence.append(int(predicted.item()))\n",
    "        decoder_input = predicted\n",
    "    \n",
    "    print(f\"Input: {test_input.tolist()}, Output: {output_sequence}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "hidden_state = jnp.zeros((num_layers, 1, hidden_dim))\n",
    "cell_state = jnp.zeros((num_layers, 1, hidden_dim))\n",
    "\n",
    "\n",
    "Error:\n",
    "The PyTorch code directly uses the encoder output as the initial state of the decoder to pass context information.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modified to use enc_hidden and enc_cell returned by the encoder as the decoder initial state\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "hidden_state, cell_state = enc_hidden, enc_cell\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# In main(), only the inference decoding process is implemented, without the training loop\n",
    "\n",
    "\n",
    "Error:\n",
    "Compared to the PyTorch code, the JAX code lacks the implementation of the training loop, loss function calculation, optimizer updates, and teacher forcing\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add training data generation, cross entropy loss function, optax-based Adam optimizer, and a training loop with teacher forcing at each time step\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "batch_size = 16\n",
    "\n",
    "src_data = jax.random.randint(rng, (batch_size, src_seq_length), 0, src_vocab_size)\n",
    "tgt_data = jax.random.randint(rng, (batch_size, tgt_seq_length), 0, tgt_vocab_size)\n",
    "\n",
    "tx = optax.adam(0.001)\n",
    "state = train_state.TrainState.create(apply_fn=None, params=params, tx=tx)\n",
    "\n",
    "\n",
    "def loss_fn(params, encoder, decoder, src, tgt):\n",
    "    encoder_outputs, (enc_hidden, enc_cell) = encoder.apply({'params': params['encoder']}, src)\n",
    "    loss = 0.0\n",
    "    hidden_state, cell_state = enc_hidden, enc_cell\n",
    "    decoder_input = jnp.zeros((src.shape[0],), dtype=jnp.int32)\n",
    "    for t in range(tgt.shape[1]):\n",
    "        logits, hidden_state, cell_state = decoder.apply({'params': params['decoder']}, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "        loss += jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, tgt[:, t]))\n",
    "        decoder_input = tgt[:, t]\n",
    "    return loss\n",
    "\n",
    "\n",
    "def create_train_state(rng, encoder, decoder, src_vocab_size, tgt_vocab_size, src_seq_length):\n",
    "    encoder_variables = encoder.init(rng, jnp.ones((1, src_seq_length), jnp.int32))\n",
    "    decoder_variables = decoder.init(\n",
    "        rng,\n",
    "        jnp.ones((1,), jnp.int32),\n",
    "        jnp.ones((1, src_seq_length, encoder.hidden_dim)),\n",
    "        jnp.ones((encoder.num_layers, 1, encoder.hidden_dim)),\n",
    "        jnp.ones((encoder.num_layers, 1, encoder.hidden_dim))\n",
    "    )\n",
    "    params = {\n",
    "        'encoder': encoder_variables['params'],\n",
    "        'decoder': decoder_variables['params']\n",
    "    }\n",
    "    tx = optax.adam(0.001)\n",
    "    return train_state.TrainState.create(apply_fn=None, params=params, tx=tx)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "decoder_input = predicted\n",
    "\n",
    "\n",
    "Error:\n",
    "The JAX code does not use teacher forcing\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "In the training loop, the target token of the current time step should be used as the input of the next decoder step\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "for t in range(tgt.shape[1]):\n",
    "    logits, hidden_state, cell_state = decoder.apply({'params': params['decoder']}, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    loss += jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, tgt[:, t]))\n",
    "    decoder_input = tgt[:, t]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# No corresponding training information\n",
    "\n",
    "\n",
    "Error:\n",
    "Lack of training information, updated gradients, loss and log printing\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add corresponding training information, update gradient, loss and log printing\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, encoder, decoder, src, tgt):\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params, encoder, decoder, src, tgt)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    state, loss = train_step(state, encoder, decoder, src_data, tgt_data)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "@jax.jit\n",
    "def train_step(state, encoder, decoder, src, tgt):\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params, encoder, decoder, src, tgt)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "    \n",
    "\n",
    "Error:\n",
    "Cannot interpret value of type <class 'main.Encoder'> as an abstract array; it does not have a dtype attribute\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Setting the static_argnums parameter in the jax.jit decorator\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@jax.jit(static_argnums=(1, 2))\n",
    "def train_step(state, encoder, decoder, src, tgt):\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params, encoder, decoder, src, tgt)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "@jax.jit(static_argnums=(1, 2))\n",
    "def train_step(state, encoder, decoder, src, tgt):\n",
    "\n",
    "\n",
    "Error:\n",
    "jit() missing 1 required positional argument: 'fun'\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use Python's built-in partial to fix static parameters and then use it as a decorator\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1, 2))\n",
    "def train_step(state, encoder, decoder, src, tgt):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b2e533",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "error",
     "timestamp": 1743290906892,
     "user": {
      "displayName": "Tracy Tang",
      "userId": "12916462288005203507"
     },
     "user_tz": 300
    },
    "id": "45b2e533",
    "outputId": "15a00351-d9e8-46fb-8d1f-3a1f0e9e928a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] - Loss: 34.3292\n",
      "Epoch [20/100] - Loss: 27.7096\n",
      "Epoch [30/100] - Loss: 19.8640\n",
      "Epoch [40/100] - Loss: 14.5860\n",
      "Epoch [50/100] - Loss: 11.3062\n",
      "Epoch [60/100] - Loss: 9.1747\n",
      "Epoch [70/100] - Loss: 7.5736\n",
      "Epoch [80/100] - Loss: 6.2618\n",
      "Epoch [90/100] - Loss: 5.1877\n",
      "Epoch [100/100] - Loss: 4.3040\n",
      "Input: [[15, 12, 3, 7, 15, 15, 9, 16, 12, 9]], Output: [4, 16, 16, 12, 12, 12, 4, 5, 4, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "#Fixed Code\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.input_dim, features=self.embed_dim)\n",
    "        self.lstm_cells = [nn.LSTMCell(features=self.hidden_dim) for _ in range(self.num_layers)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x: (batch, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch, seq_length, embed_dim)\n",
    "        batch, seq_length, _ = embedded.shape\n",
    "\n",
    "        hidden_states = [jnp.zeros((batch, self.hidden_dim)) for _ in range(self.num_layers)]\n",
    "        cell_states = [jnp.zeros((batch, self.hidden_dim)) for _ in range(self.num_layers)]\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = embedded[:, t, :]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                (cell_states[i], hidden_states[i]), x_t = cell((cell_states[i], hidden_states[i]), x_t)\n",
    "            outputs.append(x_t)\n",
    "        outputs = jnp.stack(outputs, axis=1)  # (batch, seq_length, hidden_dim)\n",
    "\n",
    "        hidden_states = jnp.stack(hidden_states, axis=0)\n",
    "        cell_states = jnp.stack(cell_states, axis=0)\n",
    "        return outputs, (hidden_states, cell_states)\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    output_dim: int\n",
    "    embed_dim: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    src_seq_length: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(num_embeddings=self.output_dim, features=self.embed_dim)\n",
    "        self.attention = nn.Dense(self.src_seq_length)\n",
    "        self.attention_combine = nn.Dense(self.embed_dim)\n",
    "        self.lstm_cells = [nn.LSTMCell(features=self.hidden_dim) for _ in range(self.num_layers)]\n",
    "        self.fc_out = nn.Dense(self.output_dim)\n",
    "\n",
    "    def __call__(self, decoder_input, encoder_outputs, hidden_state, cell_state):\n",
    "        # decoder_input: (batch,) 或 (batch, 1)\n",
    "        embedded = self.embedding(decoder_input)  # (batch, embed_dim) 或 (batch, 1, embed_dim)\n",
    "        if embedded.ndim == 3:\n",
    "            embedded = embedded.squeeze(1)  # (batch, embed_dim)\n",
    "\n",
    "        concat_input = jnp.concatenate([embedded, hidden_state[-1]], axis=-1)  # (batch, embed_dim + hidden_dim)\n",
    "        attention_scores = self.attention(concat_input)  # (batch, src_seq_length)\n",
    "        attention_weights = jax.nn.softmax(attention_scores, axis=-1)\n",
    "        context_vector = jnp.einsum('bs,bsh->bh', attention_weights, encoder_outputs)  # (batch, hidden_dim)\n",
    "\n",
    "        combined = jnp.concatenate([embedded, context_vector], axis=-1)  # (batch, embed_dim + hidden_dim)\n",
    "        combined = jax.nn.tanh(self.attention_combine(combined))  # (batch, embed_dim)\n",
    "        \n",
    "        new_hidden_states = []\n",
    "        new_cell_states = []\n",
    "        x = combined\n",
    "\n",
    "        for i, cell in enumerate(self.lstm_cells):\n",
    "            (new_cell, new_hidden), x = cell((cell_state[i], hidden_state[i]), x)\n",
    "            new_hidden_states.append(new_hidden)\n",
    "            new_cell_states.append(new_cell)\n",
    "        new_hidden_states = jnp.stack(new_hidden_states, axis=0)  # (num_layers, batch, hidden_dim)\n",
    "        new_cell_states = jnp.stack(new_cell_states, axis=0)      # (num_layers, batch, hidden_dim)\n",
    "        output = self.fc_out(x)  # (batch, output_dim)\n",
    "        return output, new_hidden_states, new_cell_states\n",
    "\n",
    "\n",
    "def loss_fn(params, encoder, decoder, src, tgt):\n",
    "    encoder_outputs, (enc_hidden, enc_cell) = encoder.apply({'params': params['encoder']}, src)\n",
    "    loss = 0.0\n",
    "    batch_size = src.shape[0]\n",
    "    hidden_state, cell_state = enc_hidden, enc_cell\n",
    "\n",
    "    decoder_input = jnp.zeros((batch_size,), dtype=jnp.int32)\n",
    "    tgt_seq_length = tgt.shape[1]\n",
    "    for t in range(tgt_seq_length):\n",
    "        logits, hidden_state, cell_state = decoder.apply({'params': params['decoder']},\n",
    "                                                           decoder_input,\n",
    "                                                           encoder_outputs,\n",
    "                                                           hidden_state,\n",
    "                                                           cell_state)\n",
    "        loss += jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, tgt[:, t]))\n",
    "\n",
    "        decoder_input = tgt[:, t]\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def create_train_state(rng, encoder, decoder, src_vocab_size, tgt_vocab_size, src_seq_length):\n",
    "    encoder_variables = encoder.init(rng, jnp.ones((1, src_seq_length), jnp.int32))\n",
    "    decoder_variables = decoder.init(\n",
    "        rng,\n",
    "        jnp.ones((1,), jnp.int32),\n",
    "        jnp.ones((1, src_seq_length, encoder.hidden_dim)),\n",
    "        jnp.ones((encoder.num_layers, 1, encoder.hidden_dim)),\n",
    "        jnp.ones((encoder.num_layers, 1, encoder.hidden_dim))\n",
    "    )\n",
    "    params = {\n",
    "        'encoder': encoder_variables['params'],\n",
    "        'decoder': decoder_variables['params']\n",
    "    }\n",
    "    tx = optax.adam(0.001)\n",
    "    return train_state.TrainState.create(apply_fn=None, params=params, tx=tx)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1, 2))\n",
    "def train_step(state, encoder, decoder, src, tgt):\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params, encoder, decoder, src, tgt)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example parameters\n",
    "    src_vocab_size = 20\n",
    "    tgt_vocab_size = 20\n",
    "    src_seq_length = 10\n",
    "    tgt_seq_length = 12\n",
    "    batch_size = 1 \n",
    "    embed_dim = 32\n",
    "    hidden_dim = 64\n",
    "    num_layers = 2\n",
    "\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    \n",
    "    encoder = Encoder(input_dim=src_vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "    decoder = Decoder(output_dim=tgt_vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                      num_layers=num_layers, src_seq_length=src_seq_length)\n",
    "    \n",
    "    src_data = jax.random.randint(rng, (batch_size, src_seq_length), 0, src_vocab_size)\n",
    "    tgt_data = jax.random.randint(rng, (batch_size, tgt_seq_length), 0, tgt_vocab_size)\n",
    "    \n",
    "    state = create_train_state(rng, encoder, decoder, src_vocab_size, tgt_vocab_size, src_seq_length)\n",
    "    \n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        state, loss = train_step(state, encoder, decoder, src_data, tgt_data)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss:.4f}\")\n",
    "    \n",
    "    test_input = jax.random.randint(rng, (1, src_seq_length), 0, src_vocab_size)\n",
    "    encoder_variables = encoder.init(rng, test_input)\n",
    "    encoder_outputs, (enc_hidden, enc_cell) = encoder.apply(encoder_variables, test_input)\n",
    "    \n",
    "    hidden_state = jnp.zeros((num_layers, 1, hidden_dim))\n",
    "    cell_state = jnp.zeros((num_layers, 1, hidden_dim))\n",
    "    \n",
    "    decoder_input = jnp.array([0])  \n",
    "    decoder_variables = decoder.init(rng, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "    \n",
    "    output_sequence = []\n",
    "    \n",
    "    @jax.jit\n",
    "    def decode_step(decoder_input, hidden_state, cell_state, variables, encoder_outputs):\n",
    "        output, new_hidden_state, new_cell_state = decoder.apply(variables, decoder_input, encoder_outputs, hidden_state, cell_state)\n",
    "        predicted = jnp.argmax(output, axis=-1)\n",
    "        return predicted, new_hidden_state, new_cell_state\n",
    "    \n",
    "    for _ in range(tgt_seq_length):\n",
    "        predicted, hidden_state, cell_state = decode_step(decoder_input, hidden_state, cell_state, decoder_variables, encoder_outputs)\n",
    "        output_sequence.append(int(predicted.item()))\n",
    "        decoder_input = predicted\n",
    "    \n",
    "    print(f\"Input: {test_input.tolist()}, Output: {output_sequence}\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52b6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
