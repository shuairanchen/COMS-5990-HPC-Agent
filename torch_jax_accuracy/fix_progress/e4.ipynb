{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26280edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.8869\n",
      "Epoch [200/1000], Loss: 0.7855\n",
      "Epoch [300/1000], Loss: 0.6945\n",
      "Epoch [400/1000], Loss: 0.6134\n",
      "Epoch [500/1000], Loss: 0.5433\n",
      "Epoch [600/1000], Loss: 0.4861\n",
      "Epoch [700/1000], Loss: 0.4404\n",
      "Epoch [800/1000], Loss: 0.4045\n",
      "Epoch [900/1000], Loss: 0.3767\n",
      "Epoch [1000/1000], Loss: 0.3551\n",
      "Learned weight: 2.0713, Learned bias: 2.4650\n",
      "Predictions for [[4.0], [7.0]]: [[10.750251770019531], [16.964160919189453]]\n"
     ]
    }
   ],
   "source": [
    "#Input\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
    "y = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n",
    "\n",
    "\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(HuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate the absolute error\n",
    "        error = torch.abs(y_pred - y_true)\n",
    "        \n",
    "        # Apply the Huber loss formula\n",
    "        loss = torch.where(error <= self.delta,\n",
    "                           0.5 * error**2,  # L2 loss for small errors\n",
    "                           self.delta * (error - 0.5 * self.delta))  # L1 loss for large errors\n",
    "        return loss.mean()  # Return the mean loss across all samples\n",
    "\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # Single input and single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "criterion = HuberLoss(delta=1.0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Display the learned parameters\n",
    "[w, b] = model.linear.parameters()\n",
    "print(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0], [7.0]])\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a98d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.7171\n",
      "Epoch [200/1000], Loss: 0.6591\n",
      "Epoch [300/1000], Loss: 0.6122\n",
      "Epoch [400/1000], Loss: 0.5753\n",
      "Epoch [500/1000], Loss: 0.5469\n",
      "Epoch [600/1000], Loss: 0.5257\n",
      "Epoch [700/1000], Loss: 0.5097\n",
      "Epoch [800/1000], Loss: 0.4977\n",
      "Epoch [900/1000], Loss: 0.4887\n",
      "Epoch [1000/1000], Loss: 0.4819\n",
      "Learned weight: 2.1092, Learned bias: 2.4387\n",
      "Predictions for [[4.0], [7.0]]: [[10.875606536865234], [17.203285217285156]]\n"
     ]
    }
   ],
   "source": [
    "#Strong LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set the random seed and create a PRNG key\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Generate synthetic data: 100 data points between 0 and 10 with noise\n",
    "key, subkey = jax.random.split(key)\n",
    "X = jax.random.uniform(subkey, shape=(100, 1)) * 10\n",
    "key, subkey = jax.random.split(key)\n",
    "noise = jax.random.normal(subkey, shape=(100, 1))\n",
    "y = 2 * X + 3 + noise  # y = 2*x + 3 + noise\n",
    "\n",
    "# Define the linear regression model function\n",
    "def predict(params, x):\n",
    "    # Computes a linear transformation: x * w + b\n",
    "    return jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "\n",
    "# Define the Huber loss function\n",
    "def huber_loss(params, x, y, delta=1.0):\n",
    "    preds = predict(params, x)\n",
    "    error = jnp.abs(preds - y)\n",
    "    loss = jnp.where(error <= delta,\n",
    "                     0.5 * error**2,            # L2 loss for small errors\n",
    "                     delta * (error - 0.5 * delta))  # L1 loss for large errors\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "# Initialize parameters for a linear layer with 1 input and 1 output.\n",
    "bound = 1.0  # Using a simple uniform initialization bound\n",
    "key, subkey = jax.random.split(key)\n",
    "w = jax.random.uniform(subkey, shape=(1, 1), minval=-bound, maxval=bound)\n",
    "key, subkey = jax.random.split(key)\n",
    "b = jax.random.uniform(subkey, shape=(1,), minval=-bound, maxval=bound)\n",
    "params = {\"w\": w, \"b\": b}\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Create a function that returns loss and gradients with respect to the parameters.\n",
    "loss_and_grad = jax.value_and_grad(huber_loss, argnums=0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss, grads = loss_and_grad(params, X, y, 1.0)\n",
    "    # Parameter update using SGD\n",
    "    params[\"w\"] = params[\"w\"] - lr * grads[\"w\"]\n",
    "    params[\"b\"] = params[\"b\"] - lr * grads[\"b\"]\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "learned_w = params[\"w\"][0, 0]\n",
    "learned_b = params[\"b\"][0]\n",
    "print(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")\n",
    "\n",
    "\n",
    "X_test = jnp.array([[4.0], [7.0]])\n",
    "predictions = predict(params, X_test)\n",
    "print(f\"Predictions for {np.array(X_test).tolist()}: {np.array(predictions).tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85163353",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument '<function update.<locals>.<lambda> at 0x000001D545DF03A0>' of type <class 'function'> is not a valid JAX type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, bias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m y \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m2.0\u001b[39m], [\u001b[38;5;241m4.0\u001b[39m], [\u001b[38;5;241m6.0\u001b[39m]])\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m     48\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(x)\n",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(key, model, x, y, epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(key, model, x, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \u001b[38;5;66;03m# MODIFIED\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m         model\u001b[38;5;241m.\u001b[39mw, model\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# MODIFIED\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(params, x, y, learning_rate)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(params, x, y, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m     23\u001b[0m     w, b \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m---> 24\u001b[0m     loss_value, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m     b \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;241m1\u001b[39m]\n",
      "    \u001b[1;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\dispatch.py:282\u001b[0m, in \u001b[0;36mcheck_arg\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_arg\u001b[39m(arg: Any):\n\u001b[0;32m    281\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arg, core\u001b[38;5;241m.\u001b[39mTracer) \u001b[38;5;129;01mor\u001b[39;00m core\u001b[38;5;241m.\u001b[39mvalid_jaxtype(arg)):\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument '<function update.<locals>.<lambda> at 0x000001D545DF03A0>' of type <class 'function'> is not a valid JAX type."
     ]
    }
   ],
   "source": [
    "#Weak LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, vmap\n",
    "import optax\n",
    "\n",
    "# Define a simple model\n",
    "class LinearModel:\n",
    "    def __init__(self, key):\n",
    "        self.w = random.normal(key, (1,))\n",
    "        self.b = random.normal(key, ())\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(model, x, y):\n",
    "    preds = model(x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Update function using functional programming\n",
    "def update(params, x, y, learning_rate=0.1):\n",
    "    w, b = params\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(lambda x: model(x), x, y)\n",
    "    w -= learning_rate * grads[0]\n",
    "    b -= learning_rate * grads[1]\n",
    "    return w, b\n",
    "\n",
    "# Training function\n",
    "def train_model(key, model, x, y, epochs=100):\n",
    "    for epoch in range(epochs):  # MODIFIED\n",
    "        model.w, model.b = update((model.w, model.b), x, y)  # MODIFIED\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Generate synthetic data\n",
    "    key = random.PRNGKey(0)  # MODIFIED: Explicit PRNG key\n",
    "    model = LinearModel(key)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    x = jnp.array([[1.0], [2.0], [3.0]])\n",
    "    y = jnp.array([[2.0], [4.0], [6.0]])\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(key, model, x, y, epochs=100)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model(x)\n",
    "    print(f\"Predictions for {x.tolist()}: {predictions.tolist()}\")\n",
    "    print(f\"Trained weights: {model.w}, bias: {model.b}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error Code:\n",
    "def update(params, x, y, learning_rate=0.1):\n",
    "    w, b = params\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(lambda x: model(x), x, y)\n",
    "    w -= learning_rate * grads[0]\n",
    "    b -= learning_rate * grads[1]\n",
    "    return w, b\n",
    "\n",
    "\n",
    "Error:\n",
    "Argument '<function update.<locals>.<lambda> at 0x000001D545DF03A0>' of type <class 'function'> is not a valid JAX type\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Extract the model logic from the class method and define a pure function that accepts a parameter tuple (w, b) and input x and returns the prediction result\n",
    "Change loss_fn to receive parameters (w, b) instead of the entire model instance, and use jax.value_and_grad to directly calculate the gradient of the parameters\n",
    "In update, directly pass the parameter tuple to loss_fn to avoid using lambda functions\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def model_fn(params, x):\n",
    "    w, b = params\n",
    "    return jnp.dot(x, w) + b\n",
    "\n",
    "def update(params, x, y, learning_rate=0.1):\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    w, b = params\n",
    "    w = w - learning_rate * grads[0]\n",
    "    b = b - learning_rate * grads[1]\n",
    "    return (w, b)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "class LinearModel:\n",
    "    def __init__(self, key):\n",
    "        self.w = random.normal(key, (1,))\n",
    "        self.b = random.normal(key, ())\n",
    "\n",
    "Error:\n",
    "Weights should be a 2D matrix (shape (1, 1)) to perform correct matrix multiplication with x\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the parameter initialization, set the shape of w to (1,1) and the shape of b to (1,)\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class LinearModel:\n",
    "    def __init__(self, key):\n",
    "        self.w = random.normal(key, (1, 1))\n",
    "        self.b = random.normal(key, (1,))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def loss_fn(params, x, y):\n",
    "    preds = model_fn(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "    \n",
    "\n",
    "Error:\n",
    "The original code uses Huber loss, while the incorrect code here uses mean square error (MSE) as the loss function\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Change the loss function to Huber loss function, set delta=1.0\n",
    "And use the L2 part when the error is less than or equal to delta: 0.5 * error²\n",
    "And use the L1 part when the error is greater than delta: delta * (error - 0.5 * delta)\n",
    "\n",
    "Correct Code:\n",
    "def loss_fn(params, x, y, delta=1.0):\n",
    "    preds = model_fn(params, x)\n",
    "    error = jnp.abs(preds - y)\n",
    "    loss = jnp.where(error <= delta,\n",
    "                     0.5 * error**2, \n",
    "                     delta * (error - 0.5 * delta))\n",
    "    return jnp.mean(loss)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, vmap\n",
    "import optax\n",
    "\n",
    "\n",
    "Error:\n",
    "The optax module was not used later.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Remove optax module\n",
    "\n",
    "Correct Code:\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, vmap\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "model = train_model(key, model, x, y, epochs=100)\n",
    "\n",
    "def train_model(key, model, x, y, epochs=100):\n",
    "\n",
    "Error:\n",
    "The parameter key is not used during training function\n",
    "\n",
    "Fix Guide:\n",
    "Remove key parameter from train_model function\n",
    "\n",
    "Correct Code:\n",
    "model = train_model(model, x, y, epochs=100)\n",
    "\n",
    "def train_model(model, x, y, epochs=100):\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def __init__(self, key):\n",
    "    self.w = random.normal(key, (1, 1))\n",
    "    self.b = random.normal(key, (1,))\n",
    "\n",
    "\n",
    "Error:\n",
    "JAX requires that the PRNG key be split each time a random number is used.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use random.split to split the key and generate a separate sub-key for each random variable\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def __init__(self, key):\n",
    "    key, subkey = random.split(key)\n",
    "    self.w = random.normal(subkey, (1, 1))\n",
    "    key, subkey = random.split(key)\n",
    "    self.b = random.normal(subkey, (1,))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "class LinearModel:\n",
    "    def __init__(self, key):\n",
    "        key, subkey = random.split(key)\n",
    "        self.w = random.normal(subkey, (1, 1))\n",
    "        key, subkey = random.split(key)\n",
    "        self.b = random.normal(subkey, (1,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "\n",
    "Error:\n",
    "The parameters are stored in self.w and self.b respectively, and the update function during subsequent training uses the method of packing the parameters into a tuple and updating them, which is inconsistent with the original code's method of using a dictionary to store parameters.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Unified use of dictionary form to store parameters\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class LinearModel:\n",
    "    def __init__(self, key):\n",
    "        key, subkey = random.split(key)\n",
    "        w = random.uniform(subkey, (1, 1), minval=-1.0, maxval=1.0)\n",
    "        key, subkey = random.split(key)\n",
    "        b = random.uniform(subkey, (1,), minval=-1.0, maxval=1.0)\n",
    "        self.params = {\"w\": w, \"b\": b}\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.params[\"w\"]) + self.params[\"b\"]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def loss_fn(params, x, y, delta=1.0):\n",
    "    preds = model_fn(params, x)\n",
    "    error = jnp.abs(preds - y)\n",
    "    loss = jnp.where(error <= delta,\n",
    "                     0.5 * error**2, \n",
    "                     delta * (error - 0.5 * delta))\n",
    "    return jnp.mean(loss)\n",
    "    \n",
    "\n",
    "Error:\n",
    "Inconsistent naming and usage of loss functions\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Rename the loss function to huber_loss, explicitly pass in the delta parameter (such as 1.0) in the update function, and modify the parameters in dictionary form for internal calculations\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def huber_loss(params, x, y, delta=1.0):\n",
    "    preds = jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "    error = jnp.abs(preds - y)\n",
    "    loss = jnp.where(error <= delta,\n",
    "                     0.5 * error**2, \n",
    "                     delta * (error - 0.5 * delta))\n",
    "    return jnp.mean(loss)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def update(params, x, y, learning_rate=0.1):\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    w, b = params\n",
    "    w = w - learning_rate * grads[0]\n",
    "    b = b - learning_rate * grads[1]\n",
    "    return (w, b)\n",
    "\n",
    "\n",
    "Error:\n",
    "The loss_fn is called here, which needs to be changed to huber_loss\n",
    "The parameters are unpacked in tuple form, which is different from the previous code\n",
    "The learning rate value is different from the original code\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Change loss_fn to huber_loss\n",
    "Change tuple to dict\n",
    "Change lr to 0.01\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def update(params, x, y, learning_rate=0.01):\n",
    "    loss_value, grads = jax.value_and_grad(huber_loss)(params, x, y, 1.0)\n",
    "    params[\"w\"] = params[\"w\"] - learning_rate * grads[\"w\"]\n",
    "    params[\"b\"] = params[\"b\"] - learning_rate * grads[\"b\"]\n",
    "    return params\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_model(model, x, y, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        model.w, model.b = update((model.w, model.b), x, y)\n",
    "    return model\n",
    "\n",
    "Error:\n",
    "Use tuple unpacking instead of dictionary when updating\n",
    "No loss log is output during the entire training process, and the training progress cannot be observed\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "In the training function, update model.params using dict form\n",
    "Add log output statement\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def train_model(model, x, y, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        model.params = update(model.params, x, y, learning_rate=0.01)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            current_loss = huber_loss(model.params, x, y, 1.0)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}\")\n",
    "    return model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "x = jnp.array([[1.0], [2.0], [3.0]])\n",
    "y = jnp.array([[2.0], [4.0], [6.0]])\n",
    "\n",
    "\n",
    "Error:\n",
    "The original code used 100 data points and added noise\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Generate 100 samples using PRNGKey and add noise\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key, subkey = random.split(key)\n",
    "x = random.uniform(subkey, shape=(100, 1)) * 10\n",
    "key, subkey = random.split(key)\n",
    "noise = random.normal(subkey, shape=(100, 1))\n",
    "y = 2 * x + 3 + noise\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "print(f\"Trained weights: {model.w}, bias: {model.b}\")\n",
    "\n",
    "\n",
    "Error:\n",
    "The model parameters are stored in the dictionary self.params. There are no direct attributes w and b. Directly calling model.w and model.b will result in an error.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Take the weights and biases from model.params and modify them to model.params[\"w\"] and model.params[\"b\"]\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "print(f\"Trained weights: {model.params['w']}, bias: {model.params['b']}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "x = jnp.array([[1.0], [2.0], [3.0]])\n",
    "\n",
    "\n",
    "Error:\n",
    "Wrong test x cases\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Change to same as original code\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "x = jnp.array([[4.0], [7.0]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d87f99dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.8937\n",
      "Epoch [200/1000], Loss: 0.8241\n",
      "Epoch [300/1000], Loss: 0.7588\n",
      "Epoch [400/1000], Loss: 0.6986\n",
      "Epoch [500/1000], Loss: 0.6439\n",
      "Epoch [600/1000], Loss: 0.5948\n",
      "Epoch [700/1000], Loss: 0.5516\n",
      "Epoch [800/1000], Loss: 0.5159\n",
      "Epoch [900/1000], Loss: 0.4874\n",
      "Epoch [1000/1000], Loss: 0.4650\n",
      "Predictions for [[4.0], [7.0]]: [[10.567580223083496], [16.88758659362793]]\n",
      "Trained weights: [[2.1066687]], bias: [2.1409056]\n"
     ]
    }
   ],
   "source": [
    "#Fixed Code\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, vmap\n",
    "\n",
    "\n",
    "def model_fn(params, x):\n",
    "    w, b = params\n",
    "    return jnp.dot(x, w) + b\n",
    "\n",
    "# Define a simple model\n",
    "class LinearModel:\n",
    "    def __init__(self, key):\n",
    "        key, subkey = random.split(key)\n",
    "        w = random.uniform(subkey, (1, 1), minval=-1.0, maxval=1.0)\n",
    "        key, subkey = random.split(key)\n",
    "        b = random.uniform(subkey, (1,), minval=-1.0, maxval=1.0)\n",
    "        self.params = {\"w\": w, \"b\": b}\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.params[\"w\"]) + self.params[\"b\"]\n",
    "\n",
    "# Loss function\n",
    "def huber_loss(params, x, y, delta=1.0):\n",
    "    preds = jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "    error = jnp.abs(preds - y)\n",
    "    loss = jnp.where(error <= delta,\n",
    "                     0.5 * error**2, \n",
    "                     delta * (error - 0.5 * delta))\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "# Update function using functional programming\n",
    "def update(params, x, y, learning_rate=0.01):\n",
    "    loss_value, grads = jax.value_and_grad(huber_loss)(params, x, y, 1.0)\n",
    "    params[\"w\"] = params[\"w\"] - learning_rate * grads[\"w\"]\n",
    "    params[\"b\"] = params[\"b\"] - learning_rate * grads[\"b\"]\n",
    "    return params\n",
    "\n",
    "# Training function\n",
    "def train_model(model, x, y, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        model.params = update(model.params, x, y, learning_rate=0.01)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            current_loss = huber_loss(model.params, x, y, 1.0)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Generate synthetic data\n",
    "    key = random.PRNGKey(0)  # MODIFIED: Explicit PRNG key\n",
    "    model = LinearModel(key)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    key, subkey = random.split(key)\n",
    "    x = random.uniform(subkey, shape=(100, 1)) * 10\n",
    "    key, subkey = random.split(key)\n",
    "    noise = random.normal(subkey, shape=(100, 1))\n",
    "    y = 2 * x + 3 + noise\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(model, x, y, epochs=1000)\n",
    "\n",
    "    x = jnp.array([[4.0], [7.0]])\n",
    "    # Test the model\n",
    "    predictions = model(x)\n",
    "    print(f\"Predictions for {x.tolist()}: {predictions.tolist()}\")\n",
    "    print(f\"Trained weights: {model.params['w']}, bias: {model.params['b']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f53458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
