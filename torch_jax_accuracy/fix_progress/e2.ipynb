{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faa3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.5655\n",
      "Epoch [200/1000], Loss: 0.4624\n",
      "Epoch [300/1000], Loss: 1.4614\n",
      "Epoch [400/1000], Loss: 0.3983\n",
      "Epoch [500/1000], Loss: 0.5415\n",
      "Epoch [600/1000], Loss: 1.6767\n",
      "Epoch [700/1000], Loss: 1.0075\n",
      "Epoch [800/1000], Loss: 0.3245\n",
      "Epoch [900/1000], Loss: 0.6541\n",
      "Epoch [1000/1000], Loss: 1.6099\n",
      "Learned weight: 1.9207, Learned bias: 3.2333\n",
      "Predictions for [[4.0], [7.0]]: [[10.91616153717041], [16.67831039428711]]\n"
     ]
    }
   ],
   "source": [
    "#Input\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
    "y = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n",
    "\n",
    "# Save the generated data to data.csv\n",
    "data = torch.cat((X, y), dim=1)\n",
    "df = pd.DataFrame(data.numpy(), columns=['X', 'y'])\n",
    "df.to_csv('data.csv', index=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class LinearRegressionDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load data from CSV file\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.X = torch.tensor(self.data['X'].values, dtype=torch.float32).view(-1, 1)\n",
    "        self.y = torch.tensor(self.data['y'].values, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Example usage of the DataLoader\n",
    "dataset = LinearRegressionDataset('data.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # Single input and single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # Forward pass\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Display the learned parameters\n",
    "[w, b] = model.linear.parameters()\n",
    "print(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0], [7.0]])\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644d5e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.6549\n",
      "Epoch [200/1000], Loss: 1.5143\n",
      "Epoch [300/1000], Loss: 0.4336\n",
      "Epoch [400/1000], Loss: 0.7000\n",
      "Epoch [500/1000], Loss: 0.2754\n",
      "Epoch [600/1000], Loss: 0.6613\n",
      "Epoch [700/1000], Loss: 1.1929\n",
      "Epoch [800/1000], Loss: 1.9943\n",
      "Epoch [900/1000], Loss: 2.2525\n",
      "Epoch [1000/1000], Loss: 0.5789\n",
      "Learned weight: 1.9570, Learned bias: 2.8438\n",
      "Predictions for [[4.0], [7.0]]: [[10.671821594238281], [16.54287338256836]]\n"
     ]
    }
   ],
   "source": [
    "#Strong LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set the random seed and create a PRNG key\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Generate synthetic data: 100 data points between 0 and 10, with noise\n",
    "key, subkey = jax.random.split(key)\n",
    "X = jax.random.uniform(subkey, shape=(100, 1)) * 10\n",
    "key, subkey = jax.random.split(key)\n",
    "noise = jax.random.normal(subkey, shape=(100, 1))\n",
    "y = 2 * X + 3 + noise  # y = 2*x + 3 + noise\n",
    "\n",
    "# Save the generated data to 'data.csv'\n",
    "data = jnp.concatenate([X, y], axis=1)\n",
    "df = pd.DataFrame(np.array(data), columns=['X', 'y'])\n",
    "df.to_csv('data.csv', index=False)\n",
    "\n",
    "# Define a simple Dataset class that loads data from CSV\n",
    "class LinearRegressionDataset:\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        # Convert the pandas Series to JAX arrays (with float32 precision)\n",
    "        self.X = jnp.array(self.data['X'].values, dtype=jnp.float32).reshape(-1, 1)\n",
    "        self.y = jnp.array(self.data['y'].values, dtype=jnp.float32).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Define a DataLoader function to yield batches\n",
    "def data_loader(dataset, batch_size, shuffle=True):\n",
    "    n = len(dataset)\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, n, batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        yield dataset.X[batch_indices], dataset.y[batch_indices]\n",
    "\n",
    "# Create dataset and specify batch size\n",
    "dataset = LinearRegressionDataset('data.csv')\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize model parameters similar to nn.Linear(1, 1)\n",
    "# Here we initialize the weight (shape (1,1)) and bias (shape (1,))\n",
    "bound = 1.0  # Using a uniform distribution bound similar to PyTorch initialization\n",
    "key, subkey = jax.random.split(key)\n",
    "w = jax.random.uniform(subkey, shape=(1, 1), minval=-bound, maxval=bound)\n",
    "key, subkey = jax.random.split(key)\n",
    "b = jax.random.uniform(subkey, shape=(1,), minval=-bound, maxval=bound)\n",
    "params = {\"w\": w, \"b\": b}\n",
    "\n",
    "# Define the forward (prediction) function\n",
    "def predict(params, x):\n",
    "    return jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "def loss_fn(params, x, y):\n",
    "    preds = predict(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Get function to compute loss and its gradients\n",
    "loss_and_grad = jax.value_and_grad(loss_fn)\n",
    "\n",
    "# Training loop over epochs and batches\n",
    "for epoch in range(epochs):\n",
    "    # Loop over batches using our custom data_loader\n",
    "    for batch_X, batch_y in data_loader(dataset, batch_size, shuffle=True):\n",
    "        loss, grads = loss_and_grad(params, batch_X, batch_y)\n",
    "        # Update parameters using SGD\n",
    "        params[\"w\"] = params[\"w\"] - lr * grads[\"w\"]\n",
    "        params[\"b\"] = params[\"b\"] - lr * grads[\"b\"]\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# Extract and print learned weight and bias\n",
    "learned_w = params[\"w\"][0, 0]\n",
    "learned_b = params[\"b\"][0]\n",
    "print(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = jnp.array([[4.0], [7.0]])\n",
    "predictions = predict(params, X_test)\n",
    "print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d1976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weight: 0.4034, Learned bias: 0.0100\n",
      "Predictions for [[4.0], [7.0]]: [[1.6237629652023315, 1.9412795305252075, 1.1064223051071167, 1.2425427436828613, 4.557937145233154, -2.4911012649536133, -1.248417854309082, 0.6250647902488708, 0.5563428997993469, -1.6758862733840942], [2.83408784866333, 3.389741897583008, 1.928741455078125, 2.166952133178711, 7.968894958496094, -4.3669257164001465, -2.1922295093536377, 1.0863655805587769, 0.9661023020744324, -2.9402992725372314]]\n"
     ]
    }
   ],
   "source": [
    "#Weak LLM\n",
    "import jax\n",
    "import jax.numpy as jnp  # MODIFIED: Ensure consistent import\n",
    "from jax import grad, jit, random  # MODIFIED: PRNG keys usage\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "\n",
    "def create_train_state(rng, learning_rate):\n",
    "    model = SimpleNN()\n",
    "    params = model.init(rng, jnp.ones([1, 1]))  # Initialize with dummy input\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        predictions = state.apply_fn(params, batch['x'])\n",
    "        return jnp.mean((predictions - batch['y']) ** 2)\n",
    "\n",
    "    grads = grad(loss_fn)(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state\n",
    "\n",
    "def main():\n",
    "    rng = random.PRNGKey(0)  # Initialize PRNG key\n",
    "    learning_rate = 0.001\n",
    "    state = create_train_state(rng, learning_rate)\n",
    "    \n",
    "    # Example training loop (with dummy data)\n",
    "    for epoch in range(10):\n",
    "        batch = {'x': jnp.array([[1.0], [2.0]]), 'y': jnp.array([[2.0], [4.0]])}  # Dummy input and output\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    # Output learned parameters\n",
    "    w = state.params['params']['Dense_0']['kernel'].flatten()[0]\n",
    "    b = state.params['params']['Dense_0']['bias'].flatten()[0]\n",
    "    print(f\"Learned weight: {w:.4f}, Learned bias: {b:.4f}\")\n",
    "\n",
    "    # Testing on new data\n",
    "    X_test = jnp.array([[4.0], [7.0]])\n",
    "    predictions = state.apply_fn(state.params, X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":  # MODIFIED: Ensure entry point\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error Code:\n",
    "class SimpleNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "  \n",
    "  \n",
    "Error:\n",
    "The original task has 1 output, the shape of the predicted value does not match the target value, and the loss cannot be calculated correctly\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Change the shape of the predicted value to 1\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class SimpleNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# Example training loop (with dummy data)\n",
    "for epoch in range(10):\n",
    "    batch = {'x': jnp.array([[1.0], [2.0]]), 'y': jnp.array([[2.0], [4.0]])}  # Dummy input and output\n",
    "    state = train_step(state, batch)\n",
    "        \n",
    "        \n",
    "Error:\n",
    "Only fixed \"dummy\" data is used, and the part of loading synthetic data from CSV files and training by batches is missing, so it is impossible to achieve the same functionality as the original code\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add data loading functions and data loaders to read data from CSV files and train in batches\n",
    "Call to load data and train in batches\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    X = jnp.array(df['X'].values, dtype=jnp.float32).reshape(-1, 1)\n",
    "    y = jnp.array(df['y'].values, dtype=jnp.float32).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def data_loader(X, y, batch_size, shuffle=True):\n",
    "    n = X.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start in range(0, n, batch_size):\n",
    "        batch_idx = indices[start:start + batch_size]\n",
    "        yield {'x': X[batch_idx], 'y': y[batch_idx]}\n",
    "        \n",
    "\n",
    "X, y = load_data('data.csv')\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in data_loader(X, y, batch_size, shuffle=True):\n",
    "        state, loss = train_step(state, batch)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "grads = grad(loss_fn)(state.params)\n",
    "new_state = state.apply_gradients(grads=grads)\n",
    "return new_state\n",
    "\n",
    "\n",
    "Error:\n",
    "The current loss value is not returned in the training step, resulting in the inability to print log information to monitor the training process in the training loop\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the train_step function to return the updated state and the loss value of the current batch\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "grads = grad(loss_fn)(state.params)\n",
    "new_state = state.apply_gradients(grads=grads)\n",
    "loss = loss_fn(state.params)\n",
    "return new_state, loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004a05bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 199.2036\n",
      "Epoch [200/1000], Loss: 104.8455\n",
      "Epoch [300/1000], Loss: 81.0222\n",
      "Epoch [400/1000], Loss: 65.5604\n",
      "Epoch [500/1000], Loss: 33.5759\n",
      "Epoch [600/1000], Loss: 9.7464\n",
      "Epoch [700/1000], Loss: 5.4672\n",
      "Epoch [800/1000], Loss: 1.5301\n",
      "Epoch [900/1000], Loss: 2.3061\n",
      "Epoch [1000/1000], Loss: 1.0849\n",
      "Learned weight: 2.0726, Learned bias: 2.2719\n",
      "Predictions for [[4.0], [7.0]]: [[10.562368392944336], [16.78019905090332]]\n"
     ]
    }
   ],
   "source": [
    "#fixed\n",
    "import jax\n",
    "import jax.numpy as jnp  # MODIFIED: Ensure consistent import\n",
    "from jax import grad, jit, random  # MODIFIED: PRNG keys usage\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    X = jnp.array(df['X'].values, dtype=jnp.float32).reshape(-1, 1)\n",
    "    y = jnp.array(df['y'].values, dtype=jnp.float32).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def data_loader(X, y, batch_size, shuffle=True):\n",
    "    n = X.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start in range(0, n, batch_size):\n",
    "        batch_idx = indices[start:start + batch_size]\n",
    "        yield {'x': X[batch_idx], 'y': y[batch_idx]}\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\n",
    "def create_train_state(rng, learning_rate):\n",
    "    model = SimpleNN()\n",
    "    params = model.init(rng, jnp.ones([1, 1]))  # Initialize with dummy input\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        predictions = state.apply_fn(params, batch['x'])\n",
    "        return jnp.mean((predictions - batch['y']) ** 2)\n",
    "\n",
    "    grads = grad(loss_fn)(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    loss = loss_fn(state.params)\n",
    "    return new_state, loss\n",
    "\n",
    "def main():\n",
    "    rng = random.PRNGKey(0)  # Initialize PRNG key\n",
    "    learning_rate = 0.001\n",
    "    state = create_train_state(rng, learning_rate)\n",
    "    \n",
    "    X, y = load_data('data.csv')\n",
    "    batch_size = 32\n",
    "    epochs = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in data_loader(X, y, batch_size, shuffle=True):\n",
    "            state, loss = train_step(state, batch)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "    # Output learned parameters\n",
    "    w = state.params['params']['Dense_0']['kernel'].flatten()[0]\n",
    "    b = state.params['params']['Dense_0']['bias'].flatten()[0]\n",
    "    print(f\"Learned weight: {w:.4f}, Learned bias: {b:.4f}\")\n",
    "\n",
    "    # Testing on new data\n",
    "    X_test = jnp.array([[4.0], [7.0]])\n",
    "    predictions = state.apply_fn(state.params, X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":  # MODIFIED: Ensure entry point\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763398fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
