{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Ov4bImwAbY",
        "outputId": "a50fa7bd-d935-4fb1-99c9-a7716039c2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] - Loss: 3.9118\n",
            "Epoch [2/5] - Loss: 3.9113\n",
            "Epoch [3/5] - Loss: 3.9108\n",
            "Epoch [4/5] - Loss: 3.9103\n",
            "Epoch [5/5] - Loss: 3.9097\n",
            "Prediction for input [[15, 28, 33, 19, 37, 24, 48, 42, 33, 35]]: 46\n"
          ]
        }
      ],
      "source": [
        "## Original Code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "# Define a simple Language Model (e.g., an LSTM-based model)\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out[:, -1, :])  # Use the last hidden state for prediction\n",
        "        return self.softmax(output)\n",
        "\n",
        "# Create synthetic training data\n",
        "torch.manual_seed(42)\n",
        "vocab_size = 50\n",
        "seq_length = 10\n",
        "batch_size = 32\n",
        "X_train = torch.randint(0, vocab_size, (batch_size, seq_length))  # Random integer input\n",
        "y_train = torch.randint(0, vocab_size, (batch_size,))  # Random target words\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "embed_size = 64\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = criterion(output, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log progress every epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Now, we will quantize the model dynamically to reduce its size and improve inference speed\n",
        "# Quantization: Apply dynamic quantization to the language model\n",
        "quantized_model = quantize_dynamic(model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n",
        "\n",
        "# Save the quantized model\n",
        "torch.save(quantized_model.state_dict(), \"quantized_language_model.pth\")\n",
        "\n",
        "# Load the quantized model and test it\n",
        "quantized_model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "\n",
        "# Apply dynamic quantization on the model after defining it\n",
        "quantized_model = quantize_dynamic(quantized_model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n",
        "\n",
        "# quantized_model.load_state_dict(torch.load(\"quantized_language_model.pth\"))\n",
        "quantized_model.eval()\n",
        "test_input = torch.randint(0, vocab_size, (1, seq_length))\n",
        "with torch.no_grad():\n",
        "    prediction = quantized_model(test_input)\n",
        "    print(f\"Prediction for input {test_input.tolist()}: {prediction.argmax(dim=1).item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Strong LLM\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import pickle\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Define an LSTM stack that processes the sequence step by step.\n",
        "# We use Flax’s LSTMCell and a Python loop to build a multi-layer LSTM.\n",
        "# ---------------------------------------------------------------------\n",
        "class LSTMStack(nn.Module):\n",
        "    hidden_size: int\n",
        "    num_layers: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # x: (batch, seq_length, embed_size)\n",
        "        batch_size = x.shape[0]\n",
        "        # Initialize LSTM states (carry, hidden) for each layer.\n",
        "        states = [\n",
        "            nn.LSTMCell.initialize_carry(self.make_rng('lstm'), (batch_size,), self.hidden_size)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "        out = None\n",
        "        seq_length = x.shape[1]\n",
        "        # Process each time step sequentially.\n",
        "        for t in range(seq_length):\n",
        "            inp = x[:, t, :]\n",
        "            new_states = []\n",
        "            for i in range(self.num_layers):\n",
        "                # Create an LSTM cell for layer i (parameters are registered by name).\n",
        "                lstm_cell = nn.LSTMCell(name=f'lstm_cell_{i}', hidden_size=self.hidden_size)\n",
        "                # Update state and get output.\n",
        "                states[i], out = lstm_cell(states[i], inp)\n",
        "                # For next layer, use the output from the current layer.\n",
        "                inp = out\n",
        "                new_states.append(states[i])\n",
        "            states = new_states\n",
        "        # Return the output of the last time step (from the last layer).\n",
        "        return out\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Define the LanguageModel using Flax modules.\n",
        "# It embeds the input tokens, processes them through the LSTM stack,\n",
        "# applies a Dense layer, and returns softmax probabilities.\n",
        "# ---------------------------------------------------------------------\n",
        "class LanguageModel(nn.Module):\n",
        "    vocab_size: int\n",
        "    embed_size: int\n",
        "    hidden_size: int\n",
        "    num_layers: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # x has shape (batch, seq_length) containing token indices.\n",
        "        x_embed = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_size)(x)\n",
        "        lstm_out = LSTMStack(hidden_size=self.hidden_size, num_layers=self.num_layers)(x_embed)\n",
        "        logits = nn.Dense(features=self.vocab_size)(lstm_out)\n",
        "        probabilities = nn.softmax(logits)\n",
        "        return probabilities\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Create synthetic training data (similar to torch.randint).\n",
        "# ---------------------------------------------------------------------\n",
        "key = jax.random.PRNGKey(42)\n",
        "vocab_size = 50\n",
        "seq_length = 10\n",
        "batch_size = 32\n",
        "X_train = jax.random.randint(key, (batch_size, seq_length), 0, vocab_size)\n",
        "key, subkey = jax.random.split(key)\n",
        "y_train = jax.random.randint(subkey, (batch_size,), 0, vocab_size)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Initialize the model, loss, and optimizer.\n",
        "# ---------------------------------------------------------------------\n",
        "embed_size = 64\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "model = LanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_size=embed_size,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        ")\n",
        "\n",
        "# Initialize model parameters.\n",
        "# Note: We pass two PRNG keys – one for parameters and one for the LSTM initialization.\n",
        "variables = model.init({'params': key, 'lstm': key}, X_train)\n",
        "params = variables['params']\n",
        "\n",
        "# Define a simple cross-entropy loss.\n",
        "def loss_fn(params, x, y):\n",
        "    preds = model.apply({'params': params}, x)\n",
        "    # Compute the negative log likelihood for the true classes.\n",
        "    loss = -jnp.mean(jnp.log(preds[jnp.arange(preds.shape[0]), y] + 1e-7))\n",
        "    return loss\n",
        "\n",
        "# Set up the Adam optimizer using Optax.\n",
        "optimizer = optax.adam(learning_rate=0.001)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, x, y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state, loss\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Training loop (5 epochs).\n",
        "# ---------------------------------------------------------------------\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    params, opt_state, loss_val = train_step(params, opt_state, X_train, y_train)\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss_val:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Quantization: Simulate dynamic quantization by converting parameters to int8 and back.\n",
        "# This is only a simple simulation; JAX does not offer PyTorch-like dynamic quantization.\n",
        "# ---------------------------------------------------------------------\n",
        "def quantize_param(param):\n",
        "    scale = jnp.max(jnp.abs(param))\n",
        "    # Avoid division by zero.\n",
        "    scale = jnp.where(scale == 0, 1.0, scale)\n",
        "    # Scale, round, and cast to int8.\n",
        "    param_int8 = jnp.round(param / scale * 127).astype(jnp.int8)\n",
        "    # Dequantize back to float32.\n",
        "    return param_int8.astype(jnp.float32) * scale / 127\n",
        "\n",
        "def quantize_params(params):\n",
        "    if isinstance(params, dict):\n",
        "        return {k: quantize_params(v) for k, v in params.items()}\n",
        "    else:\n",
        "        return quantize_param(params)\n",
        "\n",
        "quantized_params = quantize_params(params)\n",
        "\n",
        "# Save the quantized parameters (similarly to torch.save).\n",
        "with open(\"quantized_language_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(quantized_params, f)\n",
        "\n",
        "# Load the quantized parameters.\n",
        "with open(\"quantized_language_model.pkl\", \"rb\") as f:\n",
        "    loaded_params = pickle.load(f)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Evaluate the quantized model.\n",
        "# ---------------------------------------------------------------------\n",
        "test_input = jax.random.randint(key, (1, seq_length), 0, vocab_size)\n",
        "predictions = model.apply({'params': loaded_params}, test_input)\n",
        "predicted_class = jnp.argmax(predictions, axis=1)\n",
        "print(f\"Prediction for input {test_input.tolist()}: {int(predicted_class[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "UndamHSyxcYf",
        "outputId": "0de20b33-a2c3-4a39-8cd3-dbbc1838adc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lstm_rngs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3143c3a8fb24>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Initialize model parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Note: We pass two PRNG keys – one for parameters and one for the LSTM initialization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3143c3a8fb24>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# x has shape (batch, seq_length) containing token indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3143c3a8fb24>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Initialize LSTM states (carry, hidden) for each layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         states = [\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_carry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_rngs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3143c3a8fb24>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Initialize LSTM states (carry, hidden) for each layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         states = [\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_carry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_rngs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     ]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm_rngs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Weak LLM\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import numpy as np\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    # Your LSTM implementation here\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Forward pass logic for LSTM\n",
        "\n",
        "\n",
        "def process_sequence(inputs, prng_key):  # MODIFIED: Added prng_key as a parameter\n",
        "    # Instead of using Python loops, use a JAX scan to process the sequence\n",
        "    def step(carry, input_data):\n",
        "        # Define the operation per timestep\n",
        "        # Note: Include the logic for LSTM cell operations here\n",
        "        carry = carry  # update the carry state here based on LSTM operations\n",
        "        return carry, carry  # return updated state and output\n",
        "\n",
        "    # Use `jax.lax.scan` for efficient looping over the inputs\n",
        "    initial_carry = jnp.zeros((inputs.shape[0],))  # or appropriate shape\n",
        "    outputs, _ = jax.lax.scan(step, initial_carry, inputs)\n",
        "    return outputs\n",
        "\n",
        "def loss_fn(params, X, y):\n",
        "    # Your loss function implementation here\n",
        "    return jnp.mean((X - y) ** 2)  # Example loss calculation\n",
        "\n",
        "def main():\n",
        "    # Initialize your parameters and data here\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    num_epochs = 100\n",
        "    key = jax.random.PRNGKey(0)  # Initialize PRNG key\n",
        "\n",
        "    # Example inputs; replace with actual data loading logic\n",
        "    X_train = jax.random.normal(key, (batch_size, input_size))\n",
        "    y_train = jax.random.normal(key, (batch_size, input_size))\n",
        "\n",
        "    # Initialize model, optimizer, etc.\n",
        "    model = LSTM()  # Initialize the LSTM model\n",
        "    params = model.init(key, X_train)  # Initialize model parameters\n",
        "    optimizer = optax.adam(learning_rate=0.001)  # Example optimizer\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        key, subkey = jax.random.split(key)  # MODIFIED: Split the PRNG key for each iteration\n",
        "        outputs = process_sequence(X_train, subkey)  # MODIFIED: Pass subkey to process_sequence\n",
        "        current_loss = loss_fn(params, outputs, y_train)  # Calculate loss based on outputs\n",
        "\n",
        "        # Update weights, optimizer state, etc.\n",
        "        grad = jax.grad(loss_fn)(params, outputs, y_train)  # Compute gradients\n",
        "        updates, opt_state = optimizer.update(grad, opt_state)  # Update optimizer state\n",
        "        params = optax.apply_updates(params, updates)  # Apply updates to parameters\n",
        "\n",
        "        # Log progress every epoch\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {current_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SqC5PONZywcG",
        "outputId": "996c44a2-9d8b-4912-89b5-39643002298a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Incompatible shapes for broadcasting: shapes=[(32,), (32, 10)]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_try_broadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrank_promoted_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'broadcast_shapes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_try_broadcast_shapes\u001b[0;34m(name, *shapes)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         raise TypeError(f'{name} got incompatible shapes for broadcasting: '\n\u001b[0m\u001b[1;32m    133\u001b[0m                         f'{\", \".join(map(str, map(tuple, shapes)))}.')\n",
            "\u001b[0;31mTypeError\u001b[0m: broadcast_shapes got incompatible shapes for broadcasting: (1, 32), (32, 10).",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       return cached(config.trace_context() if trace_context_in_key else _ignore(),\n\u001b[0m\u001b[1;32m    303\u001b[0m                     *args, **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mcached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_broadcast_shapes_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_shapes_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Raise ValueError here for backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incompatible shapes for broadcasting: shapes={list(shapes)}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(32,), (32, 10)]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_try_broadcast_shapes\u001b[0;34m(name, *shapes)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         raise TypeError(f'{name} got incompatible shapes for broadcasting: '\n\u001b[0m\u001b[1;32m    133\u001b[0m                         f'{\", \".join(map(str, map(tuple, shapes)))}.')\n",
            "\u001b[0;31mTypeError\u001b[0m: broadcast_shapes got incompatible shapes for broadcasting: (1, 32), (32, 10).",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f410a8e2ea56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f410a8e2ea56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# MODIFIED: Split the PRNG key for each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# MODIFIED: Pass subkey to process_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate loss based on outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Update weights, optimizer state, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f410a8e2ea56>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(params, X, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Your loss function implementation here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/ufunc_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"where argument of {self}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__static_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'call'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_vectorized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/ufuncs.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0mArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m-\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m   \"\"\"\n\u001b[0;32m-> 1544\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpromote_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subtract\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0m_check_no_float0s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m   \u001b[0mcheck_for_prngkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpromote_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpromote_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_rank_promotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"allow\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           \u001b[0m_rank_promotion_warning_or_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mresult_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_rank\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    216\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Raise ValueError here for backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incompatible shapes for broadcasting: shapes={list(shapes)}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast_shardings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mavals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(32,), (32, 10)]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Error Code\n",
        "def loss_fn(params, X, y):\n",
        "    # Your loss function implementation here\n",
        "    return jnp.mean((X - y) ** 2)  # Example loss calculation\n",
        "\n",
        "Error:\n",
        "\n",
        "ValueError: Incompatible shapes for broadcasting: shapes=[(32,), (32, 10)]\n",
        "Fix guide:\n",
        "Fix broadcasting shape error and RNG issue\n",
        "\n",
        "Correct code\n",
        " def __call__(self, x):\n",
        "    # Forward pass logic for LSTM\n",
        "    rng = self.make_rng('lstm')\n",
        "    return process_sequence(x, rng)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Error Code\n",
        "outputs = jnp.swapaxes(outputs, 0, 1)\n",
        "return outputs\n",
        "\n",
        "Error:\n",
        "IndexError: index 1 is out of bounds for axis 0 with size 1\n",
        "\n",
        "Fix guide\n",
        "Add the @nn.compact Decorator @nn.compact\n",
        "\n",
        "Correct code\n",
        "@nn.compact\n",
        "def __call__(self, x):\n",
        "  # Dummy Dense layer to force parameter creation and proper input tracing.\n",
        "  x = nn.Dense(features=x.shape[-1], name='dummy_dense')(x)\n",
        "  rng = self.make_rng('lstm')\n",
        "  return process_sequence(x, rng)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Error Code\n",
        "x = nn.Dense(features=x.shape[-1], name='dummy_dense')(x)\n",
        "Error:\n",
        "AssignSubModuleError: Submodule Dense must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.AssignSubModuleError)\n",
        "\n",
        "Fix guide\n",
        "\n",
        "Define the Submodule in setup():\n",
        "Create the Dense submodule in the module’s setup() method, where you can use a fixed (or pre-determined) number of output features. For instance, if you know the sequence length in advance, use that as the feature count.\n",
        "Pass the Feature Count as an Argument:\n",
        "Modify the module to accept a features parameter. Then, in setup(), instantiate the dummy Dense layer using this parameter.\n",
        "Update the __call__ Method:\n",
        "In __call__, call the pre-defined Dense submodule and then continue with your processing logic.\n",
        "\n",
        "Correct Code\n",
        "x = self.dummy_dense(x)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Error Code\n",
        "outputs, _ = jax.lax.scan(step, initial_carry, inputs)\n",
        "outputs = jnp.swapaxes(outputs, 0, 1)\n",
        "\n",
        "Error\n",
        "IndexError: index 1 is out of bounds for axis 0 with size 1\n",
        "\n",
        "Fix guide\n",
        "ensure outputs has at least 2 dimensions before calling jnp.swapaxes\n",
        "\n",
        "Correct Code\n",
        "if outputs.ndim == 1:  # If outputs is 1D, add a dimension\n",
        "      outputs = outputs[:, None]  # Shape: (seq_length,) -> (seq_length, 1)\n",
        "      outputs = jnp.swapaxes(outputs, 0, 1)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Error Code\n",
        "if outputs.ndim == 1:  # If outputs is 1D, add a dimension\n",
        "      outputs = outputs[:, None]  # Shape: (seq_length,) -> (seq_length, 1)\n",
        "      outputs = jnp.swapaxes(outputs, 0, 1\n",
        "\n",
        "Error\n",
        "IndexError: index 1 is out of bounds for axis 0 with size 1\n",
        "\n",
        "Fix Guide\n",
        "ensure outputs is at least 2D before calling jnp.swapaxes\n",
        "\n",
        "Correct Code\n",
        "if outputs.ndim == 1:\n",
        "  outputs = outputs[None, :]\n",
        "outputs = jnp.swapaxes(outputs, 0, 1)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rudGUhXGyzXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Fixed Code\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    features: int  # Expected to be the sequence length (e.g., 10)\n",
        "\n",
        "    def setup(self):\n",
        "        # Define the Dense submodule with a fixed feature size.\n",
        "        self.dummy_dense = nn.Dense(features=self.features, name='dummy_dense')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Use the pre-defined Dense layer.\n",
        "        x = self.dummy_dense(x)\n",
        "        rng = self.make_rng('lstm')\n",
        "        return process_sequence(x, rng)\n",
        "\n",
        "def process_sequence(inputs, prng_key):\n",
        "    # Ensure inputs is 2D (batch, seq_length).\n",
        "    assert inputs.ndim == 2, f\"Expected inputs to be 2D (batch, seq_length), got {inputs.shape}\"\n",
        "    # Transpose to (seq_length, batch) for scanning over time.\n",
        "    inputs = jnp.swapaxes(inputs, 0, 1)\n",
        "\n",
        "    def step(carry, input_data):\n",
        "        # Example step: add the input to the carry.\n",
        "        new_carry = carry + input_data\n",
        "        return new_carry, new_carry\n",
        "\n",
        "    batch = inputs.shape[1]  # Number of samples in the batch.\n",
        "    initial_carry = jnp.zeros((batch,))\n",
        "    outputs, _ = jax.lax.scan(step, initial_carry, inputs)\n",
        "    if outputs.ndim == 1:  # If seq_length = 1, outputs is 1D\n",
        "        outputs = outputs[None, :]\n",
        "    #   outputs = jnp.swapaxes(outputs, 0, 1)\n",
        "    outputs = jnp.swapaxes(outputs, 0, 1)\n",
        "    return outputs\n",
        "\n",
        "def loss_fn(params, X, y):\n",
        "    # Use a fixed RNG for LSTM operations during loss computation.\n",
        "    preds = model.apply({'params': params}, X, rngs={'lstm': jax.random.PRNGKey(0)})\n",
        "    return jnp.mean((preds - y) ** 2)\n",
        "\n",
        "def main():\n",
        "    batch_size = 32\n",
        "    seq_length = 10  # Number of time steps.\n",
        "    num_epochs = 100\n",
        "    key = jax.random.PRNGKey(0)\n",
        "\n",
        "    # Generate explicit training data of shape (batch, seq_length).\n",
        "    X_train = jax.random.normal(key, (batch_size, seq_length))\n",
        "    key, subkey = jax.random.split(key)\n",
        "    y_train = jax.random.normal(subkey, (batch_size, seq_length))\n",
        "\n",
        "    global model\n",
        "    # Initialize LSTM with fixed feature size (equal to seq_length).\n",
        "    model = LSTM(features=seq_length)\n",
        "    # Use separate PRNG keys for parameters and LSTM operations.\n",
        "    params_key, lstm_key = jax.random.split(key)\n",
        "    variables = model.init({'params': params_key, 'lstm': lstm_key}, X_train)\n",
        "    params = variables['params']\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        key, subkey = jax.random.split(key)\n",
        "        outputs = process_sequence(X_train, subkey)\n",
        "        current_loss = loss_fn(params, X_train, y_train)\n",
        "        grad = jax.grad(loss_fn)(params, X_train, y_train)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {current_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCx-_kbqy1RN",
        "outputId": "5efbf19b-015c-4be6-9246-451c855ca3f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] - Loss: 19.6439\n",
            "Epoch [2/100] - Loss: 19.3840\n",
            "Epoch [3/100] - Loss: 19.1270\n",
            "Epoch [4/100] - Loss: 18.8730\n",
            "Epoch [5/100] - Loss: 18.6221\n",
            "Epoch [6/100] - Loss: 18.3743\n",
            "Epoch [7/100] - Loss: 18.1296\n",
            "Epoch [8/100] - Loss: 17.8881\n",
            "Epoch [9/100] - Loss: 17.6499\n",
            "Epoch [10/100] - Loss: 17.4148\n",
            "Epoch [11/100] - Loss: 17.1830\n",
            "Epoch [12/100] - Loss: 16.9545\n",
            "Epoch [13/100] - Loss: 16.7293\n",
            "Epoch [14/100] - Loss: 16.5074\n",
            "Epoch [15/100] - Loss: 16.2889\n",
            "Epoch [16/100] - Loss: 16.0736\n",
            "Epoch [17/100] - Loss: 15.8617\n",
            "Epoch [18/100] - Loss: 15.6531\n",
            "Epoch [19/100] - Loss: 15.4478\n",
            "Epoch [20/100] - Loss: 15.2457\n",
            "Epoch [21/100] - Loss: 15.0470\n",
            "Epoch [22/100] - Loss: 14.8515\n",
            "Epoch [23/100] - Loss: 14.6592\n",
            "Epoch [24/100] - Loss: 14.4700\n",
            "Epoch [25/100] - Loss: 14.2841\n",
            "Epoch [26/100] - Loss: 14.1012\n",
            "Epoch [27/100] - Loss: 13.9214\n",
            "Epoch [28/100] - Loss: 13.7446\n",
            "Epoch [29/100] - Loss: 13.5708\n",
            "Epoch [30/100] - Loss: 13.3999\n",
            "Epoch [31/100] - Loss: 13.2319\n",
            "Epoch [32/100] - Loss: 13.0667\n",
            "Epoch [33/100] - Loss: 12.9043\n",
            "Epoch [34/100] - Loss: 12.7446\n",
            "Epoch [35/100] - Loss: 12.5876\n",
            "Epoch [36/100] - Loss: 12.4332\n",
            "Epoch [37/100] - Loss: 12.2813\n",
            "Epoch [38/100] - Loss: 12.1320\n",
            "Epoch [39/100] - Loss: 11.9851\n",
            "Epoch [40/100] - Loss: 11.8407\n",
            "Epoch [41/100] - Loss: 11.6986\n",
            "Epoch [42/100] - Loss: 11.5589\n",
            "Epoch [43/100] - Loss: 11.4214\n",
            "Epoch [44/100] - Loss: 11.2862\n",
            "Epoch [45/100] - Loss: 11.1532\n",
            "Epoch [46/100] - Loss: 11.0223\n",
            "Epoch [47/100] - Loss: 10.8936\n",
            "Epoch [48/100] - Loss: 10.7669\n",
            "Epoch [49/100] - Loss: 10.6423\n",
            "Epoch [50/100] - Loss: 10.5197\n",
            "Epoch [51/100] - Loss: 10.3990\n",
            "Epoch [52/100] - Loss: 10.2803\n",
            "Epoch [53/100] - Loss: 10.1635\n",
            "Epoch [54/100] - Loss: 10.0486\n",
            "Epoch [55/100] - Loss: 9.9355\n",
            "Epoch [56/100] - Loss: 9.8242\n",
            "Epoch [57/100] - Loss: 9.7147\n",
            "Epoch [58/100] - Loss: 9.6069\n",
            "Epoch [59/100] - Loss: 9.5008\n",
            "Epoch [60/100] - Loss: 9.3965\n",
            "Epoch [61/100] - Loss: 9.2937\n",
            "Epoch [62/100] - Loss: 9.1926\n",
            "Epoch [63/100] - Loss: 9.0931\n",
            "Epoch [64/100] - Loss: 8.9951\n",
            "Epoch [65/100] - Loss: 8.8987\n",
            "Epoch [66/100] - Loss: 8.8038\n",
            "Epoch [67/100] - Loss: 8.7103\n",
            "Epoch [68/100] - Loss: 8.6183\n",
            "Epoch [69/100] - Loss: 8.5277\n",
            "Epoch [70/100] - Loss: 8.4384\n",
            "Epoch [71/100] - Loss: 8.3506\n",
            "Epoch [72/100] - Loss: 8.2640\n",
            "Epoch [73/100] - Loss: 8.1788\n",
            "Epoch [74/100] - Loss: 8.0948\n",
            "Epoch [75/100] - Loss: 8.0121\n",
            "Epoch [76/100] - Loss: 7.9306\n",
            "Epoch [77/100] - Loss: 7.8503\n",
            "Epoch [78/100] - Loss: 7.7711\n",
            "Epoch [79/100] - Loss: 7.6932\n",
            "Epoch [80/100] - Loss: 7.6163\n",
            "Epoch [81/100] - Loss: 7.5405\n",
            "Epoch [82/100] - Loss: 7.4658\n",
            "Epoch [83/100] - Loss: 7.3922\n",
            "Epoch [84/100] - Loss: 7.3195\n",
            "Epoch [85/100] - Loss: 7.2479\n",
            "Epoch [86/100] - Loss: 7.1773\n",
            "Epoch [87/100] - Loss: 7.1076\n",
            "Epoch [88/100] - Loss: 7.0389\n",
            "Epoch [89/100] - Loss: 6.9711\n",
            "Epoch [90/100] - Loss: 6.9042\n",
            "Epoch [91/100] - Loss: 6.8382\n",
            "Epoch [92/100] - Loss: 6.7731\n",
            "Epoch [93/100] - Loss: 6.7088\n",
            "Epoch [94/100] - Loss: 6.6453\n",
            "Epoch [95/100] - Loss: 6.5827\n",
            "Epoch [96/100] - Loss: 6.5208\n",
            "Epoch [97/100] - Loss: 6.4598\n",
            "Epoch [98/100] - Loss: 6.3995\n",
            "Epoch [99/100] - Loss: 6.3400\n",
            "Epoch [100/100] - Loss: 6.2812\n"
          ]
        }
      ]
    }
  ]
}