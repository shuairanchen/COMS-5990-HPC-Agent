{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee668575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.5825\n",
      "Epoch [200/1000], Loss: 0.8951\n",
      "Epoch [300/1000], Loss: 0.7411\n",
      "Epoch [400/1000], Loss: 0.3951\n",
      "Epoch [500/1000], Loss: 0.2077\n",
      "Epoch [600/1000], Loss: 0.1377\n",
      "Epoch [700/1000], Loss: 0.0902\n",
      "Epoch [800/1000], Loss: 0.0622\n",
      "Epoch [900/1000], Loss: 0.0366\n",
      "Epoch [1000/1000], Loss: 0.0394\n",
      "Predictions for [[[0.8894234895706177], [0.05691683292388916], [0.8250501155853271], [0.4220901131629944], [0.2867562174797058], [0.20837479829788208], [0.9482327699661255], [0.5099708437919617], [0.058805227279663086], [0.8394275903701782]], [[0.23448842763900757], [0.7937500476837158], [0.026732206344604492], [0.6515005230903625], [0.7079432010650635], [0.7461163997650146], [0.1356351375579834], [0.15332472324371338], [0.03719818592071533], [0.44056856632232666]]]: [[5.098686218261719], [4.298356056213379]]\n"
     ]
    }
   ],
   "source": [
    "#Input\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, ff_dim, output_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Pooling across the sequence\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "seq_length = 10\n",
    "num_samples = 100\n",
    "input_dim = 1\n",
    "X = torch.rand(num_samples, seq_length, input_dim)  # Random sequences\n",
    "y = torch.sum(X, dim=1)  # Target is the sum of each sequence\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = 1\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "ff_dim = 64\n",
    "output_dim = 1\n",
    "\n",
    "model = TransformerModel(input_dim, embed_dim, num_heads, num_layers, ff_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.rand(2, seq_length, input_dim)\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c6435f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRngError",
     "evalue": "SelfAttention_0 needs PRNG for \"dropout\" (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.InvalidRngError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRngError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params, opt_state, loss\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 108\u001b[0m     params, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 102\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(params, opt_state, x, y)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(params, opt_state, x, y):\n\u001b[1;32m--> 102\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n\u001b[0;32m    104\u001b[0m     params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[1;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 91\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(params, x, y, train)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmse_loss\u001b[39m(params, x, y, train):\n\u001b[1;32m---> 91\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean((preds \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m, in \u001b[0;36mTransformerModel.__call__\u001b[1;34m(self, x, train)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Pass through a stack of Transformer encoder blocks.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Pool across the sequence dimension (mean pooling).\u001b[39;00m\n\u001b[0;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mTransformerEncoderBlock.__call__\u001b[1;34m(self, x, train)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m, train):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Multi-head self-attention; note: dropout is disabled when deterministic=True.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mqkv_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Add & Norm\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm()(x \u001b[38;5;241m+\u001b[39m attn_output)\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\flax\\linen\\attention.py:743\u001b[0m, in \u001b[0;36mSelfAttention.__call__\u001b[1;34m(self, inputs_q, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies multi-head dot product self-attention on the input data.\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03mProjects the inputs into multi-headed query, key, and value vectors,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m  output of shape ``[batch_sizes..., length, features]``.\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    736\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    737\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelfAttention will be deprecated soon. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    738\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`MultiHeadDotProductAttention.__call__(inputs_q)` instead. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    741\u001b[0m   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    742\u001b[0m )\n\u001b[1;32m--> 743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m  \u001b[49m\u001b[43minputs_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m  \u001b[49m\u001b[43msow_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msow_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\flax\\linen\\attention.py:573\u001b[0m, in \u001b[0;36mMultiHeadDotProductAttention.__call__\u001b[1;34m(self, inputs_q, inputs_k, inputs_v, inputs_kv, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[0;32m    569\u001b[0m   m_deterministic \u001b[38;5;241m=\u001b[39m merge_param(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeterministic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic, deterministic\n\u001b[0;32m    571\u001b[0m   )\n\u001b[0;32m    572\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m m_deterministic \u001b[38;5;129;01mand\u001b[39;00m dropout_rng \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 573\u001b[0m     dropout_rng \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    575\u001b[0m   m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\flax\\core\\scope.py:752\u001b[0m, in \u001b[0;36mScope.make_rng\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    750\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    751\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidRngError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m needs PRNG for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_valid()\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_trace_level()\n",
      "\u001b[1;31mInvalidRngError\u001b[0m: SelfAttention_0 needs PRNG for \"dropout\" (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.InvalidRngError)"
     ]
    }
   ],
   "source": [
    "#Strong LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    ff_dim: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, train):\n",
    "        # Multi-head self-attention; note: dropout is disabled when deterministic=True.\n",
    "        attn_output = nn.SelfAttention(num_heads=self.num_heads,\n",
    "                                       qkv_features=self.embed_dim,\n",
    "                                       dropout_rate=self.dropout_rate,\n",
    "                                       deterministic=not train)(x)\n",
    "        # Add & Norm\n",
    "        x = nn.LayerNorm()(x + attn_output)\n",
    "        # Feedforward network\n",
    "        ff_output = nn.Dense(self.ff_dim)(x)\n",
    "        ff_output = nn.relu(ff_output)\n",
    "        ff_output = nn.Dense(self.embed_dim)(ff_output)\n",
    "        # Add & Norm\n",
    "        x = nn.LayerNorm()(x + ff_output)\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    ff_dim: int\n",
    "    output_dim: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, train=True):\n",
    "        # x shape: (batch, seq_length, input_dim)\n",
    "        # Map input to embedding space.\n",
    "        x = nn.Dense(self.embed_dim)(x)\n",
    "        # Pass through a stack of Transformer encoder blocks.\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerEncoderBlock(embed_dim=self.embed_dim,\n",
    "                                        num_heads=self.num_heads,\n",
    "                                        ff_dim=self.ff_dim,\n",
    "                                        dropout_rate=self.dropout_rate)(x, train=train)\n",
    "        # Pool across the sequence dimension (mean pooling).\n",
    "        x = jnp.mean(x, axis=1)\n",
    "        # Map to output.\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "seq_length = 10\n",
    "num_samples = 100\n",
    "input_dim = 1\n",
    "\n",
    "# Generate random sequences with shape (num_samples, seq_length, input_dim)\n",
    "key, subkey = jax.random.split(key)\n",
    "X = jax.random.uniform(subkey, shape=(num_samples, seq_length, input_dim))\n",
    "# Target: sum of each sequence, resulting in shape (num_samples, 1)\n",
    "y = jnp.sum(X, axis=1)\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "ff_dim = 64\n",
    "output_dim = 1\n",
    "\n",
    "# Create the model instance.\n",
    "model = TransformerModel(input_dim=input_dim,\n",
    "                         embed_dim=embed_dim,\n",
    "                         num_heads=num_heads,\n",
    "                         num_layers=num_layers,\n",
    "                         ff_dim=ff_dim,\n",
    "                         output_dim=output_dim)\n",
    "\n",
    "# Initialize model parameters using a dummy input.\n",
    "dummy_input = jnp.ones((num_samples, seq_length, input_dim))\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = model.init(rng, dummy_input, train=True)\n",
    "\n",
    "# Define Mean Squared Error loss.\n",
    "def mse_loss(params, x, y, train):\n",
    "    preds = model.apply(params, x, train=train)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Setup the Adam optimizer.\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, x, y):\n",
    "    loss, grads = jax.value_and_grad(mse_loss)(params, x, y, True)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    params, opt_state, loss = train_step(params, opt_state, X, y)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "# Generate new random test data with shape (2, seq_length, input_dim)\n",
    "key, subkey = jax.random.split(key)\n",
    "X_test = jax.random.uniform(subkey, shape=(2, seq_length, input_dim))\n",
    "predictions = model.apply(params, X_test, train=False)\n",
    "print(\"Predictions for\", np.array(X_test).tolist(), \":\", np.array(predictions).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f948cf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret value of type <class '__main__.SimpleModel'> as an abstract array; it does not have a dtype attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m\n\u001b[0;32m     83\u001b[0m     train_model(X, y, num_epochs, key)  \u001b[38;5;66;03m# MODIFIED: pass key to train_model\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 83\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m y \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m500\u001b[39m)  \u001b[38;5;66;03m# Sample labels\u001b[39;00m\n\u001b[0;32m     82\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 83\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 67\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, y, num_epochs, key)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, dataset_size, \u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m     63\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m: X_shuffled[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m32\u001b[39m],\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: y_shuffled[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m32\u001b[39m]\n\u001b[0;32m     66\u001b[0m     }\n\u001b[1;32m---> 67\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Example log after each epoch\u001b[39;00m\n\u001b[0;32m     70\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mtarget}, X)\n",
      "    \u001b[1;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\api_util.py:577\u001b[0m, in \u001b[0;36m_shaped_abstractify_slow\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    575\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(x\u001b[38;5;241m.\u001b[39mdtype, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    578\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot interpret value of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an abstract array; it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not have a dtype attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m core\u001b[38;5;241m.\u001b[39mShapedArray(np\u001b[38;5;241m.\u001b[39mshape(x), dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type,\n\u001b[0;32m    581\u001b[0m                         named_shape\u001b[38;5;241m=\u001b[39mnamed_shape)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret value of type <class '__main__.SimpleModel'> as an abstract array; it does not have a dtype attribute"
     ]
    }
   ],
   "source": [
    "#Weak LLM\n",
    "import jax\n",
    "import jax.numpy as jnp  # Ensured consistent import statement\n",
    "from flax import linen as nn\n",
    "from jax import random, grad, jit, vmap\n",
    "import optax\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\"A simple feedforward neural network model.\"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_model() -> SimpleModel:\n",
    "    \"\"\"Create an instance of the SimpleModel.\"\"\"\n",
    "    return SimpleModel()\n",
    "\n",
    "\n",
    "def compute_loss(logits, labels):\n",
    "    \"\"\"Compute the binary cross-entropy loss.\"\"\"\n",
    "    return jnp.mean(jax.nn.sigmoid_cross_entropy(logits=logits, labels=labels))\n",
    "\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Calculate the accuracy of the model predictions.\"\"\"\n",
    "    preds = jnp.round(jax.nn.sigmoid(logits))\n",
    "    return jnp.mean(preds == labels)\n",
    "\n",
    "\n",
    "@jit\n",
    "def train_step(optimizer, model, batch):\n",
    "    \"\"\"Perform a single training step.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({'params': params}, batch['X'])\n",
    "        return compute_loss(logits, batch['y'])\n",
    "    \n",
    "    grads = grad(loss_fn)(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(grads)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_model(X, y, num_epochs, key):\n",
    "    \"\"\"Train the model on the provided data.\"\"\"\n",
    "    model = create_model()\n",
    "    params = model.init(key, jnp.ones((1, X.shape[1])))\n",
    "    optimizer = optax.adam(learning_rate=0.001).init(params)\n",
    "\n",
    "    dataset_size = X.shape[0]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle dataset\n",
    "        perm = random.permutation(key, dataset_size)\n",
    "        X_shuffled = X[perm]\n",
    "        y_shuffled = y[perm]\n",
    "        \n",
    "        for i in range(0, dataset_size, 32):\n",
    "            batch = {\n",
    "                'X': X_shuffled[i:i + 32],\n",
    "                'y': y_shuffled[i:i + 32]\n",
    "            }\n",
    "            optimizer = train_step(optimizer, model, batch)\n",
    "        \n",
    "        # Example log after each epoch\n",
    "        logits = model.apply({'params': optimizer.target}, X)\n",
    "        train_acc = accuracy(logits, y)\n",
    "        print(f\"Epoch {epoch + 1}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for the script.\"\"\"\n",
    "    # Example data generation with explicit PRNG key\n",
    "    key = random.PRNGKey(0)  # Initialize PRNG key\n",
    "    X = random.uniform(key, (1000, 10))  # MODIFIED: Added explicit PRNG key\n",
    "    y = jnp.array([0, 1] * 500)  # Sample labels\n",
    "\n",
    "    num_epochs = 10\n",
    "    train_model(X, y, num_epochs, key)  # MODIFIED: pass key to train_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error Code:\n",
    "class SimpleModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "Error:\n",
    "The translated JAX code implements a simple fully connected network, which is inconsistent with the Transformer model implemented in the original PyTorch code.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use Flax to implement a Transformer model. The steps include:\n",
    "Use a Dense layer to implement input embedding\n",
    "Implement the Transformer encoder layer\n",
    "Mean pooling on the sequence dimension\n",
    "Connect to the output layer to get the final regression result\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    ff_dim: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        attn = nn.SelfAttention(num_heads=self.num_heads,\n",
    "                                qkv_features=self.embed_dim,\n",
    "                                dropout_rate=self.dropout_rate,\n",
    "                                deterministic=not train)(x)\n",
    "        x = x + attn\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        ff = nn.Dense(self.ff_dim)(x)\n",
    "        ff = nn.relu(ff)\n",
    "        ff = nn.Dense(self.embed_dim)(ff)\n",
    "        x = x + ff\n",
    "        x = nn.LayerNorm()(x)\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    ff_dim: int\n",
    "    output_dim: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        x = nn.Dense(self.embed_dim)(x)\n",
    "\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerEncoderLayer(embed_dim=self.embed_dim,\n",
    "                                        num_heads=self.num_heads,\n",
    "                                        ff_dim=self.ff_dim,\n",
    "                                        dropout_rate=self.dropout_rate)(x, train=train)\n",
    "\n",
    "        x = jnp.mean(x, axis=1)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def compute_loss(logits, labels):\n",
    "    return jnp.mean(jax.nn.sigmoid_cross_entropy(logits=logits, labels=labels))\n",
    "\n",
    "\n",
    "Error:\n",
    "The original PyTorch code is a regression task. The goal is to calculate the sequence and the mean square error (MSE) loss should be used\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the loss function to mean square error\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def compute_loss(predictions, targets):\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# Example data generation with explicit PRNG key\n",
    "key = random.PRNGKey(0)\n",
    "X = random.uniform(key, (1000, 10))  # MODIFIED: Added explicit PRNG key\n",
    "y = jnp.array([0, 1] * 500)  # Sample labels\n",
    "\n",
    "\n",
    "Error:\n",
    "The generated data X lacks feature dimensions. The shape of X in the original code should be (num_samples, seq_length, input_dim)\n",
    "The generated labels y are alternating 0 and 1, which does not match the goal of the regression task (summing the sequence elements)\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Change the shape of X to (num_samples, seq_length, input_dim)\n",
    "Define y as the sum of X along the sequence dimension, i.e. y = jnp.sum(X, axis=1)\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key = random.PRNGKey(0)\n",
    "num_samples = 100\n",
    "seq_length = 10\n",
    "input_dim = 1\n",
    "X = random.uniform(key, (num_samples, seq_length, input_dim))\n",
    "y = jnp.sum(X, axis=1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_step(optimizer, model, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({'params': params}, batch['X'])\n",
    "        return compute_loss(logits, batch['y'])\n",
    "    \n",
    "    grads = grad(loss_fn)(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(grads)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "Error:\n",
    "The optimizer usage does not match\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use the TrainState class to encapsulate the parameters and optimizer state, and call state.apply_gradients to update them in the training step\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "from flax.training import train_state\n",
    "\n",
    "def create_train_state(rng, model, learning_rate, input_shape):\n",
    "    params = model.init(rng, jnp.ones(input_shape))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        predictions = state.apply_fn({'params': params}, batch['X'], train=True)\n",
    "        loss = compute_loss(predictions, batch['y'])\n",
    "        return loss, predictions\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, preds), grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# Shuffle dataset\n",
    "perm = random.permutation(key, dataset_size)\n",
    "X_shuffled = X[perm]\n",
    "y_shuffled = y[perm]\n",
    "\n",
    "\n",
    "Error:\n",
    "Repeatedly using the same PRNG key for random operations will result in the same random sequence being generated each time in JAX\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use random.split to generate a new key before each random operation to ensure randomness\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "rng, key = random.split(rng)\n",
    "perm = random.permutation(key, dataset_size)\n",
    "X_shuffled = X[perm]\n",
    "y_shuffled = y[perm]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_model(X, y, num_epochs, key):\n",
    "    ...\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle dataset\n",
    "        rng, key = random.split(rng)\n",
    "        perm = random.permutation(key, dataset_size)\n",
    "        ...\n",
    "\n",
    "\n",
    "Error:\n",
    "local variable 'rng' referenced before assignment\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Need to keep the variable name of the random number generator consistent\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def train_model(X, y, num_epochs, key):\n",
    "    dataset_size = X.shape[0]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle dataset\n",
    "        key, subkey = random.split(key)\n",
    "        perm = random.permutation(subkey, dataset_size)\n",
    "        X_shuffled = X[perm]\n",
    "        y_shuffled = y[perm]\n",
    "        \n",
    "        for i in range(0, dataset_size, 32):\n",
    "            batch = {\n",
    "                'X': X_shuffled[i:i + 32],\n",
    "                'y': y_shuffled[i:i + 32]\n",
    "            }\n",
    "            optimizer = train_step(optimizer, batch)\n",
    "        \n",
    "        # Example log after each epoch\n",
    "        logits = model.apply({'params': optimizer.target}, X)\n",
    "        train_acc = accuracy(logits, y)\n",
    "        print(f\"Epoch {epoch + 1}, Train Accuracy: {train_acc:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_model(X, y, num_epochs, key):\n",
    "    model = create_model()\n",
    "    params = model.init(key, jnp.ones((1, X.shape[1])))\n",
    "    optimizer = optax.adam(learning_rate=0.001).init(params)\n",
    "    ...\n",
    "    for i in range(0, dataset_size, 32):\n",
    "        batch = {\n",
    "            'X': X_shuffled[i:i + 32],\n",
    "            'y': y_shuffled[i:i + 32]\n",
    "        }\n",
    "        optimizer = train_step(optimizer, model, batch)\n",
    "    ...\n",
    "\n",
    "\n",
    "Error:\n",
    "local variable 'optimizer' referenced before assignment\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use TrainState to manage model parameters and optimizer state.\n",
    "Define a create_train_state function, use the model's init method and optax optimizer to create a training state\n",
    "Call this function in train_model to generate a training state, and use this object in subsequent training steps\n",
    "At the same time, modify the calling method of train_step, and return (new_state, loss), and need to receive these two return values\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def train_model(X, y, num_epochs, key):\n",
    "    model = TransformerModel(\n",
    "        input_dim=1,\n",
    "        embed_dim=16,\n",
    "        num_heads=2,\n",
    "        num_layers=2,\n",
    "        ff_dim=64,\n",
    "        output_dim=1\n",
    "    )\n",
    "    state = create_train_state(key, model, learning_rate=0.001, input_shape=X.shape)\n",
    "\n",
    "    dataset_size = X.shape[0]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        perm = random.permutation(subkey, dataset_size)\n",
    "        X_shuffled = X[perm]\n",
    "        y_shuffled = y[perm]\n",
    "        \n",
    "        for i in range(0, dataset_size, 32):\n",
    "            batch = {\n",
    "                'X': X_shuffled[i:i + 32],\n",
    "                'y': y_shuffled[i:i + 32]\n",
    "            }\n",
    "            state, loss = train_step(state, batch)\n",
    "        \n",
    "        logits = state.apply_fn({'params': state.params}, X, train=False)\n",
    "        train_acc = accuracy(logits, y)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    return state, model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        predictions = state.apply_fn({'params': params}, batch['X'], train=True)\n",
    "        loss = compute_loss(predictions, batch['y'])\n",
    "        return loss, predictions\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, preds), grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\n",
    "\n",
    "Error:\n",
    "SelfAttention_0 needs PRNG for \"dropout\" (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.InvalidRngError)\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the train_step function to accept an additional dropout random number key and pass in rngs={'dropout': dropout_rng} when calling apply_fn\n",
    "During the training process, a new key needs to be assigned to dropout before each batch is processed\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@jit\n",
    "def train_step(state, batch, dropout_rng):\n",
    "    def loss_fn(params):\n",
    "        predictions = state.apply_fn({'params': params}, batch['X'], train=True, rngs={'dropout': dropout_rng})\n",
    "        loss = compute_loss(predictions, batch['y'])\n",
    "        return loss, predictions\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, preds), grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_model(X, y, num_epochs, key):\n",
    "    ...\n",
    "    for i in range(0, dataset_size, 32):\n",
    "        batch = {\n",
    "            'X': X_shuffled[i:i + 32],\n",
    "            'y': y_shuffled[i:i + 32]\n",
    "        }\n",
    "        state, loss = train_step(state, batch)\n",
    "    ...\n",
    "\n",
    "\n",
    "Error:\n",
    "train_step() missing 1 required positional argument: 'dropout_rng'\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "In the training loop, use random.split to generate a new key for dropout before processing each batch and pass it to train_step\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key, dropout_key = random.split(key)\n",
    "state, loss = train_step(state, batch, dropout_key)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def create_model() -> SimpleModel:\n",
    "    return SimpleModel()\n",
    "\n",
    "\n",
    "Error:\n",
    "Reference to undefined SimpleModel class\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Delete the function\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "# def create_model() -> SimpleModel:\n",
    "    # return SimpleModel()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def accuracy(logits, labels):\n",
    "    preds = jnp.round(jax.nn.sigmoid(logits))\n",
    "    return jnp.mean(preds == labels)\n",
    "\n",
    "\n",
    "Error:\n",
    "This function uses sigmoid and round to calculate the accuracy and is not suitable for regression tasks.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Remove this function\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "# def accuracy(logits, labels):\n",
    "    # preds = jnp.round(jax.nn.sigmoid(logits))\n",
    "    # return jnp.mean(preds == labels)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "logits = state.apply_fn({'params': state.params}, X, train=False)\n",
    "train_acc = accuracy(logits, y)\n",
    "print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "Error:\n",
    "The accuracy function is called in the training loop to calculate the accuracy, which is meaningless for regression tasks.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Removed accuracy calls and instead computed evaluation metrics for regression tasks\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "predictions = state.apply_fn({'params': state.params}, X, train=False)\n",
    "eval_loss = compute_loss(predictions, y)\n",
    "print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "ff = nn.relu(ff)\n",
    "\n",
    "\n",
    "Error:\n",
    "The nn module does not have a built-in relu function\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Replace nn.relu with jax.nn.relu\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "ff = jax.nn.relu(ff)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "key = random.PRNGKey(0)\n",
    "num_samples = 100\n",
    "seq_length = 10\n",
    "input_dim = 1\n",
    "X = random.uniform(key, (num_samples, seq_length, input_dim))\n",
    "y = jnp.sum(X, axis=1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_model(X, y, num_epochs, key)\n",
    "\n",
    "\n",
    "Error:\n",
    "Reusing the same PRNG key may cause randomness issues or unexpected behavior\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use random.split to split a new key for subsequent passing to train_model\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key = random.PRNGKey(0)\n",
    "num_samples = 100\n",
    "seq_length = 10\n",
    "input_dim = 1\n",
    "key, subkey = random.split(key)\n",
    "X = random.uniform(subkey, (num_samples, seq_length, input_dim))\n",
    "y = jnp.sum(X, axis=1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_model(X, y, num_epochs, key)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "ff = nn.Dense(self.ff_dim)(x)\n",
    "ff = jax.nn.relu(ff)\n",
    "ff = nn.Dense(self.embed_dim)(ff)\n",
    "\n",
    "\n",
    "Error:\n",
    "In PyTorch's nn.TransformerEncoderLayer, in addition to the built-in dropout in the self-attention part, the output of the feed-forward network is usually processed by dropout for regularization.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Insert a nn.Dropout layer after the relu activation and before the second fully connected layer, and pass in the deterministic=not train parameter\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "ff = nn.Dense(self.ff_dim)(x)\n",
    "ff = jax.nn.relu(ff)\n",
    "ff = nn.Dropout(rate=self.dropout_rate)(ff, deterministic=not train)\n",
    "ff = nn.Dense(self.embed_dim)(ff)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def main():\n",
    "    # Example data generation with explicit PRNG key\n",
    "    key = random.PRNGKey(0)\n",
    "    num_samples = 100\n",
    "    seq_length = 10\n",
    "    input_dim = 1\n",
    "    key, subkey = random.split(key)\n",
    "    X = random.uniform(subkey, (num_samples, seq_length, input_dim))\n",
    "    y = jnp.sum(X, axis=1)\n",
    "\n",
    "    num_epochs = 10\n",
    "    train_model(X, y, num_epochs, key)\n",
    "\n",
    "\n",
    "Error:\n",
    "The main function main() only calls train_model for training, and there is no part similar to testing new data in PyTorch code\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add the test code at the end of the main() function\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def main():\n",
    "    # Example data generation with explicit PRNG key\n",
    "    key = random.PRNGKey(0)\n",
    "    num_samples = 100\n",
    "    seq_length = 10\n",
    "    input_dim = 1\n",
    "    key, subkey = random.split(key)\n",
    "    X = random.uniform(subkey, (num_samples, seq_length, input_dim))\n",
    "    y = jnp.sum(X, axis=1)\n",
    "\n",
    "    num_epochs = 10\n",
    "    state, model = train_model(X, y, num_epochs, key)\n",
    "    \n",
    "    # Testing on new data\n",
    "    key, subkey = random.split(key)\n",
    "    X_test = random.uniform(subkey, (2, seq_length, input_dim))\n",
    "    predictions = state.apply_fn({'params': state.params}, X_test, train=False)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "x = nn.Dense(self.embed_dim)(x)\n",
    "\n",
    "\n",
    "Error:\n",
    "There is no explicit definition of the \"Embedding layer\" like in the PyTorch code, nor is there any use of the declared input_dim parameter\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use a named Dense layer as the embedding layer in the __call__ method of TransformerModel to explicitly map the input from input_dim to embed_dim\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "x = nn.Dense(self.embed_dim, name=\"embedding\")(x)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "attn = nn.SelfAttention(num_heads=self.num_heads,\n",
    "                         qkv_features=self.embed_dim,\n",
    "                         dropout_rate=self.dropout_rate,\n",
    "                         deterministic=not train)(x)\n",
    "x = x + attn\n",
    "\n",
    "\n",
    "Error:\n",
    "PyTorch's nn.TransformerEncoderLayer usually applies dropout to the self-attention output in the residual branch\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Before adding the self-attention output to the input, perform another dropout\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "attn = nn.SelfAttention(num_heads=self.num_heads,\n",
    "                        qkv_features=self.embed_dim,\n",
    "                        dropout_rate=self.dropout_rate,\n",
    "                        deterministic=not train)(x)\n",
    "attn = nn.Dropout(rate=self.dropout_rate)(attn, deterministic=not train)\n",
    "x = x + attn\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "for i in range(0, dataset_size, 32):\n",
    "    key, dropout_key = random.split(key)\n",
    "    batch = {\n",
    "        'X': X_shuffled[i:i + 32],\n",
    "        'y': y_shuffled[i:i + 32]\n",
    "    }\n",
    "    state, loss = train_step(state, batch, dropout_key)\n",
    "        \n",
    "predictions = state.apply_fn({'params': state.params}, X, train=False)\n",
    "eval_loss = compute_loss(predictions, y)\n",
    "print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "Error:\n",
    "The printed loss is only the loss of the last batch in the current epoch, not the average training loss of the entire epoch, which is inconsistent with the expectation of printing the overall progress each time in the PyTorch code.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "In each epoch, the losses of all batches are accumulated and the average is calculated before output\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "for i in range(0, dataset_size, 32):\n",
    "    key, dropout_key = random.split(key)\n",
    "    batch = {\n",
    "        'X': X_shuffled[i:i + 32],\n",
    "        'y': y_shuffled[i:i + 32]\n",
    "    }\n",
    "    state, batch_loss = train_step(state, batch, dropout_key)\n",
    "    total_loss += batch_loss\n",
    "    num_batches += 1\n",
    "avg_loss = total_loss / num_batches\n",
    "predictions = state.apply_fn({'params': state.params}, X, train=False)\n",
    "eval_loss = compute_loss(predictions, y)\n",
    "print(f\"Epoch {epoch + 1}, Avg Loss: {avg_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "avg_loss = total_loss / num_batches\n",
    "predictions = state.apply_fn({'params': state.params}, X, train=False)\n",
    "eval_loss = compute_loss(predictions, y)\n",
    "print(f\"Epoch {epoch + 1}, Avg Loss: {avg_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "Error:\n",
    "The PyTorch code only prints once every 100 epochs\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the print statement to calculate the forward propagation loss on the full dataset only when (epoch + 1) is divisible by 100\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "if (epoch + 1) % 100 == 0:\n",
    "    predictions = state.apply_fn({'params': state.params}, X, train=False)\n",
    "    loss_value = compute_loss(predictions, y)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_value:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "Error:\n",
    "The PyTorch code trains for 1000 epochs, while the JAX code only trains for 10 epochs\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Change num_epochs to 1000\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "num_epochs = 1000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54763120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.8359\n",
      "Epoch [200/1000], Loss: 0.0190\n",
      "Epoch [300/1000], Loss: 0.0066\n",
      "Epoch [400/1000], Loss: 0.0119\n",
      "Epoch [500/1000], Loss: 0.0020\n",
      "Epoch [600/1000], Loss: 0.0080\n",
      "Epoch [700/1000], Loss: 0.0078\n",
      "Epoch [800/1000], Loss: 0.0069\n",
      "Epoch [900/1000], Loss: 0.0013\n",
      "Epoch [1000/1000], Loss: 0.0036\n",
      "Predictions for [[[0.21237587928771973], [0.17597758769989014], [0.22925710678100586], [0.37311553955078125], [0.7130759954452515], [0.9348740577697754], [0.38974833488464355], [0.8368901014328003], [0.10798454284667969], [0.5377466678619385]], [[0.09019839763641357], [0.7678695917129517], [0.1505894660949707], [0.036005616188049316], [0.9545391798019409], [0.8174539804458618], [0.747868537902832], [0.03028249740600586], [0.3456763029098511], [0.07068979740142822]]]: [[4.516904354095459], [4.035348892211914]]\n"
     ]
    }
   ],
   "source": [
    "#Fixed Code\n",
    "import jax\n",
    "import jax.numpy as jnp  # Ensured consistent import statement\n",
    "from flax import linen as nn\n",
    "from jax import random, grad, jit, vmap\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    ff_dim: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        attn = nn.SelfAttention(num_heads=self.num_heads,\n",
    "                        qkv_features=self.embed_dim,\n",
    "                        dropout_rate=self.dropout_rate,\n",
    "                        deterministic=not train)(x)\n",
    "        attn = nn.Dropout(rate=self.dropout_rate)(attn, deterministic=not train)\n",
    "        x = x + attn\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        ff = nn.Dense(self.ff_dim)(x)\n",
    "        ff = jax.nn.relu(ff)\n",
    "        ff = nn.Dropout(rate=self.dropout_rate)(ff, deterministic=not train)\n",
    "        ff = nn.Dense(self.embed_dim)(ff)\n",
    "        x = x + ff\n",
    "        x = nn.LayerNorm()(x)\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    ff_dim: int\n",
    "    output_dim: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        x = nn.Dense(self.embed_dim, name=\"embedding\")(x)\n",
    "\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerEncoderLayer(embed_dim=self.embed_dim,\n",
    "                                        num_heads=self.num_heads,\n",
    "                                        ff_dim=self.ff_dim,\n",
    "                                        dropout_rate=self.dropout_rate)(x, train=train)\n",
    "\n",
    "        x = jnp.mean(x, axis=1)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_loss(predictions, targets):\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "\n",
    "def create_train_state(rng, model, learning_rate, input_shape):\n",
    "    params = model.init(rng, jnp.ones(input_shape))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch, dropout_rng):\n",
    "    def loss_fn(params):\n",
    "        predictions = state.apply_fn({'params': params}, batch['X'], train=True, rngs={'dropout': dropout_rng})\n",
    "        loss = compute_loss(predictions, batch['y'])\n",
    "        return loss, predictions\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, preds), grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\n",
    "\n",
    "def train_model(X, y, num_epochs, key):\n",
    "    model = TransformerModel(\n",
    "        input_dim=1,\n",
    "        embed_dim=16,\n",
    "        num_heads=2,\n",
    "        num_layers=2,\n",
    "        ff_dim=64,\n",
    "        output_dim=1\n",
    "    )\n",
    "    state = create_train_state(key, model, learning_rate=0.001, input_shape=X.shape)\n",
    "\n",
    "    dataset_size = X.shape[0]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        perm = random.permutation(subkey, dataset_size)\n",
    "        X_shuffled = X[perm]\n",
    "        y_shuffled = y[perm]\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for i in range(0, dataset_size, 32):\n",
    "            key, dropout_key = random.split(key)\n",
    "            batch = {\n",
    "                'X': X_shuffled[i:i + 32],\n",
    "                'y': y_shuffled[i:i + 32]\n",
    "            }\n",
    "            state, batch_loss = train_step(state, batch, dropout_key)\n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            predictions = state.apply_fn({'params': state.params}, X, train=False)\n",
    "            loss_value = compute_loss(predictions, y)\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_value:.4f}\")\n",
    "\n",
    "    return state, model\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for the script.\"\"\"\n",
    "    # Example data generation with explicit PRNG key\n",
    "    key = random.PRNGKey(0)\n",
    "    num_samples = 100\n",
    "    seq_length = 10\n",
    "    input_dim = 1\n",
    "    key, subkey = random.split(key)\n",
    "    X = random.uniform(subkey, (num_samples, seq_length, input_dim))\n",
    "    y = jnp.sum(X, axis=1)\n",
    "\n",
    "    num_epochs = 1000\n",
    "    state, model = train_model(X, y, num_epochs, key)\n",
    "    \n",
    "    # Testing on new data\n",
    "    key, subkey = random.split(key)\n",
    "    X_test = random.uniform(subkey, (2, seq_length, input_dim))\n",
    "    predictions = state.apply_fn({'params': state.params}, X_test, train=False)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f07b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
