{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a15e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.6039\n",
      "Epoch [200/1000], Loss: 1.0242\n",
      "Epoch [300/1000], Loss: 0.8017\n",
      "Epoch [400/1000], Loss: 0.7163\n",
      "Epoch [500/1000], Loss: 0.6836\n",
      "Epoch [600/1000], Loss: 0.6710\n",
      "Epoch [700/1000], Loss: 0.6662\n",
      "Epoch [800/1000], Loss: 0.6643\n",
      "Epoch [900/1000], Loss: 0.6636\n",
      "Epoch [1000/1000], Loss: 0.6634\n",
      "Learned weight: 1.9577, Learned bias: 3.2045\n",
      "Predictions for [[4.0], [7.0]]: [[11.035286903381348], [16.90837860107422]]\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
    "y = 2 * X + 3 + torch.randn(100, 1)  # Linear relationship with noise\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # Single input and single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Display the learned parameters\n",
    "[w, b] = model.linear.parameters()\n",
    "print(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0], [7.0]])\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02eeb002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.4685\n",
      "Epoch [200/1000], Loss: 1.2193\n",
      "Epoch [300/1000], Loss: 1.1290\n",
      "Epoch [400/1000], Loss: 1.0963\n",
      "Epoch [500/1000], Loss: 1.0844\n",
      "Epoch [600/1000], Loss: 1.0801\n",
      "Epoch [700/1000], Loss: 1.0786\n",
      "Epoch [800/1000], Loss: 1.0780\n",
      "Epoch [900/1000], Loss: 1.0778\n",
      "Epoch [1000/1000], Loss: 1.0777\n",
      "Learned weight: 2.0446, Learned bias: 2.8493\n",
      "Predictions for [[4.0], [7.0]]: [[11.027803421020508], [17.161645889282227]]\n"
     ]
    }
   ],
   "source": [
    "# Strong LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Set the random seed and create a PRNG key\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "key, subkey = jax.random.split(key)\n",
    "X = jax.random.uniform(subkey, shape=(100, 1)) * 10  # 100 data points between 0 and 10\n",
    "key, subkey = jax.random.split(key)\n",
    "noise = jax.random.normal(subkey, shape=(100, 1))\n",
    "y = 2 * X + 3 + noise  # Linear relationship with noise\n",
    "\n",
    "# Initialize model parameters similar to PyTorch's nn.Linear(1, 1)\n",
    "# PyTorch uses a uniform distribution in [-1/sqrt(in_features), 1/sqrt(in_features)]\n",
    "bound = 1.0  # For in_features=1, bound = 1.\n",
    "key, subkey = jax.random.split(key)\n",
    "w = jax.random.uniform(subkey, shape=(1, 1), minval=-bound, maxval=bound)\n",
    "key, subkey = jax.random.split(key)\n",
    "b = jax.random.uniform(subkey, shape=(1,), minval=-bound, maxval=bound)\n",
    "params = {\"w\": w, \"b\": b}\n",
    "\n",
    "# Define the forward pass (prediction function)\n",
    "def predict(params, x):\n",
    "    # Using dot product to mimic nn.Linear (x is (batch, 1), w is (1, 1), b is (1,))\n",
    "    return jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "def loss_fn(params, x, y):\n",
    "    preds = predict(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Set learning rate and number of epochs\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Get a function to compute loss and its gradients\n",
    "loss_and_grad = jax.value_and_grad(loss_fn)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss, grads = loss_and_grad(params, X, y)\n",
    "    \n",
    "    # Update parameters using SGD\n",
    "    params[\"w\"] = params[\"w\"] - lr * grads[\"w\"]\n",
    "    params[\"b\"] = params[\"b\"] - lr * grads[\"b\"]\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "# Display the learned parameters\n",
    "learned_w = params[\"w\"][0, 0]\n",
    "learned_b = params[\"b\"][0]\n",
    "print(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = jnp.array([[4.0], [7.0]])\n",
    "predictions = predict(params, X_test)\n",
    "print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b080a3de",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Generate data\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     learned_params \u001b[38;5;241m=\u001b[39m train_model(X, y)\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(num_samples)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_data\u001b[39m(num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      9\u001b[0m     X \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, num_samples)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Adjusted for noise\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\deprecations.py:55\u001b[0m, in \u001b[0;36mdeprecation_getattr.<locals>.getattr\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     54\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'jax.numpy' has no attribute 'random'"
     ]
    }
   ],
   "source": [
    "# Weak LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, vmap\n",
    "import optax\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(num_samples=100):\n",
    "    X = jnp.linspace(0, 10, num_samples).reshape(-1, 1)\n",
    "    y = 2 * X + 1 + jnp.random.normal(0, 1, X.shape)  # Adjusted for noise\n",
    "    return X, y\n",
    "\n",
    "# Linear regression model\n",
    "def model(params, x):\n",
    "    w, b = params\n",
    "    return w * x + b\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(params, x, y):\n",
    "    preds = model(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Gradient computation\n",
    "@jax.jit\n",
    "def compute_gradient(params, x, y):\n",
    "    return grad(loss_fn)(params, x, y)\n",
    "\n",
    "# Training step\n",
    "@jax.jit\n",
    "def train_step(params, x, y, rng_key):\n",
    "    grads = compute_gradient(params, x, y)\n",
    "    return params - 0.01 * grads  # Update rate\n",
    "\n",
    "# Batch training step using vectorization\n",
    "def train_model(X, y, num_epochs=1000):\n",
    "    key = random.PRNGKey(0)  # Initialize PRNG key\n",
    "    params = jnp.array([0.0, 0.0])  # Initial parameters (w, b)\n",
    "    \n",
    "    # Vectorized training over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Create a new PRNG key for each batch (if batching)\n",
    "        key, subkey = random.split(key)\n",
    "        params = train_step(params, X, y, subkey)  # // MODIFIED: Removed loop for vectorization\n",
    "\n",
    "    return params\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Generate data\n",
    "    X, y = generate_data(100)\n",
    "    \n",
    "    # Train the model\n",
    "    learned_params = train_model(X, y)\n",
    "    \n",
    "    # Display the learned parameters\n",
    "    w, b = learned_params\n",
    "    print(f\"Learned weight: {w.item():.4f}, Learned bias: {b.item():.4f}\")\n",
    "\n",
    "    # Testing on new data\n",
    "    X_test = jnp.array([[4.0], [7.0]])\n",
    "    predictions = model(learned_params, X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fd00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error Code:\n",
    "X = jnp.linspace(0, 10, num_samples).reshape(-1, 1)\n",
    "y = 2 * X + 1 + jnp.random.normal(0, 1, X.shape)  # Adjusted for noise\n",
    "\n",
    "\n",
    "Error:\n",
    "AttributeError: module 'jax.numpy' has no attribute 'random'\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Correct random number generation requires the use of jax.random.normal and the need to pass in the PRNG key\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key = random.PRNGKey(0)\n",
    "X = jnp.linspace(0, 10, num_samples).reshape(-1, 1)\n",
    "noise = random.normal(key, shape=X.shape)\n",
    "y = 2 * X + 1 + noise\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "y = 2 * X + 1 + noise\n",
    "\n",
    "\n",
    "Error:\n",
    "The linear relationship when the data was generated should be 2 * X + 3 instead of 2 * X + 1\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "The linear relationship when the data is generated should be 2 * X + 3\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "y = 2 * X + 3 + noise\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_step(params, x, y, rng_key):\n",
    "\n",
    "\n",
    "Error:\n",
    "The rng_key parameter is passed into the train_step function, but the training step does not require randomness\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Removed unused rng_key parameter\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def train_step(params, x, y):\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# Batch training step using vectorization\n",
    "def train_model(X, y, num_epochs=1000):\n",
    "    key = random.PRNGKey(0)  # Initialize PRNG key\n",
    "    params = jnp.array([0.0, 0.0])  # Initial parameters (w, b)\n",
    "    \n",
    "    # Vectorized training over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Create a new PRNG key for each batch (if batching)\n",
    "        key, subkey = random.split(key)\n",
    "        params = train_step(params, X, y, subkey)  # // MODIFIED: Removed loop for vectorization\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "Error:\n",
    "Since the training step does not require randomness, the generation and passing of rng_key should also be removed when training the model.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Remove the generation and passing of rng_key when training the model\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def train_model(X, y, num_epochs=1000):\n",
    "    params = jnp.array([0.0, 0.0])  # Initial parameters (w, b)\n",
    "    for epoch in range(num_epochs):\n",
    "        params = train_step(params, X, y)\n",
    "    return params\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "params = jnp.array([0.0, 0.0])  # Initial parameters (w, b)\n",
    "\n",
    "\n",
    "Error:\n",
    "Does not conform to the expected data structure and does not take advantage of random initialization\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "The parameters are initialized using a dictionary structure, and the weights and biases are initialized using random uniform distribution\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "bound = 1.0  # For in_features=1, bound = 1.\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "w = random.uniform(subkey, shape=(1, 1), minval=-bound, maxval=bound)\n",
    "key, subkey = random.split(key)\n",
    "b = random.uniform(subkey, shape=(1,), minval=-bound, maxval=bound)\n",
    "params = {\"w\": w, \"b\": b}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def model(params, x):\n",
    "    w, b = params\n",
    "    return w * x + b\n",
    "\n",
    "\n",
    "Error:\n",
    "The parameter structure and operation method are inconsistent with the original code\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "The forward function should take a dictionary of arguments and use matrix multiplication to emulate the behavior of nn.Linear\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def model(params, x):\n",
    "    return jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "return params - 0.01 * grads\n",
    "\n",
    "\n",
    "Error:\n",
    "In the original code, params is a dictionary (including \"w\" and \"b\"), and each parameter in the dictionary should be updated separately\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Each parameter in the dictionary should be updated separately\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "return {\n",
    "        \"w\": params[\"w\"] - 0.01 * grads[\"w\"],\n",
    "        \"b\": params[\"b\"] - 0.01 * grads[\"b\"]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "for epoch in range(num_epochs):\n",
    "        params = train_step(params, X, y)\n",
    "    return params\n",
    "            \n",
    "\n",
    "Error:\n",
    "In this training loop, no loss value for the current round is calculated, and no print statements are added to output log information\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "In each epoch, first calculate the loss and gradient, then update the parameters, and print the log when the conditions are met\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "for epoch in range(num_epochs):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, X, y)\n",
    "        params = {\n",
    "            \"w\": params[\"w\"] - 0.01 * grads[\"w\"],\n",
    "            \"b\": params[\"b\"] - 0.01 * grads[\"b\"]\n",
    "        }\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "    return params\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a47ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.6636\n",
      "Epoch [200/1000], Loss: 1.1676\n",
      "Epoch [300/1000], Loss: 0.9843\n",
      "Epoch [400/1000], Loss: 0.9166\n",
      "Epoch [500/1000], Loss: 0.8915\n",
      "Epoch [600/1000], Loss: 0.8823\n",
      "Epoch [700/1000], Loss: 0.8789\n",
      "Epoch [800/1000], Loss: 0.8776\n",
      "Epoch [900/1000], Loss: 0.8771\n",
      "Epoch [1000/1000], Loss: 0.8770\n",
      "Learned weight: 2.0338, Learned bias: 2.9082\n",
      "Predictions for [[4.0], [7.0]]: [[11.043389320373535], [17.14480972290039]]\n"
     ]
    }
   ],
   "source": [
    "# fixed code\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(num_samples=100):\n",
    "    key = random.PRNGKey(0)\n",
    "    X = jnp.linspace(0, 10, num_samples).reshape(-1, 1)\n",
    "    noise = random.normal(key, shape=X.shape)\n",
    "    y = 2 * X + 3 + noise  \n",
    "    return X, y\n",
    "\n",
    "# Linear regression model\n",
    "def model(params, x):\n",
    "    return jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(params, x, y):\n",
    "    preds = model(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Gradient computation\n",
    "@jit\n",
    "def compute_gradient(params, x, y):\n",
    "    return grad(loss_fn)(params, x, y)\n",
    "\n",
    "# Training step\n",
    "@jit\n",
    "def train_step(params, x, y):\n",
    "    grads = compute_gradient(params, x, y)\n",
    "    return {\n",
    "        \"w\": params[\"w\"] - 0.01 * grads[\"w\"],\n",
    "        \"b\": params[\"b\"] - 0.01 * grads[\"b\"]\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "def train_model(X, y, num_epochs=1000):\n",
    "    bound = 1.0  # For in_features=1, bound = 1.\n",
    "    key = random.PRNGKey(0)\n",
    "    key, subkey = random.split(key)\n",
    "    w = random.uniform(subkey, shape=(1, 1), minval=-bound, maxval=bound)\n",
    "    key, subkey = random.split(key)\n",
    "    b = random.uniform(subkey, shape=(1,), minval=-bound, maxval=bound)\n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, X, y)\n",
    "        params = {\n",
    "            \"w\": params[\"w\"] - 0.01 * grads[\"w\"],\n",
    "            \"b\": params[\"b\"] - 0.01 * grads[\"b\"]\n",
    "        }\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "    return params\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    X, y = generate_data(100)\n",
    "    learned_params = train_model(X, y)\n",
    "    learned_w = learned_params[\"w\"][0, 0]\n",
    "    learned_b = learned_params[\"b\"][0]\n",
    "    print(f\"Learned weight: {learned_w:.4f}, Learned bias: {learned_b:.4f}\")\n",
    "    \n",
    "    X_test = jnp.array([[4.0], [7.0]])\n",
    "    predictions = model(learned_params, X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a0d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
