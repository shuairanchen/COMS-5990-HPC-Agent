{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMwAVPOzl-jj",
        "outputId": "f49a4bf5-551e-4108-850b-3e3247800974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 0.1162\n",
            "Epoch [2/500], Loss: 0.0031\n",
            "Epoch [3/500], Loss: 0.0095\n",
            "Epoch [4/500], Loss: 0.0002\n",
            "Epoch [5/500], Loss: 0.0000\n",
            "Epoch [6/500], Loss: 0.0268\n",
            "Epoch [7/500], Loss: 0.0164\n",
            "Epoch [8/500], Loss: 0.0763\n",
            "Epoch [9/500], Loss: 0.0163\n",
            "Epoch [10/500], Loss: 0.0009\n",
            "Epoch [11/500], Loss: 0.0009\n",
            "Epoch [12/500], Loss: 0.0009\n",
            "Epoch [13/500], Loss: 0.0015\n",
            "Epoch [14/500], Loss: 0.0000\n",
            "Epoch [15/500], Loss: 0.0006\n",
            "Epoch [16/500], Loss: 0.0002\n",
            "Epoch [17/500], Loss: 0.0004\n",
            "Epoch [18/500], Loss: 0.0025\n",
            "Epoch [19/500], Loss: 0.0065\n",
            "Epoch [20/500], Loss: 0.0022\n",
            "Epoch [21/500], Loss: 0.0046\n",
            "Epoch [22/500], Loss: 0.0066\n",
            "Epoch [23/500], Loss: 0.0073\n",
            "Epoch [24/500], Loss: 0.0084\n",
            "Epoch [25/500], Loss: 0.0089\n",
            "Epoch [26/500], Loss: 0.0071\n",
            "Epoch [27/500], Loss: 0.0052\n",
            "Epoch [28/500], Loss: 0.0048\n",
            "Epoch [29/500], Loss: 0.0043\n",
            "Epoch [30/500], Loss: 0.0040\n",
            "Epoch [31/500], Loss: 0.0034\n",
            "Epoch [32/500], Loss: 0.0026\n",
            "Epoch [33/500], Loss: 0.0016\n",
            "Epoch [34/500], Loss: 0.0004\n",
            "Epoch [35/500], Loss: 0.0003\n",
            "Epoch [36/500], Loss: 0.0000\n",
            "Epoch [37/500], Loss: 0.0000\n",
            "Epoch [38/500], Loss: 0.0029\n",
            "Epoch [39/500], Loss: 0.0262\n",
            "Epoch [40/500], Loss: 0.0017\n",
            "Epoch [41/500], Loss: 0.0048\n",
            "Epoch [42/500], Loss: 0.0063\n",
            "Epoch [43/500], Loss: 0.0084\n",
            "Epoch [44/500], Loss: 0.0002\n",
            "Epoch [45/500], Loss: 0.0007\n",
            "Epoch [46/500], Loss: 0.0005\n",
            "Epoch [47/500], Loss: 0.0002\n",
            "Epoch [48/500], Loss: 0.0001\n",
            "Epoch [49/500], Loss: 0.0000\n",
            "Epoch [50/500], Loss: 0.0000\n",
            "Epoch [51/500], Loss: 0.0002\n",
            "Epoch [52/500], Loss: 0.0011\n",
            "Epoch [53/500], Loss: 0.0048\n",
            "Epoch [54/500], Loss: 0.0000\n",
            "Epoch [55/500], Loss: 0.0078\n",
            "Epoch [56/500], Loss: 0.0005\n",
            "Epoch [57/500], Loss: 0.0078\n",
            "Epoch [58/500], Loss: 0.0000\n",
            "Epoch [59/500], Loss: 0.0024\n",
            "Epoch [60/500], Loss: 0.0001\n",
            "Epoch [61/500], Loss: 0.0002\n",
            "Epoch [62/500], Loss: 0.0000\n",
            "Epoch [63/500], Loss: 0.0001\n",
            "Epoch [64/500], Loss: 0.0007\n",
            "Epoch [65/500], Loss: 0.0028\n",
            "Epoch [66/500], Loss: 0.0074\n",
            "Epoch [67/500], Loss: 0.0000\n",
            "Epoch [68/500], Loss: 0.0009\n",
            "Epoch [69/500], Loss: 0.0011\n",
            "Epoch [70/500], Loss: 0.0023\n",
            "Epoch [71/500], Loss: 0.0035\n",
            "Epoch [72/500], Loss: 0.0163\n",
            "Epoch [73/500], Loss: 0.0024\n",
            "Epoch [74/500], Loss: 0.0048\n",
            "Epoch [75/500], Loss: 0.0039\n",
            "Epoch [76/500], Loss: 0.0023\n",
            "Epoch [77/500], Loss: 0.0026\n",
            "Epoch [78/500], Loss: 0.0021\n",
            "Epoch [79/500], Loss: 0.0007\n",
            "Epoch [80/500], Loss: 0.0000\n",
            "Epoch [81/500], Loss: 0.0000\n",
            "Epoch [82/500], Loss: 0.0008\n",
            "Epoch [83/500], Loss: 0.0029\n",
            "Epoch [84/500], Loss: 0.0073\n",
            "Epoch [85/500], Loss: 0.0001\n",
            "Epoch [86/500], Loss: 0.0008\n",
            "Epoch [87/500], Loss: 0.0027\n",
            "Epoch [88/500], Loss: 0.0050\n",
            "Epoch [89/500], Loss: 0.0023\n",
            "Epoch [90/500], Loss: 0.0005\n",
            "Epoch [91/500], Loss: 0.0009\n",
            "Epoch [92/500], Loss: 0.0031\n",
            "Epoch [93/500], Loss: 0.0007\n",
            "Epoch [94/500], Loss: 0.0001\n",
            "Epoch [95/500], Loss: 0.0001\n",
            "Epoch [96/500], Loss: 0.0000\n",
            "Epoch [97/500], Loss: 0.0000\n",
            "Epoch [98/500], Loss: 0.0000\n",
            "Epoch [99/500], Loss: 0.0000\n",
            "Epoch [100/500], Loss: 0.0001\n",
            "Epoch [101/500], Loss: 0.0008\n",
            "Epoch [102/500], Loss: 0.0027\n",
            "Epoch [103/500], Loss: 0.0044\n",
            "Epoch [104/500], Loss: 0.0072\n",
            "Epoch [105/500], Loss: 0.0056\n",
            "Epoch [106/500], Loss: 0.0034\n",
            "Epoch [107/500], Loss: 0.0023\n",
            "Epoch [108/500], Loss: 0.0016\n",
            "Epoch [109/500], Loss: 0.0013\n",
            "Epoch [110/500], Loss: 0.0013\n",
            "Epoch [111/500], Loss: 0.0014\n",
            "Epoch [112/500], Loss: 0.0018\n",
            "Epoch [113/500], Loss: 0.0028\n",
            "Epoch [114/500], Loss: 0.0046\n",
            "Epoch [115/500], Loss: 0.0065\n",
            "Epoch [116/500], Loss: 0.0068\n",
            "Epoch [117/500], Loss: 0.0055\n",
            "Epoch [118/500], Loss: 0.0036\n",
            "Epoch [119/500], Loss: 0.0038\n",
            "Epoch [120/500], Loss: 0.0008\n",
            "Epoch [121/500], Loss: 0.0000\n",
            "Epoch [122/500], Loss: 0.0000\n",
            "Epoch [123/500], Loss: 0.0001\n",
            "Epoch [124/500], Loss: 0.0002\n",
            "Epoch [125/500], Loss: 0.0005\n",
            "Epoch [126/500], Loss: 0.0011\n",
            "Epoch [127/500], Loss: 0.0026\n",
            "Epoch [128/500], Loss: 0.0028\n",
            "Epoch [129/500], Loss: 0.0212\n",
            "Epoch [130/500], Loss: 0.0018\n",
            "Epoch [131/500], Loss: 0.0000\n",
            "Epoch [132/500], Loss: 0.0005\n",
            "Epoch [133/500], Loss: 0.0001\n",
            "Epoch [134/500], Loss: 0.0003\n",
            "Epoch [135/500], Loss: 0.0004\n",
            "Epoch [136/500], Loss: 0.0003\n",
            "Epoch [137/500], Loss: 0.0001\n",
            "Epoch [138/500], Loss: 0.0001\n",
            "Epoch [139/500], Loss: 0.0003\n",
            "Epoch [140/500], Loss: 0.0005\n",
            "Epoch [141/500], Loss: 0.0000\n",
            "Epoch [142/500], Loss: 0.0003\n",
            "Epoch [143/500], Loss: 0.0018\n",
            "Epoch [144/500], Loss: 0.0031\n",
            "Epoch [145/500], Loss: 0.0020\n",
            "Epoch [146/500], Loss: 0.0057\n",
            "Epoch [147/500], Loss: 0.0081\n",
            "Epoch [148/500], Loss: 0.0025\n",
            "Epoch [149/500], Loss: 0.0000\n",
            "Epoch [150/500], Loss: 0.0001\n",
            "Epoch [151/500], Loss: 0.0002\n",
            "Epoch [152/500], Loss: 0.0005\n",
            "Epoch [153/500], Loss: 0.0010\n",
            "Epoch [154/500], Loss: 0.0016\n",
            "Epoch [155/500], Loss: 0.0025\n",
            "Epoch [156/500], Loss: 0.0038\n",
            "Epoch [157/500], Loss: 0.0055\n",
            "Epoch [158/500], Loss: 0.0049\n",
            "Epoch [159/500], Loss: 0.0003\n",
            "Epoch [160/500], Loss: 0.0000\n",
            "Epoch [161/500], Loss: 0.0047\n",
            "Epoch [162/500], Loss: 0.0058\n",
            "Epoch [163/500], Loss: 0.0013\n",
            "Epoch [164/500], Loss: 0.0002\n",
            "Epoch [165/500], Loss: 0.0002\n",
            "Epoch [166/500], Loss: 0.0002\n",
            "Epoch [167/500], Loss: 0.0003\n",
            "Epoch [168/500], Loss: 0.0004\n",
            "Epoch [169/500], Loss: 0.0006\n",
            "Epoch [170/500], Loss: 0.0008\n",
            "Epoch [171/500], Loss: 0.0011\n",
            "Epoch [172/500], Loss: 0.0015\n",
            "Epoch [173/500], Loss: 0.0021\n",
            "Epoch [174/500], Loss: 0.0027\n",
            "Epoch [175/500], Loss: 0.0032\n",
            "Epoch [176/500], Loss: 0.0029\n",
            "Epoch [177/500], Loss: 0.0002\n",
            "Epoch [178/500], Loss: 0.0028\n",
            "Epoch [179/500], Loss: 0.0000\n",
            "Epoch [180/500], Loss: 0.0017\n",
            "Epoch [181/500], Loss: 0.0037\n",
            "Epoch [182/500], Loss: 0.0004\n",
            "Epoch [183/500], Loss: 0.0015\n",
            "Epoch [184/500], Loss: 0.0056\n",
            "Epoch [185/500], Loss: 0.0012\n",
            "Epoch [186/500], Loss: 0.0017\n",
            "Epoch [187/500], Loss: 0.0047\n",
            "Epoch [188/500], Loss: 0.0023\n",
            "Epoch [189/500], Loss: 0.0024\n",
            "Epoch [190/500], Loss: 0.0026\n",
            "Epoch [191/500], Loss: 0.0026\n",
            "Epoch [192/500], Loss: 0.0008\n",
            "Epoch [193/500], Loss: 0.0014\n",
            "Epoch [194/500], Loss: 0.0003\n",
            "Epoch [195/500], Loss: 0.0002\n",
            "Epoch [196/500], Loss: 0.0001\n",
            "Epoch [197/500], Loss: 0.0003\n",
            "Epoch [198/500], Loss: 0.0006\n",
            "Epoch [199/500], Loss: 0.0011\n",
            "Epoch [200/500], Loss: 0.0017\n",
            "Epoch [201/500], Loss: 0.0025\n",
            "Epoch [202/500], Loss: 0.0035\n",
            "Epoch [203/500], Loss: 0.0042\n",
            "Epoch [204/500], Loss: 0.0044\n",
            "Epoch [205/500], Loss: 0.0042\n",
            "Epoch [206/500], Loss: 0.0034\n",
            "Epoch [207/500], Loss: 0.0034\n",
            "Epoch [208/500], Loss: 0.0021\n",
            "Epoch [209/500], Loss: 0.0011\n",
            "Epoch [210/500], Loss: 0.0017\n",
            "Epoch [211/500], Loss: 0.0017\n",
            "Epoch [212/500], Loss: 0.0022\n",
            "Epoch [213/500], Loss: 0.0027\n",
            "Epoch [214/500], Loss: 0.0039\n",
            "Epoch [215/500], Loss: 0.0030\n",
            "Epoch [216/500], Loss: 0.0040\n",
            "Epoch [217/500], Loss: 0.0021\n",
            "Epoch [218/500], Loss: 0.0014\n",
            "Epoch [219/500], Loss: 0.0016\n",
            "Epoch [220/500], Loss: 0.0001\n",
            "Epoch [221/500], Loss: 0.0000\n",
            "Epoch [222/500], Loss: 0.0000\n",
            "Epoch [223/500], Loss: 0.0002\n",
            "Epoch [224/500], Loss: 0.0006\n",
            "Epoch [225/500], Loss: 0.0015\n",
            "Epoch [226/500], Loss: 0.0030\n",
            "Epoch [227/500], Loss: 0.0003\n",
            "Epoch [228/500], Loss: 0.0111\n",
            "Epoch [229/500], Loss: 0.0000\n",
            "Epoch [230/500], Loss: 0.0007\n",
            "Epoch [231/500], Loss: 0.0034\n",
            "Epoch [232/500], Loss: 0.0168\n",
            "Epoch [233/500], Loss: 0.0023\n",
            "Epoch [234/500], Loss: 0.0000\n",
            "Epoch [235/500], Loss: 0.0000\n",
            "Epoch [236/500], Loss: 0.0029\n",
            "Epoch [237/500], Loss: 0.0002\n",
            "Epoch [238/500], Loss: 0.0063\n",
            "Epoch [239/500], Loss: 0.0006\n",
            "Epoch [240/500], Loss: 0.0014\n",
            "Epoch [241/500], Loss: 0.0000\n",
            "Epoch [242/500], Loss: 0.0000\n",
            "Epoch [243/500], Loss: 0.0000\n",
            "Epoch [244/500], Loss: 0.0000\n",
            "Epoch [245/500], Loss: 0.0000\n",
            "Epoch [246/500], Loss: 0.0001\n",
            "Epoch [247/500], Loss: 0.0003\n",
            "Epoch [248/500], Loss: 0.0006\n",
            "Epoch [249/500], Loss: 0.0011\n",
            "Epoch [250/500], Loss: 0.0018\n",
            "Epoch [251/500], Loss: 0.0028\n",
            "Epoch [252/500], Loss: 0.0044\n",
            "Epoch [253/500], Loss: 0.0068\n",
            "Epoch [254/500], Loss: 0.0090\n",
            "Epoch [255/500], Loss: 0.0081\n",
            "Epoch [256/500], Loss: 0.0034\n",
            "Epoch [257/500], Loss: 0.0040\n",
            "Epoch [258/500], Loss: 0.0039\n",
            "Epoch [259/500], Loss: 0.0022\n",
            "Epoch [260/500], Loss: 0.0012\n",
            "Epoch [261/500], Loss: 0.0009\n",
            "Epoch [262/500], Loss: 0.0007\n",
            "Epoch [263/500], Loss: 0.0006\n",
            "Epoch [264/500], Loss: 0.0006\n",
            "Epoch [265/500], Loss: 0.0007\n",
            "Epoch [266/500], Loss: 0.0010\n",
            "Epoch [267/500], Loss: 0.0019\n",
            "Epoch [268/500], Loss: 0.0039\n",
            "Epoch [269/500], Loss: 0.0030\n",
            "Epoch [270/500], Loss: 0.0006\n",
            "Epoch [271/500], Loss: 0.0005\n",
            "Epoch [272/500], Loss: 0.0020\n",
            "Epoch [273/500], Loss: 0.0000\n",
            "Epoch [274/500], Loss: 0.0000\n",
            "Epoch [275/500], Loss: 0.0007\n",
            "Epoch [276/500], Loss: 0.0002\n",
            "Epoch [277/500], Loss: 0.0001\n",
            "Epoch [278/500], Loss: 0.0005\n",
            "Epoch [279/500], Loss: 0.0004\n",
            "Epoch [280/500], Loss: 0.0000\n",
            "Epoch [281/500], Loss: 0.0003\n",
            "Epoch [282/500], Loss: 0.0022\n",
            "Epoch [283/500], Loss: 0.0001\n",
            "Epoch [284/500], Loss: 0.0026\n",
            "Epoch [285/500], Loss: 0.0028\n",
            "Epoch [286/500], Loss: 0.0010\n",
            "Epoch [287/500], Loss: 0.0000\n",
            "Epoch [288/500], Loss: 0.0023\n",
            "Epoch [289/500], Loss: 0.0000\n",
            "Epoch [290/500], Loss: 0.0006\n",
            "Epoch [291/500], Loss: 0.0013\n",
            "Epoch [292/500], Loss: 0.0008\n",
            "Epoch [293/500], Loss: 0.0000\n",
            "Epoch [294/500], Loss: 0.0001\n",
            "Epoch [295/500], Loss: 0.0014\n",
            "Epoch [296/500], Loss: 0.0017\n",
            "Epoch [297/500], Loss: 0.0006\n",
            "Epoch [298/500], Loss: 0.0000\n",
            "Epoch [299/500], Loss: 0.0000\n",
            "Epoch [300/500], Loss: 0.0006\n",
            "Epoch [301/500], Loss: 0.0011\n",
            "Epoch [302/500], Loss: 0.0021\n",
            "Epoch [303/500], Loss: 0.0002\n",
            "Epoch [304/500], Loss: 0.0001\n",
            "Epoch [305/500], Loss: 0.0056\n",
            "Epoch [306/500], Loss: 0.0022\n",
            "Epoch [307/500], Loss: 0.0756\n",
            "Epoch [308/500], Loss: 0.0002\n",
            "Epoch [309/500], Loss: 0.0000\n",
            "Epoch [310/500], Loss: 0.0000\n",
            "Epoch [311/500], Loss: 0.0000\n",
            "Epoch [312/500], Loss: 0.0000\n",
            "Epoch [313/500], Loss: 0.0000\n",
            "Epoch [314/500], Loss: 0.0000\n",
            "Epoch [315/500], Loss: 0.0000\n",
            "Epoch [316/500], Loss: 0.0000\n",
            "Epoch [317/500], Loss: 0.0000\n",
            "Epoch [318/500], Loss: 0.0001\n",
            "Epoch [319/500], Loss: 0.0016\n",
            "Epoch [320/500], Loss: 0.0044\n",
            "Epoch [321/500], Loss: 0.0025\n",
            "Epoch [322/500], Loss: 0.0010\n",
            "Epoch [323/500], Loss: 0.0007\n",
            "Epoch [324/500], Loss: 0.0006\n",
            "Epoch [325/500], Loss: 0.0006\n",
            "Epoch [326/500], Loss: 0.0007\n",
            "Epoch [327/500], Loss: 0.0008\n",
            "Epoch [328/500], Loss: 0.0008\n",
            "Epoch [329/500], Loss: 0.0004\n",
            "Epoch [330/500], Loss: 0.0000\n",
            "Epoch [331/500], Loss: 0.0012\n",
            "Epoch [332/500], Loss: 0.0000\n",
            "Epoch [333/500], Loss: 0.0024\n",
            "Epoch [334/500], Loss: 0.0002\n",
            "Epoch [335/500], Loss: 0.0002\n",
            "Epoch [336/500], Loss: 0.0006\n",
            "Epoch [337/500], Loss: 0.0011\n",
            "Epoch [338/500], Loss: 0.0019\n",
            "Epoch [339/500], Loss: 0.0005\n",
            "Epoch [340/500], Loss: 0.0000\n",
            "Epoch [341/500], Loss: 0.0002\n",
            "Epoch [342/500], Loss: 0.0014\n",
            "Epoch [343/500], Loss: 0.0060\n",
            "Epoch [344/500], Loss: 0.0159\n",
            "Epoch [345/500], Loss: 0.0072\n",
            "Epoch [346/500], Loss: 0.0014\n",
            "Epoch [347/500], Loss: 0.0011\n",
            "Epoch [348/500], Loss: 0.0000\n",
            "Epoch [349/500], Loss: 0.0002\n",
            "Epoch [350/500], Loss: 0.0000\n",
            "Epoch [351/500], Loss: 0.0000\n",
            "Epoch [352/500], Loss: 0.0000\n",
            "Epoch [353/500], Loss: 0.0001\n",
            "Epoch [354/500], Loss: 0.0003\n",
            "Epoch [355/500], Loss: 0.0014\n",
            "Epoch [356/500], Loss: 0.0014\n",
            "Epoch [357/500], Loss: 0.0024\n",
            "Epoch [358/500], Loss: 0.0032\n",
            "Epoch [359/500], Loss: 0.0017\n",
            "Epoch [360/500], Loss: 0.0008\n",
            "Epoch [361/500], Loss: 0.0005\n",
            "Epoch [362/500], Loss: 0.0003\n",
            "Epoch [363/500], Loss: 0.0003\n",
            "Epoch [364/500], Loss: 0.0006\n",
            "Epoch [365/500], Loss: 0.0018\n",
            "Epoch [366/500], Loss: 0.0040\n",
            "Epoch [367/500], Loss: 0.0005\n",
            "Epoch [368/500], Loss: 0.0002\n",
            "Epoch [369/500], Loss: 0.0011\n",
            "Epoch [370/500], Loss: 0.0018\n",
            "Epoch [371/500], Loss: 0.0019\n",
            "Epoch [372/500], Loss: 0.0001\n",
            "Epoch [373/500], Loss: 0.0027\n",
            "Epoch [374/500], Loss: 0.0001\n",
            "Epoch [375/500], Loss: 0.0002\n",
            "Epoch [376/500], Loss: 0.0003\n",
            "Epoch [377/500], Loss: 0.0004\n",
            "Epoch [378/500], Loss: 0.0005\n",
            "Epoch [379/500], Loss: 0.0009\n",
            "Epoch [380/500], Loss: 0.0021\n",
            "Epoch [381/500], Loss: 0.0000\n",
            "Epoch [382/500], Loss: 0.0023\n",
            "Epoch [383/500], Loss: 0.0007\n",
            "Epoch [384/500], Loss: 0.0010\n",
            "Epoch [385/500], Loss: 0.0021\n",
            "Epoch [386/500], Loss: 0.0015\n",
            "Epoch [387/500], Loss: 0.0029\n",
            "Epoch [388/500], Loss: 0.0000\n",
            "Epoch [389/500], Loss: 0.0000\n",
            "Epoch [390/500], Loss: 0.0001\n",
            "Epoch [391/500], Loss: 0.0002\n",
            "Epoch [392/500], Loss: 0.0005\n",
            "Epoch [393/500], Loss: 0.0022\n",
            "Epoch [394/500], Loss: 0.0045\n",
            "Epoch [395/500], Loss: 0.0047\n",
            "Epoch [396/500], Loss: 0.0017\n",
            "Epoch [397/500], Loss: 0.0001\n",
            "Epoch [398/500], Loss: 0.0000\n",
            "Epoch [399/500], Loss: 0.0003\n",
            "Epoch [400/500], Loss: 0.0005\n",
            "Epoch [401/500], Loss: 0.0011\n",
            "Epoch [402/500], Loss: 0.0027\n",
            "Epoch [403/500], Loss: 0.0029\n",
            "Epoch [404/500], Loss: 0.0004\n",
            "Epoch [405/500], Loss: 0.0010\n",
            "Epoch [406/500], Loss: 0.0060\n",
            "Epoch [407/500], Loss: 0.0005\n",
            "Epoch [408/500], Loss: 0.0001\n",
            "Epoch [409/500], Loss: 0.0020\n",
            "Epoch [410/500], Loss: 0.0074\n",
            "Epoch [411/500], Loss: 0.0001\n",
            "Epoch [412/500], Loss: 0.0002\n",
            "Epoch [413/500], Loss: 0.0002\n",
            "Epoch [414/500], Loss: 0.0002\n",
            "Epoch [415/500], Loss: 0.0002\n",
            "Epoch [416/500], Loss: 0.0002\n",
            "Epoch [417/500], Loss: 0.0003\n",
            "Epoch [418/500], Loss: 0.0006\n",
            "Epoch [419/500], Loss: 0.0016\n",
            "Epoch [420/500], Loss: 0.0045\n",
            "Epoch [421/500], Loss: 0.0052\n",
            "Epoch [422/500], Loss: 0.0037\n",
            "Epoch [423/500], Loss: 0.0015\n",
            "Epoch [424/500], Loss: 0.0007\n",
            "Epoch [425/500], Loss: 0.0009\n",
            "Epoch [426/500], Loss: 0.0014\n",
            "Epoch [427/500], Loss: 0.0020\n",
            "Epoch [428/500], Loss: 0.0025\n",
            "Epoch [429/500], Loss: 0.0031\n",
            "Epoch [430/500], Loss: 0.0050\n",
            "Epoch [431/500], Loss: 0.0035\n",
            "Epoch [432/500], Loss: 0.0020\n",
            "Epoch [433/500], Loss: 0.0003\n",
            "Epoch [434/500], Loss: 0.0002\n",
            "Epoch [435/500], Loss: 0.0000\n",
            "Epoch [436/500], Loss: 0.0000\n",
            "Epoch [437/500], Loss: 0.0000\n",
            "Epoch [438/500], Loss: 0.0004\n",
            "Epoch [439/500], Loss: 0.0044\n",
            "Epoch [440/500], Loss: 0.0012\n",
            "Epoch [441/500], Loss: 0.0009\n",
            "Epoch [442/500], Loss: 0.0000\n",
            "Epoch [443/500], Loss: 0.0001\n",
            "Epoch [444/500], Loss: 0.0025\n",
            "Epoch [445/500], Loss: 0.0019\n",
            "Epoch [446/500], Loss: 0.0001\n",
            "Epoch [447/500], Loss: 0.0000\n",
            "Epoch [448/500], Loss: 0.0026\n",
            "Epoch [449/500], Loss: 0.0000\n",
            "Epoch [450/500], Loss: 0.0010\n",
            "Epoch [451/500], Loss: 0.0000\n",
            "Epoch [452/500], Loss: 0.0000\n",
            "Epoch [453/500], Loss: 0.0001\n",
            "Epoch [454/500], Loss: 0.0002\n",
            "Epoch [455/500], Loss: 0.0000\n",
            "Epoch [456/500], Loss: 0.0009\n",
            "Epoch [457/500], Loss: 0.0001\n",
            "Epoch [458/500], Loss: 0.0007\n",
            "Epoch [459/500], Loss: 0.0000\n",
            "Epoch [460/500], Loss: 0.0006\n",
            "Epoch [461/500], Loss: 0.0000\n",
            "Epoch [462/500], Loss: 0.0017\n",
            "Epoch [463/500], Loss: 0.0012\n",
            "Epoch [464/500], Loss: 0.0011\n",
            "Epoch [465/500], Loss: 0.0000\n",
            "Epoch [466/500], Loss: 0.0001\n",
            "Epoch [467/500], Loss: 0.0004\n",
            "Epoch [468/500], Loss: 0.0011\n",
            "Epoch [469/500], Loss: 0.0008\n",
            "Epoch [470/500], Loss: 0.0008\n",
            "Epoch [471/500], Loss: 0.0000\n",
            "Epoch [472/500], Loss: 0.0000\n",
            "Epoch [473/500], Loss: 0.0006\n",
            "Epoch [474/500], Loss: 0.0013\n",
            "Epoch [475/500], Loss: 0.0024\n",
            "Epoch [476/500], Loss: 0.0021\n",
            "Epoch [477/500], Loss: 0.0000\n",
            "Epoch [478/500], Loss: 0.0001\n",
            "Epoch [479/500], Loss: 0.0004\n",
            "Epoch [480/500], Loss: 0.0010\n",
            "Epoch [481/500], Loss: 0.0056\n",
            "Epoch [482/500], Loss: 0.0010\n",
            "Epoch [483/500], Loss: 0.0002\n",
            "Epoch [484/500], Loss: 0.0570\n",
            "Epoch [485/500], Loss: 0.6827\n",
            "Epoch [486/500], Loss: 0.5329\n",
            "Epoch [487/500], Loss: 0.1323\n",
            "Epoch [488/500], Loss: 0.1423\n",
            "Epoch [489/500], Loss: 0.0127\n",
            "Epoch [490/500], Loss: 0.0080\n",
            "Epoch [491/500], Loss: 0.0045\n",
            "Epoch [492/500], Loss: 0.0007\n",
            "Epoch [493/500], Loss: 0.0008\n",
            "Epoch [494/500], Loss: 0.0001\n",
            "Epoch [495/500], Loss: 0.0000\n",
            "Epoch [496/500], Loss: 0.0000\n",
            "Epoch [497/500], Loss: 0.0011\n",
            "Epoch [498/500], Loss: 0.0005\n",
            "Epoch [499/500], Loss: 0.0007\n",
            "Epoch [500/500], Loss: 0.0040\n",
            "Predictions for new sequence: [[3.1368894577026367]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Generate synthetic sequential data\n",
        "torch.manual_seed(42)\n",
        "sequence_length = 10\n",
        "num_samples = 100\n",
        "\n",
        "# Create a sine wave dataset\n",
        "X = torch.linspace(0, 4 * 3.14159, steps=num_samples).unsqueeze(1)\n",
        "y = torch.sin(X)\n",
        "\n",
        "# Prepare data for RNN\n",
        "def create_in_out_sequences(data, seq_length):\n",
        "    in_seq = []\n",
        "    out_seq = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        in_seq.append(data[i:i + seq_length])\n",
        "        out_seq.append(data[i + seq_length])\n",
        "    return torch.stack(in_seq), torch.stack(out_seq)\n",
        "\n",
        "X_seq, y_seq = create_in_out_sequences(y, sequence_length)\n",
        "\n",
        "# Define the RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(50, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Use the last output of the RNN\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = RNNModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    for sequences, labels in zip(X_seq, y_seq):\n",
        "        sequences = sequences.unsqueeze(0)  # Add batch dimension\n",
        "        labels = labels.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Testing on new data\n",
        "X_test = torch.linspace(4 * 3.14159, 5 * 3.14159, steps=10).unsqueeze(1)\n",
        "\n",
        "# Reshape to (batch_size, sequence_length, input_size)\n",
        "X_test = X_test.unsqueeze(0)  # Add batch dimension, shape becomes (1, 10, 1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test)\n",
        "    print(f\"Predictions for new sequence: {predictions.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# Data Preparation\n",
        "# -------------------------------\n",
        "sequence_length = 10\n",
        "num_samples = 100\n",
        "\n",
        "# Create a sine wave dataset (using 4 * pi as the end point)\n",
        "X = jnp.linspace(0, 4 * jnp.pi, num=num_samples)[:, None]\n",
        "y = jnp.sin(X)\n",
        "\n",
        "def create_in_out_sequences(data, seq_length):\n",
        "    in_seq = []\n",
        "    out_seq = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        in_seq.append(data[i:i + seq_length])\n",
        "        out_seq.append(data[i + seq_length])\n",
        "    return jnp.stack(in_seq), jnp.stack(out_seq)\n",
        "\n",
        "X_seq, y_seq = create_in_out_sequences(y, sequence_length)\n",
        "\n",
        "# -------------------------------\n",
        "# Define the RNN Model in Flax\n",
        "# -------------------------------\n",
        "class RNNCell(nn.Module):\n",
        "    hidden_size: int = 50\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, carry, x):\n",
        "        # carry: previous hidden state, shape (batch, hidden_size)\n",
        "        # x: current input, shape (batch, input_size)\n",
        "        # Compute new hidden state: tanh(W_ih*x + W_hh*carry + b)\n",
        "        new_h = nn.tanh(\n",
        "            nn.Dense(self.hidden_size, name=\"ih\")(x) +\n",
        "            nn.Dense(self.hidden_size, use_bias=False, name=\"hh\")(carry)\n",
        "        )\n",
        "        return new_h, new_h  # returning new state as both carry and output\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    hidden_size: int = 50\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # x shape: (batch, seq_length, input_size)\n",
        "        batch_size = x.shape[0]\n",
        "        init_carry = jnp.zeros((batch_size, self.hidden_size))\n",
        "        # Instead of instantiating RNNCell, pass the class to nn.scan\n",
        "        rnn_scan = nn.scan(\n",
        "            RNNCell,  # pass the class instead of an instance\n",
        "            in_axes=1,\n",
        "            out_axes=1,\n",
        "            variable_broadcast=\"params\",\n",
        "            split_rngs={\"params\": False},\n",
        "        )(hidden_size=self.hidden_size)  # now provide the argument for hidden_size\n",
        "        carry, ys = rnn_scan(init_carry, x)\n",
        "        # Use the output at the final time step\n",
        "        last_output = ys[:, -1, :]\n",
        "        output = nn.Dense(1)(last_output)\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Initialize the Model and Optimizer\n",
        "# -------------------------------\n",
        "model = RNNModel()\n",
        "rng = jax.random.PRNGKey(42)\n",
        "# Sample input with shape (batch=1, seq_length, input_size)\n",
        "sample_input = jnp.ones((1, sequence_length, 1))\n",
        "params = model.init(rng, sample_input)\n",
        "\n",
        "# Set up the Adam optimizer from Optax\n",
        "optimizer = optax.adam(learning_rate=0.001)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# -------------------------------\n",
        "# Define Loss and Training Step\n",
        "# -------------------------------\n",
        "def loss_fn(params, x, y):\n",
        "    preds = model.apply(params, x)\n",
        "    return jnp.mean((preds - y) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, x, y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "# -------------------------------\n",
        "# Training Loop\n",
        "# -------------------------------\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    # Loop over each sample (each sample has shape (sequence_length, 1))\n",
        "    for seq, label in zip(X_seq, y_seq):\n",
        "        # Add a batch dimension: new shape becomes (1, sequence_length, 1)\n",
        "        seq = seq[None, :, :]\n",
        "        label = label[None, :]\n",
        "        params, opt_state, loss = train_step(params, opt_state, seq, label)\n",
        "        epoch_loss += loss\n",
        "    epoch_loss /= len(X_seq)\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Testing on New Data\n",
        "# -------------------------------\n",
        "# Create new test data (from 4*pi to 5*pi)\n",
        "X_test = jnp.linspace(4 * jnp.pi, 5 * jnp.pi, num=10)[:, None]\n",
        "X_test = X_test[None, :, :]  # Add batch dimension: shape becomes (1, 10, 1)\n",
        "predictions = model.apply(params, X_test)\n",
        "print(\"Predictions for new sequence:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-_jQCYnoxZk",
        "outputId": "40838676-e6a6-4a0c-88be-929ae5d03cd9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 0.0716\n",
            "Epoch [5/500], Loss: 0.0019\n",
            "Epoch [10/500], Loss: 0.1030\n",
            "Epoch [15/500], Loss: 0.0176\n",
            "Epoch [20/500], Loss: 0.0087\n",
            "Epoch [25/500], Loss: 0.0101\n",
            "Epoch [30/500], Loss: 0.0030\n",
            "Epoch [35/500], Loss: 0.0073\n",
            "Epoch [40/500], Loss: 0.0100\n",
            "Epoch [45/500], Loss: 0.0001\n",
            "Epoch [50/500], Loss: 0.0001\n",
            "Epoch [55/500], Loss: 0.0142\n",
            "Epoch [60/500], Loss: 0.0001\n",
            "Epoch [65/500], Loss: 0.0007\n",
            "Epoch [70/500], Loss: 0.0120\n",
            "Epoch [75/500], Loss: 0.0004\n",
            "Epoch [80/500], Loss: 0.0010\n",
            "Epoch [85/500], Loss: 0.0089\n",
            "Epoch [90/500], Loss: 0.0103\n",
            "Epoch [95/500], Loss: 0.0006\n",
            "Epoch [100/500], Loss: 0.0003\n",
            "Epoch [105/500], Loss: 0.0047\n",
            "Epoch [110/500], Loss: 0.0066\n",
            "Epoch [115/500], Loss: 0.0008\n",
            "Epoch [120/500], Loss: 0.0042\n",
            "Epoch [125/500], Loss: 0.0062\n",
            "Epoch [130/500], Loss: 0.0005\n",
            "Epoch [135/500], Loss: 0.0031\n",
            "Epoch [140/500], Loss: 0.0041\n",
            "Epoch [145/500], Loss: 0.0021\n",
            "Epoch [150/500], Loss: 0.0024\n",
            "Epoch [155/500], Loss: 0.0015\n",
            "Epoch [160/500], Loss: 0.0026\n",
            "Epoch [165/500], Loss: 0.0039\n",
            "Epoch [170/500], Loss: 0.0029\n",
            "Epoch [175/500], Loss: 0.0029\n",
            "Epoch [180/500], Loss: 0.0048\n",
            "Epoch [185/500], Loss: 0.0035\n",
            "Epoch [190/500], Loss: 0.0044\n",
            "Epoch [195/500], Loss: 0.0041\n",
            "Epoch [200/500], Loss: 0.0022\n",
            "Epoch [205/500], Loss: 0.0005\n",
            "Epoch [210/500], Loss: 0.0099\n",
            "Epoch [215/500], Loss: 0.0019\n",
            "Epoch [220/500], Loss: 0.0003\n",
            "Epoch [225/500], Loss: 0.0017\n",
            "Epoch [230/500], Loss: 0.0044\n",
            "Epoch [235/500], Loss: 0.0015\n",
            "Epoch [240/500], Loss: 0.0023\n",
            "Epoch [245/500], Loss: 0.0045\n",
            "Epoch [250/500], Loss: 0.0020\n",
            "Epoch [255/500], Loss: 0.0041\n",
            "Epoch [260/500], Loss: 0.0018\n",
            "Epoch [265/500], Loss: 0.0039\n",
            "Epoch [270/500], Loss: 0.0023\n",
            "Epoch [275/500], Loss: 0.0011\n",
            "Epoch [280/500], Loss: 0.0043\n",
            "Epoch [285/500], Loss: 0.0040\n",
            "Epoch [290/500], Loss: 0.0014\n",
            "Epoch [295/500], Loss: 0.0012\n",
            "Epoch [300/500], Loss: 0.0029\n",
            "Epoch [305/500], Loss: 0.0045\n",
            "Epoch [310/500], Loss: 0.0027\n",
            "Epoch [315/500], Loss: 0.0032\n",
            "Epoch [320/500], Loss: 0.0028\n",
            "Epoch [325/500], Loss: 0.0015\n",
            "Epoch [330/500], Loss: 0.0015\n",
            "Epoch [335/500], Loss: 0.0011\n",
            "Epoch [340/500], Loss: 0.0009\n",
            "Epoch [345/500], Loss: 0.0043\n",
            "Epoch [350/500], Loss: 0.0051\n",
            "Epoch [355/500], Loss: 0.0022\n",
            "Epoch [360/500], Loss: 0.0001\n",
            "Epoch [365/500], Loss: 0.0001\n",
            "Epoch [370/500], Loss: 0.0005\n",
            "Epoch [375/500], Loss: 0.0021\n",
            "Epoch [380/500], Loss: 0.0047\n",
            "Epoch [385/500], Loss: 0.0036\n",
            "Epoch [390/500], Loss: 0.0034\n",
            "Epoch [395/500], Loss: 0.0026\n",
            "Epoch [400/500], Loss: 0.0006\n",
            "Epoch [405/500], Loss: 0.0026\n",
            "Epoch [410/500], Loss: 0.0010\n",
            "Epoch [415/500], Loss: 0.0008\n",
            "Epoch [420/500], Loss: 0.0026\n",
            "Epoch [425/500], Loss: 0.0005\n",
            "Epoch [430/500], Loss: 0.0035\n",
            "Epoch [435/500], Loss: 0.0005\n",
            "Epoch [440/500], Loss: 0.0009\n",
            "Epoch [445/500], Loss: 0.0047\n",
            "Epoch [450/500], Loss: 0.0019\n",
            "Epoch [455/500], Loss: 0.0018\n",
            "Epoch [460/500], Loss: 0.0014\n",
            "Epoch [465/500], Loss: 0.0010\n",
            "Epoch [470/500], Loss: 0.0033\n",
            "Epoch [475/500], Loss: 0.0001\n",
            "Epoch [480/500], Loss: 0.0004\n",
            "Epoch [485/500], Loss: 0.0072\n",
            "Epoch [490/500], Loss: 0.0045\n",
            "Epoch [495/500], Loss: 0.0011\n",
            "Epoch [500/500], Loss: 0.0032\n",
            "Predictions for new sequence: [[1.7477443]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weak LLM\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "import numpy as np\n",
        "\n",
        "# RNN Cell Definition\n",
        "class RNNCell(nn.Module):\n",
        "    hidden_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        # MODIFIED: Initialize weights for the RNN cell\n",
        "        self.W_ih = self.param('W_ih', nn.initializers.xavier_uniform(), (self.hidden_size, self.hidden_size))\n",
        "        self.W_hh = self.param('W_hh', nn.initializers.xavier_uniform(), (self.hidden_size, self.hidden_size))\n",
        "\n",
        "    def __call__(self, x, hidden_state):\n",
        "        # MODIFIED: Ensure hidden state is properly utilized and returned\n",
        "        new_hidden_state = jnp.tanh(jnp.dot(x, self.W_ih) + jnp.dot(hidden_state, self.W_hh))\n",
        "        return new_hidden_state\n",
        "\n",
        "# RNN Module Definition\n",
        "class RNN(nn.Module):\n",
        "    hidden_size: int\n",
        "    output_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.rnn_cell = RNNCell(self.hidden_size)\n",
        "        self.fc = nn.Dense(self.output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # MODIFIED: Initialized hidden state explicitly\n",
        "        hidden_state = jnp.zeros((x.shape[0], self.hidden_size))\n",
        "\n",
        "        def rnn_step(hidden_state, x_t):\n",
        "            return self.rnn_cell(x_t, hidden_state)  # MODIFIED: Pass hidden state explicitly\n",
        "\n",
        "        # Using jax.lax.scan for efficient state propagation\n",
        "        hidden_states = jax.lax.scan(rnn_step, hidden_state, x)[0]  # MODIFIED: Capture hidden states\n",
        "        output = self.fc(hidden_states)\n",
        "        return output\n",
        "\n",
        "# Loss Function\n",
        "def compute_loss(logits, targets):\n",
        "    return jnp.mean(jax.nn.softmax_cross_entropy(logits=logits, labels=targets))\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Sample data for training (Dummy data)\n",
        "    x_train = jnp.array(np.random.rand(100, 10, 1))  # 100 samples, 10 timesteps, 1 feature\n",
        "    y_train = jnp.array(np.random.randint(0, 2, (100, 10, 2)))  # 2 classes\n",
        "\n",
        "    model = RNN(hidden_size=16, output_size=2)  # Instantiate the RNN model\n",
        "    params = model.init(jax.random.PRNGKey(0), x_train)  # Initialize parameters\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    # Training Loop\n",
        "    epochs = 5\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        logits = model.apply(params, x_train)\n",
        "        loss = compute_loss(logits, y_train)\n",
        "\n",
        "        # Compute gradients and update parameters\n",
        "        grads = jax.grad(compute_loss)(params, y_train)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
        "\n",
        "    # Testing on new data\n",
        "    X_test = np.linspace(4 * np.pi, 5 * np.pi, 10).reshape(-1, 1)\n",
        "    X_test = jnp.expand_dims(X_test, axis=0)  # Add batch dimension\n",
        "\n",
        "    predictions = model.apply(params, X_test)\n",
        "    print(f\"Predictions for new sequence: {predictions.tolist()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_pEzdyltrUBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Error Code\n",
        "\n",
        "  model = RNN(hidden_size=16, output_size=2)  # Instantiate the RNN model\n",
        "  params = model.init(jax.random.PRNGKey(0), x_train)\n",
        "\n",
        "def setup(self):\n",
        "        self.W_ih = self.param('W_ih', nn.initializers.xavier_uniform(), (self.hidden_size, self.hidden_size))\n",
        "        self.W_hh = self.param('W_hh', nn.initializers.xavier_uniform(), (self.hidden_size, self.hidden_size))\n",
        "def __call__(self, x, hidden_state):\n",
        "        # MODIFIED: Ensure hidden state is properly utilized and returned\n",
        "        new_hidden_state = jnp.tanh(jnp.dot(x, self.W_ih) + jnp.dot(hidden_state, self.W_hh))\n",
        "\n",
        "Error: TypeError: dot_general requires contracting dimensions to\n",
        "have the same shape, got (1,) and (16,).\n",
        "\n",
        "Fix Guide:\n",
        "The error occurs due to a shape mismatch in the matrix multiplication within\n",
        "RNNCell. The input x has shape (batch_size, input_features=1),\n",
        "but W_ih is initialized with shape (hidden_size, hidden_size)=(16, 16),\n",
        "causing a dimension mismatch in the dot product.\n",
        "\n",
        "Correct Code\n",
        "def setup(self):\n",
        "        self.W_ih = self.param('W_ih', nn.initializers.xavier_uniform(),\n",
        "                              (self.input_size, self.hidden_size))\n",
        "        self.W_hh = self.param('W_hh', nn.initializers.xavier_uniform(),\n",
        "                              (self.hidden_size, self.hidden_size))\n",
        "\n",
        "    def __call__(self, x, hidden_state):\n",
        "        new_hidden_state = jnp.tanh(jnp.dot(x, self.W_ih) + jnp.dot(hidden_state, self.W_hh))\n",
        "        return new_hidden_state\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Error Code\n",
        "final_hidden_state, _ = jax.lax.scan(rnn_step, hidden_state, x, length=x.shape[1])\n",
        "\n",
        "Error:\n",
        "ValueError: scan got `length` argument of 10 which disagrees with leading axis sizes [100].\n",
        "\n",
        "Fix Guide\n",
        "The error occurs in jax.lax.scan because the length parameter\n",
        "(set to x.shape[1] = 10) doesn't match the leading axis size of the input x\n",
        "that scan is iterating over. In jax.lax.scan, the x argument is expected to have\n",
        "its first dimension as the sequence length to scan over, but:\n",
        "x has shape (100, 10, 1) where 100 is the batch size and 10 is the sequence length.\n",
        "scan interprets the first dimension (100) as the sequence length,\n",
        "conflicting with length=10. We want to transpose x to (sequence_length, batch_size, features) = (10, 100, 1)\n",
        " before scanning.\n",
        "\n",
        "Correct Code\n",
        "def __call__(self, x):\n",
        "        # Transpose x to (sequence_length, batch_size, input_size)\n",
        "        x = jnp.transpose(x, (1, 0, 2))  # From (100, 10, 1) to (10, 100, 1)\n",
        "        hidden_state = jnp.zeros((x.shape[1], self.hidden_size))  # batch_size = 100\n",
        "\n",
        "        def rnn_step(hidden_state, x_t):\n",
        "            return self.rnn_cell(x_t, hidden_state), None\n",
        "\n",
        "        # Scan over sequence dimension (10)\n",
        "        final_hidden_state, _ = jax.lax.scan(rnn_step, hidden_state, x)\n",
        "        output = self.fc(final_hidden_state)\n",
        "        return output\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Error Code\n",
        "new_hidden_state = jnp.tanh(jnp.dot(x, self.W_ih) + jnp.dot(hidden_state, self.W_hh))\n",
        "        return new_hidden_state\n",
        "\n",
        "  Error:\n",
        "TypeError: add got incompatible shapes for broadcasting: (100, 16), (10, 16).\n",
        "\n",
        "Fix guide\n",
        "x has shape (100, 1),10 is the sequence length and 100 is the batch size.\n",
        "W_ih has shape (1, 16),jnp.dot(x, self.W_ih) has shape (100, 16).\n",
        "hidden_state has shape (100, 16), and W_hh has shape (16, 16),\n",
        "so jnp.dot(hidden_state, self.W_hh) also has shape (100, 16).\n",
        "The final hidden state should reflect the batch size (100) and hidden size (16).\n",
        "\n",
        "Correct Code:\n",
        "def __call__(self, x):\n",
        "    x = jnp.transpose(x, (1, 0, 2))  # From (batch, seq, feat) to (seq, batch, feat)\n",
        "    hidden_state = jnp.zeros((x.shape[1], self.hidden_size))  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "    def rnn_step(hidden_state, x_t):\n",
        "        new_hidden = self.rnn_cell(x_t, hidden_state)\n",
        "        return new_hidden, None  # Return new hidden state as carry\n",
        "\n",
        "    final_hidden_state, _ = jax.lax.scan(rnn_step, hidden_state, x)\n",
        "    output = self.fc(final_hidden_state)\n",
        "    return output\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Error Code\n",
        "def __call__(self, x):\n",
        "    x = jnp.transpose(x, (1, 0, 2))  # From (batch, seq, feat) to (seq, batch, feat)\n",
        "    hidden_state = jnp.zeros((x.shape[1], self.hidden_size))  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "    def rnn_step(hidden_state, x_t):\n",
        "        new_hidden = self.rnn_cell(x_t, hidden_state)\n",
        "        return new_hidden, None  # Return new hidden state as carry\n",
        "\n",
        "    final_hidden_state, _ = jax.lax.scan(rnn_step, hidden_state, x)\n",
        "    output = self.fc(final_hidden_state)\n",
        "    return output\n",
        "\n",
        "Error:\n",
        "\n",
        "UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[1,16] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\n",
        "JAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\n",
        "The function being traced when the value leaked was rnn_step at <ipython-input-9-8a3c00ebe7fa>:21 traced for scan.\n",
        "------------------------------\n",
        "The leaked intermediate value was created on line /usr/local/lib/python3.11/dist-packages/flax/core/scope.py:968:14 (Scope.param).\n",
        "------------------------------\n",
        "When the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n",
        "------------------------------\n",
        "Fix guide:\n",
        "The error occurs because JAX's transformations (like scan) expect pure functions,\n",
        "but the rnn_cell call within rnn_step involves Flax's parameter initialization,\n",
        "which has side effects (accessing self.W_ih and self.W_hh).\n",
        "When traced by JAX, this creates an unexpected tracer leak.\n",
        "To fix this, we need to explicitly pass the parameters to rnn_step and avoid relying on self.rnn_cellâ€™s implicit state during the scan\n",
        "\n",
        "correct code:\n",
        "def __call__(self, x, rng=None):  # Added rng for initialization if needed\n",
        "    x = jnp.transpose(x, (1, 0, 2))  # From (batch, seq, feat) to (seq, batch, feat)\n",
        "    hidden_state = jnp.zeros((x.shape[1], self.hidden_size))  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "    def rnn_step(hidden_state, x_t, params):  # Added params argument\n",
        "        W_ih = params['rnn_cell']['W_ih']\n",
        "        W_hh = params['rnn_cell']['W_hh']\n",
        "        new_hidden = jnp.tanh(jnp.dot(x_t, W_ih) + jnp.dot(hidden_state, W_hh))\n",
        "        return new_hidden, None\n",
        "\n",
        "    # Pass params explicitly (assuming params is available from apply)\n",
        "    final_hidden_state, _ = jax.lax.scan(\n",
        "        lambda hs, xt: rnn_step(hs, xt, self.params), hidden_state, x\n",
        "    )\n",
        "    output = self.fc(final_hidden_state)\n",
        "    return output\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Error\n",
        "UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[1,16] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\n",
        "JAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\n",
        "The function being traced when the value leaked was <lambda> at <ipython-input-28-2fb73f6ef55e>:50 traced for scan.\n",
        "The leaked intermediate value was created on line /usr/local/lib/python3.11/dist-packages/flax/core/scope.py:968:14 (Scope.param).\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VureMYZ1rU7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed Code\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "import numpy as np\n",
        "\n",
        "# RNN Cell Definition\n",
        "class RNNCell(nn.Module):\n",
        "    input_size: int\n",
        "    hidden_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.W_ih = self.param('W_ih', nn.initializers.xavier_uniform(),\n",
        "                              (self.input_size, self.hidden_size))\n",
        "        self.W_hh = self.param('W_hh', nn.initializers.xavier_uniform(),\n",
        "                              (self.hidden_size, self.hidden_size))\n",
        "\n",
        "    def __call__(self, carry, x):\n",
        "        new_carry = jnp.tanh(jnp.dot(x, self.W_ih) + jnp.dot(carry, self.W_hh))\n",
        "        return new_carry, None\n",
        "\n",
        "# RNN Module Definition\n",
        "class RNN(nn.Module):\n",
        "    input_size: int\n",
        "    hidden_size: int\n",
        "    output_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        # Wrap RNNCell with nn.scan for proper parameter handling\n",
        "        self.scanned_rnn_cell = nn.scan(\n",
        "            RNNCell,\n",
        "            variable_broadcast=\"params\",\n",
        "            split_rngs={\"params\": False},\n",
        "            in_axes=0,\n",
        "            out_axes=0\n",
        "        )(input_size=self.input_size, hidden_size=self.hidden_size)\n",
        "        self.fc = nn.Dense(self.output_size)\n",
        "    def __call__(self, x):\n",
        "        # Transpose x from (batch, seq, feat) to (seq, batch, feat)\n",
        "        x = jnp.transpose(x, (1, 0, 2))\n",
        "        batch_size = x.shape[1]\n",
        "        init_carry = jnp.zeros((batch_size, self.hidden_size))\n",
        "        final_carry, _ = self.scanned_rnn_cell(init_carry, x)\n",
        "        output = self.fc(final_carry)\n",
        "        return output\n",
        "    # def __call__(self, x, params=None, rng=None):\n",
        "    #     x = jnp.transpose(x, (1, 0, 2))  # From (batch, seq, feat) to (seq, batch, feat)\n",
        "    #     hidden_state = jnp.zeros((x.shape[1], self.hidden_size))  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "    #     def rnn_step(hidden_state, x_t, cell_params):\n",
        "    #         W_ih = cell_params['W_ih']\n",
        "    #         W_hh = cell_params['W_hh']\n",
        "    #         new_hidden = jnp.tanh(jnp.dot(x_t, W_ih) + jnp.dot(hidden_state, W_hh))\n",
        "    #         return new_hidden, None\n",
        "\n",
        "        # Use params if provided (apply), otherwise call rnn_cell directly (init)\n",
        "        # if params is not None:\n",
        "        #     cell_params = params['rnn_cell']\n",
        "        #     final_hidden_state, _ = jax.lax.scan(\n",
        "        #         lambda hs, xt: rnn_step(hs, xt, cell_params), hidden_state, x\n",
        "        #     )\n",
        "        # else:\n",
        "        #     final_hidden_state, _ = jax.lax.scan(\n",
        "        #         lambda hs, xt: (self.rnn_cell(xt, hs), None), hidden_state, x\n",
        "        #     )\n",
        "        # output = self.fc(final_hidden_state)\n",
        "        # return output\n",
        "\n",
        "# Loss Function\n",
        "def compute_loss(params, model, x, targets):\n",
        "    logits = model.apply(params, x)\n",
        "    return jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=targets))\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Sample data for training\n",
        "    x_train = jnp.array(np.random.rand(100, 10, 1))  # 100 samples, 10 timesteps, 1 feature\n",
        "    y_train = jnp.array(np.random.randint(0, 2, (100, 2)))  # 2 classes, output at last timestep\n",
        "    # y_train = jnp.array(np.eye(2)[np.random.randint(0, 2, 100)])\n",
        "\n",
        "    # Instantiate the RNN model\n",
        "    model = RNN(input_size=1, hidden_size=16, output_size=2)\n",
        "    params = model.init(jax.random.PRNGKey(0), x_train)\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    # Training Loop\n",
        "    epochs = 5\n",
        "    for epoch in range(epochs):\n",
        "        loss, grads = jax.value_and_grad(compute_loss)(params, model, x_train, y_train)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
        "\n",
        "    # Testing on new data\n",
        "    X_test = np.linspace(4 * np.pi, 5 * np.pi, 10).reshape(1, 10, 1)  # 1 sample, 10 timesteps\n",
        "    predictions = model.apply(params, X_test)\n",
        "    print(f\"Predictions for new sequence: {predictions.tolist()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivb9pLPrrYil",
        "outputId": "3fd34b83-01a7-4d57-9fa0-2f6107619b00"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.6701\n",
            "Epoch [2/5], Loss: 0.6684\n",
            "Epoch [3/5], Loss: 0.6669\n",
            "Epoch [4/5], Loss: 0.6655\n",
            "Epoch [5/5], Loss: 0.6643\n",
            "Predictions for new sequence: [[-0.16616860032081604, -0.9254869222640991]]\n"
          ]
        }
      ]
    }
  ]
}