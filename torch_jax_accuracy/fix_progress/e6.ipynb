{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e882a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 6.1875\n",
      "Epoch [20/100], Loss: 5.6829\n",
      "Epoch [30/100], Loss: 5.2245\n",
      "Epoch [40/100], Loss: 4.8079\n",
      "Epoch [50/100], Loss: 4.4293\n",
      "Epoch [60/100], Loss: 4.0854\n",
      "Epoch [70/100], Loss: 3.7728\n",
      "Epoch [80/100], Loss: 3.4888\n",
      "Epoch [90/100], Loss: 3.2307\n",
      "Epoch [100/100], Loss: 2.9962\n"
     ]
    }
   ],
   "source": [
    "#Input\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(100, 1) * 10  # 100 data points between 0 and 10\n",
    "y = 3 * X + 5 + torch.randn(100, 1)  # Linear relationship with noise\n",
    "\n",
    "# Define a simple Linear Regression Model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # Single input and single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"runs/linear_regression\")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log loss to TensorBoard\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "\n",
    "    # Log progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Run TensorBoard using the logs generated\n",
    "# Command to run: tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb423e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 5.4223\n",
      "Epoch [20/100], Loss: 5.0028\n",
      "Epoch [30/100], Loss: 4.6238\n",
      "Epoch [40/100], Loss: 4.2814\n",
      "Epoch [50/100], Loss: 3.9721\n",
      "Epoch [60/100], Loss: 3.6926\n",
      "Epoch [70/100], Loss: 3.4401\n",
      "Epoch [80/100], Loss: 3.2120\n",
      "Epoch [90/100], Loss: 3.0059\n",
      "Epoch [100/100], Loss: 2.8198\n"
     ]
    }
   ],
   "source": [
    "#Strong LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from tensorboardX import SummaryWriter  # pip install tensorboardX\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, subkey = jax.random.split(key)\n",
    "X = jax.random.uniform(subkey, shape=(100, 1)) * 10  # 100 data points between 0 and 10\n",
    "key, subkey = jax.random.split(key)\n",
    "y = 3 * X + 5 + jax.random.normal(subkey, shape=(100, 1))  # Linear relationship with noise\n",
    "\n",
    "def predict(params, x):\n",
    "    # Linear model: y = x*w + b\n",
    "    return jnp.dot(x, params[\"w\"]) + params[\"b\"]\n",
    "\n",
    "def loss_fn(params, X, y):\n",
    "    preds = predict(params, X)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Initialize weight and bias (mimicking default behavior in nn.Linear)\n",
    "key, subkey = jax.random.split(key)\n",
    "params = {\n",
    "    \"w\": 0.1 * jax.random.normal(subkey, shape=(1, 1)),\n",
    "    \"b\": jnp.zeros((1,))\n",
    "}\n",
    "\n",
    "optimizer = optax.sgd(learning_rate=0.01)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/linear_regression\")\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, X, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    # Log loss to TensorBoard\n",
    "    writer.add_scalar(\"Loss/train\", float(loss), epoch)\n",
    "    \n",
    "    # Log progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {float(loss):.4f}\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# To run TensorBoard using the generated logs, execute the following command in your terminal:\n",
    "# tensorboard --logdir=runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d2a0ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like requires ndarray or scalar arguments, got <class '__main__.LinearRegressionModel'> at position 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m train_model(model, inputs, targets)  \u001b[38;5;66;03m# MODIFIED: Clearer function call\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegressionModel(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# MODIFIED: Clearer initialization\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, inputs, targets, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, inputs, targets, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m):\n\u001b[0;32m     33\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate)\n\u001b[1;32m---> 34\u001b[0m     opt_state \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     37\u001b[0m         grads \u001b[38;5;241m=\u001b[39m compute_gradients(model, inputs, targets)  \u001b[38;5;66;03m# MODIFIED: Use optimized gradient computation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optax\\transforms\\_combining.py:64\u001b[0m, in \u001b[0;36mchain.<locals>.init_fn\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_fn\u001b[39m(params):\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_fns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optax\\transforms\\_combining.py:64\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_fn\u001b[39m(params):\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m init_fns)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optax\\_src\\transform.py:278\u001b[0m, in \u001b[0;36mscale_by_adam.<locals>.init_fn\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_fn\u001b[39m(params):\n\u001b[1;32m--> 278\u001b[0m   mu \u001b[38;5;241m=\u001b[39m \u001b[43motu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_zeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu_dtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# First moment\u001b[39;00m\n\u001b[0;32m    279\u001b[0m   nu \u001b[38;5;241m=\u001b[39m otu\u001b[38;5;241m.\u001b[39mtree_zeros_like(params)  \u001b[38;5;66;03m# Second moment\u001b[39;00m\n\u001b[0;32m    280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ScaleByAdamState(count\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mzeros([], jnp\u001b[38;5;241m.\u001b[39mint32), mu\u001b[38;5;241m=\u001b[39mmu, nu\u001b[38;5;241m=\u001b[39mnu)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optax\\tree_utils\\_tree_math.py:247\u001b[0m, in \u001b[0;36mtree_zeros_like\u001b[1;34m(tree, dtype)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_zeros_like\u001b[39m(\n\u001b[0;32m    235\u001b[0m     tree: Any,\n\u001b[0;32m    236\u001b[0m     dtype: Optional[jax\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mDTypeLike] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    237\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    238\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an all-zeros tree with the same structure.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    an all-zeros tree with the same structure as ``tree``.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\tree.py:154\u001b[0m, in \u001b[0;36mmap\u001b[1;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[0;32m    115\u001b[0m         tree: Any,\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;241m*\u001b[39mrest: Any,\n\u001b[0;32m    117\u001b[0m         is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    - :func:`jax.tree.reduce`\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\tree_util.py:343\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[0;32m    341\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m    342\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\tree_util.py:343\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    341\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m    342\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optax\\tree_utils\\_tree_math.py:247\u001b[0m, in \u001b[0;36mtree_zeros_like.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_zeros_like\u001b[39m(\n\u001b[0;32m    235\u001b[0m     tree: Any,\n\u001b[0;32m    236\u001b[0m     dtype: Optional[jax\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mDTypeLike] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    237\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    238\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an all-zeros tree with the same structure.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    an all-zeros tree with the same structure as ``tree``.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m, tree)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3304\u001b[0m, in \u001b[0;36mzeros_like\u001b[1;34m(a, dtype, shape, device)\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[38;5;129m@util\u001b[39m\u001b[38;5;241m.\u001b[39mimplements(np\u001b[38;5;241m.\u001b[39mzeros_like)\n\u001b[0;32m   3299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeros_like\u001b[39m(a: ArrayLike \u001b[38;5;241m|\u001b[39m DuckTypedArray,\n\u001b[0;32m   3300\u001b[0m                dtype: DTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3301\u001b[0m                shape: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   3302\u001b[0m                device: xc\u001b[38;5;241m.\u001b[39mDevice \u001b[38;5;241m|\u001b[39m Sharding \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m   3303\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m)):  \u001b[38;5;66;03m# support duck typing\u001b[39;00m\n\u001b[1;32m-> 3304\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_arraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzeros_like\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3305\u001b[0m   dtypes\u001b[38;5;241m.\u001b[39mcheck_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros_like\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3306\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\numpy\\util.py:335\u001b[0m, in \u001b[0;36mcheck_arraylike\u001b[1;34m(fun_name, emit_warning, stacklevel, *args)\u001b[0m\n\u001b[0;32m    332\u001b[0m   warnings\u001b[38;5;241m.\u001b[39mwarn(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m In a future JAX release this will be an error.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    333\u001b[0m                 category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(fun_name, \u001b[38;5;28mtype\u001b[39m(arg), pos))\n",
      "\u001b[1;31mTypeError\u001b[0m: zeros_like requires ndarray or scalar arguments, got <class '__main__.LinearRegressionModel'> at position 0."
     ]
    }
   ],
   "source": [
    "#Weak LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, vmap\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "\n",
    "# Linear regression model definition\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    input_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.w = self.param('w', nn.initializers.xavier_uniform(), (self.input_dim, 1))\n",
    "        self.b = self.param('b', nn.initializers.xavier_uniform(), (1,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(model, inputs, targets):\n",
    "    predictions = model(inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Jitted gradient computation using vectorization\n",
    "@jit\n",
    "def compute_gradients(model, inputs, targets):\n",
    "    return grad(loss_fn)(model, inputs, targets)  # MODIFIED: Use JAX's vectorized grad function\n",
    "\n",
    "# Training function\n",
    "def train_model(model, inputs, targets, num_epochs=1000, learning_rate=0.01):\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        grads = compute_gradients(model, inputs, targets)  # MODIFIED: Use optimized gradient computation\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        model = model.apply(updates)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            current_loss = loss_fn(model, inputs, targets)\n",
    "            print(f\"Epoch {epoch}, Loss: {current_loss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Generate synthetic data\n",
    "    inputs = jnp.array([[1.0], [2.0], [3.0]])  # Input features\n",
    "    targets = jnp.array([[2.0], [3.0], [4.0]])  # Target output\n",
    "\n",
    "    # Initialize model\n",
    "    model = LinearRegressionModel(input_dim=1)  # MODIFIED: Clearer initialization\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, inputs, targets)  # MODIFIED: Clearer function call\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Entry point of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43483d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nError Code:\\n@jit\\ndef compute_gradients(params, inputs, targets, model):\\n    return grad(loss_fn)(params, inputs, targets, model)\\n    \\n\\nError:\\nCannot interpret value of type <class '__main__.LinearRegressionModel'> as an abstract array; it does not have a dtype attribute\\n\\n\\nFix Guide:\\nThe model parameter needs to be marked as a static parameter\\n\\n\\nCorrect Code:\\n@jit(static_argnums=(3,))\\ndef compute_gradients(params, inputs, targets, model):\\n    return grad(loss_fn)(params, inputs, targets, model)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Error Code:\n",
    "self.b = self.param('b', nn.initializers.xavier_uniform(), (1,))\n",
    "\n",
    "\n",
    "Error:\n",
    "Can't compute input and output sizes of a 1-dimensional weights tensor. Must be at least 2D\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "For bias parameters, zero initialization is usually sufficient. Change the initializer to nn.initializers.zeros\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "self.b = self.param('b', nn.initializers.zeros, (1,))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "trained_model = train_model(model, inputs, targets)\n",
    "\n",
    "\n",
    "Error:\n",
    "train_model() missing 1 required positional argument: 'targets'\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the function call to pass in the correct order of parameters: first pass in the initialized parameter dictionary params, then pass in the model model, then inputs and targets\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "trained_params = train_model(params, model, inputs, targets)\n",
    "final_predictions = model.apply(trained_params, inputs)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "model = LinearRegressionModel(input_dim=1)\n",
    "\n",
    "\n",
    "Error:\n",
    "The model parameters need to be initialized by calling model.init(rng, inputs)\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Call model.init with a random key and input example to get the parameter dictionary, and then use the parameters in subsequent training\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "model = LinearRegressionModel(input_dim=1)\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = model.init(key, inputs)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def loss_fn(model, inputs, targets):\n",
    "    predictions = model(inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "    \n",
    "\n",
    "Error:\n",
    "Directly calling model(inputs) cannot pass in parameters\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the loss function so that its first parameter is a parameter dictionary and pass in the model object to call the apply method\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def loss_fn(params, inputs, targets, model):\n",
    "    predictions = model.apply(params, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "@jit\n",
    "def compute_gradients(model, inputs, targets):\n",
    "    return grad(loss_fn)(model, inputs, targets)\n",
    "    \n",
    "\n",
    "Error:\n",
    "The loss function passes in the model instance instead of the parameters\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the function parameters so that the first parameter is a parameter dictionary and pass in the model object\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@jit\n",
    "def compute_gradients(params, inputs, targets, model):\n",
    "    return grad(loss_fn)(params, inputs, targets, model)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "updates, opt_state = optimizer.update(grads, opt_state)\n",
    "model = model.apply(updates)\n",
    "\n",
    "\n",
    "Error:\n",
    "In Flax + Optax, update parameters using optax.apply_updates(params, updates) instead of calling model.apply\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Assign the updated parameters to params\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "updates, opt_state = optimizer.update(grads, opt_state)\n",
    "params = optax.apply_updates(params, updates)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def train_model(model, inputs, targets, num_epochs=1000, learning_rate=0.01):\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(model)\n",
    "    ...\n",
    "    return model\n",
    "    \n",
    "\n",
    "Error:\n",
    "During training, the parameter dictionary should be passed in and updated instead of the model instance\n",
    "Parameters should be passed in when initializing the optimizer\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the parameters of the training function so that it receives a parameter dictionary and returns the updated parameters on return\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def train_model(params, model, inputs, targets, num_epochs=100, learning_rate=0.01):\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        grads = compute_gradients(params, inputs, targets, model)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            current_loss = loss_fn(params, inputs, targets, model)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "            writer.add_scalar(\"Loss/train\", current_loss, epoch)\n",
    "\n",
    "    return params\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "import tensorboard\n",
    "\n",
    "\n",
    "Error:\n",
    "SummaryWriter is used in the PyTorch code to record the training process, while the tensorboard module is imported in the JAX code but not actually used.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use tensorboardX to create a SummaryWriter and log scalars during training\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/linear_regression\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "inputs = jnp.array([[1.0], [2.0], [3.0]])\n",
    "targets = jnp.array([[2.0], [3.0], [4.0]])\n",
    "\n",
    "\n",
    "Error:\n",
    "The original PyTorch code generates 100 random data in the interval [0,10] and adds noise. Here we use only 3 data points.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Generate 100 random data points using jax.random and add noise to construct the target value\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "inputs = jax.random.uniform(subkey1, (100, 1), minval=0.0, maxval=10.0)\n",
    "noise = jax.random.normal(subkey2, (100, 1))\n",
    "targets = 3 * inputs + 5 + noise\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "@jit\n",
    "def compute_gradients(params, inputs, targets, model):\n",
    "    return grad(loss_fn)(params, inputs, targets, model)\n",
    "    \n",
    "\n",
    "Error:\n",
    "Cannot interpret value of type <class '__main__.LinearRegressionModel'> as an abstract array; it does not have a dtype attribute\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "The model parameter needs to be marked as a static parameter\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "@jit(static_argnums=(3,))\n",
    "def compute_gradients(params, inputs, targets, model):\n",
    "    return grad(loss_fn)(params, inputs, targets, model)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "@jit(static_argnums=(3,))\n",
    "def compute_gradients(params, inputs, targets, model):\n",
    "    return grad(loss_fn)(params, inputs, targets, model)\n",
    "    \n",
    "\n",
    "Error:\n",
    "jit() missing 1 required positional argument: 'fun'\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "First define the function compute_gradients\n",
    "Use jit to explicitly convert the function and specify the static parameter static_argnums=(3,)\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def compute_gradients(params, inputs, targets, model):\n",
    "    return grad(loss_fn)(params, inputs, targets, model)\n",
    "compute_gradients = jit(compute_gradients, static_argnums=(3,))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "\n",
    "Error:\n",
    "The Adam optimizer is used here, while the original PyTorch code uses SGD (stochastic gradient descent)\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use optax.sgd(learning_rate) instead of optax.adam(learning_rate)\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "optimizer = optax.sgd(learning_rate)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "if epoch % 10 == 0:\n",
    "    current_loss = loss_fn(params, inputs, targets, model)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", current_loss, epoch)\n",
    "    \n",
    "\n",
    "Error:\n",
    "The original PyTorch code prints when (epoch + 1) % 10 == 0, that is, it prints at the 10th, 20th, ... epochs.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the condition to if (epoch + 1) % 10 == 0:\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "if (epoch + 1) % 10 == 0:\n",
    "    current_loss = loss_fn(params, inputs, targets, model)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", current_loss, epoch)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "writer = SummaryWriter(log_dir=\"runs/linear_regression\")\n",
    "...\n",
    "return params\n",
    "\n",
    "Error:\n",
    "Failure to call writer.close() may result in the log file not being written to disk correctly or resources not being released, which may affect the log viewing of TensorBoard.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Call writer.close() after the training loop ends and before returning the arguments\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "writer = SummaryWriter(log_dir=\"runs/linear_regression\")\n",
    "...\n",
    "writer.close()\n",
    "return params\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8aa88e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 4.6780\n",
      "Epoch [20/100], Loss: 4.3839\n",
      "Epoch [30/100], Loss: 4.1136\n",
      "Epoch [40/100], Loss: 3.8654\n",
      "Epoch [50/100], Loss: 3.6373\n",
      "Epoch [60/100], Loss: 3.4277\n",
      "Epoch [70/100], Loss: 3.2352\n",
      "Epoch [80/100], Loss: 3.0583\n",
      "Epoch [90/100], Loss: 2.8958\n",
      "Epoch [100/100], Loss: 2.7465\n"
     ]
    }
   ],
   "source": [
    "#Fixed Code\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, vmap\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Linear regression model definition\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    input_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.w = self.param('w', nn.initializers.xavier_uniform(), (self.input_dim, 1))\n",
    "        self.b = self.param('b', nn.initializers.zeros, (1,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(params, inputs, targets, model):\n",
    "    predictions = model.apply(params, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Jitted gradient computation using vectorization\n",
    "def compute_gradients(params, inputs, targets, model):\n",
    "    return grad(loss_fn)(params, inputs, targets, model)\n",
    "compute_gradients = jit(compute_gradients, static_argnums=(3,))\n",
    "\n",
    "# Training function\n",
    "def train_model(params, model, inputs, targets, num_epochs=100, learning_rate=0.01):\n",
    "    optimizer = optax.sgd(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "    writer = SummaryWriter(log_dir=\"runs/linear_regression\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        grads = compute_gradients(params, inputs, targets, model)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            current_loss = loss_fn(params, inputs, targets, model)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "            writer.add_scalar(\"Loss/train\", current_loss, epoch)\n",
    "    \n",
    "    writer.close()\n",
    "    return params\n",
    "\n",
    "def main():\n",
    "    # Generate synthetic data\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "    inputs = jax.random.uniform(subkey1, (100, 1), minval=0.0, maxval=10.0)\n",
    "    noise = jax.random.normal(subkey2, (100, 1))\n",
    "    targets = 3 * inputs + 5 + noise\n",
    "\n",
    "    # Initialize model\n",
    "    model = LinearRegressionModel(input_dim=1)  # MODIFIED: Clearer initialization\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    params = model.init(key, inputs)\n",
    "\n",
    "    # Train the model\n",
    "    trained_params = train_model(params, model, inputs, targets)\n",
    "    final_predictions = model.apply(trained_params, inputs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Entry point of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fe54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
