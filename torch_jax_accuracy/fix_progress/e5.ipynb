{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac481921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 2.3831\n",
      "Epoch [200/1000], Loss: 0.8492\n",
      "Epoch [300/1000], Loss: 0.7732\n",
      "Epoch [400/1000], Loss: 0.7514\n",
      "Epoch [500/1000], Loss: 0.7371\n",
      "Epoch [600/1000], Loss: 0.7291\n",
      "Epoch [700/1000], Loss: 0.7251\n",
      "Epoch [800/1000], Loss: 0.7233\n",
      "Epoch [900/1000], Loss: 0.7225\n",
      "Epoch [1000/1000], Loss: 0.7221\n",
      "Predictions for [[4.0, 3.0], [7.0, 8.0]]: [[9.915834426879883], [23.08173179626465]]\n"
     ]
    }
   ],
   "source": [
    "#Input\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(100, 2) * 10  # 100 data points with 2 features\n",
    "y = (X[:, 0] + X[:, 1] * 2).unsqueeze(1) + torch.randn(100, 1)  # Non-linear relationship with noise\n",
    "\n",
    "# Define the Deep Neural Network Model\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()       # Activation function\n",
    "        self.fc2 = nn.Linear(10, 1) # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = DNNModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Testing on new data\n",
    "X_test = torch.tensor([[4.0, 3.0], [7.0, 8.0]])\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b4231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.3837\n",
      "Epoch [200/1000], Loss: 1.1774\n",
      "Epoch [300/1000], Loss: 1.0918\n",
      "Epoch [400/1000], Loss: 1.0450\n",
      "Epoch [500/1000], Loss: 1.0342\n",
      "Epoch [600/1000], Loss: 1.0314\n",
      "Epoch [700/1000], Loss: 1.0294\n",
      "Epoch [800/1000], Loss: 1.0276\n",
      "Epoch [900/1000], Loss: 1.0258\n",
      "Epoch [1000/1000], Loss: 1.0239\n",
      "Predictions for [[4.0, 3.0], [7.0, 8.0]]: [[10.156254768371582], [23.063282012939453]]\n"
     ]
    }
   ],
   "source": [
    "#Strong LLM\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a PRNG key\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Generate synthetic data: 100 data points with 2 features in [0, 10)\n",
    "key, subkey = jax.random.split(key)\n",
    "X = jax.random.uniform(subkey, shape=(100, 2)) * 10\n",
    "\n",
    "# Compute target: y = (X[:, 0] + 2 * X[:, 1]) + noise, with shape (100, 1)\n",
    "key, subkey = jax.random.split(key)\n",
    "noise = jax.random.normal(subkey, shape=(100, 1))\n",
    "# Compute (X[:,0] + 2*X[:,1]) and expand dims to (100, 1)\n",
    "y = (X[:, 0] + 2 * X[:, 1]).reshape(-1, 1) + noise\n",
    "\n",
    "\n",
    "# We define a two-layer network:\n",
    "#  - fc1: linear transformation from 2 -> 10\n",
    "#  - ReLU activation\n",
    "#  - fc2: linear transformation from 10 -> 1\n",
    "def predict(params, x):\n",
    "    hidden = jnp.dot(x, params['W1']) + params['b1']  # (batch, 10)\n",
    "    hidden = jax.nn.relu(hidden)\n",
    "    output = jnp.dot(hidden, params['W2']) + params['b2']  # (batch, 1)\n",
    "    return output\n",
    "\n",
    "\n",
    "# For simplicity, we initialize weights using a uniform distribution.\n",
    "# You could also use other initializers (e.g., Xavier/Glorot uniform).\n",
    "def init_params(key):\n",
    "    keys = jax.random.split(key, 4)\n",
    "    # fc1: shape (2, 10)\n",
    "    W1 = jax.random.uniform(keys[0], shape=(2, 10), minval=-1.0, maxval=1.0)\n",
    "    b1 = jax.random.uniform(keys[1], shape=(10,), minval=-1.0, maxval=1.0)\n",
    "    # fc2: shape (10, 1)\n",
    "    W2 = jax.random.uniform(keys[2], shape=(10, 1), minval=-1.0, maxval=1.0)\n",
    "    b2 = jax.random.uniform(keys[3], shape=(1,), minval=-1.0, maxval=1.0)\n",
    "    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "params = init_params(key)\n",
    "\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    preds = predict(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "\n",
    "# Use the Adam optimizer from optax\n",
    "optimizer = optax.adam(learning_rate=0.01)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# Define a function to compute loss and gradients\n",
    "loss_and_grad = jax.value_and_grad(loss_fn)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss, grads = loss_and_grad(params, X, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    # Log progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "X_test = jnp.array([[4.0, 3.0], [7.0, 8.0]])\n",
    "predictions = predict(params, X_test)\n",
    "print(f\"Predictions for {np.array(X_test).tolist()}: {np.array(predictions).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567547ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (10,) and (5,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Displaying loss\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m targets \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m5\u001b[39m,))  \u001b[38;5;66;03m# Example target data\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Using loss function\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mloss_fn\u001b[1;34m(params, inputs, targets)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params: Any, inputs: jnp\u001b[38;5;241m.\u001b[39mndarray, targets: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the loss.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Simulate predictions\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean((predictions \u001b[38;5;241m-\u001b[39m targets) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:4573\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m   4571\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4572\u001b[0m     contract_dims \u001b[38;5;241m=\u001b[39m ((a_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (b_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m,))\n\u001b[1;32m-> 4573\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontract_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4574\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[1;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\jax\\_src\\lax\\lax.py:2705\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[1;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[0;32m   2703\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2704\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2705\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[0;32m   2707\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[1;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (10,) and (5,)."
     ]
    }
   ],
   "source": [
    "#Weak LLM\n",
    "import jax.numpy as jnp  # MODIFIED: Ensured consistent import for jax.numpy as jnp\n",
    "from jax import random  # MODIFIED: Added necessary import for random functionality\n",
    "from typing import Any, Tuple\n",
    "\n",
    "def init_params(key: Any, input_shape: Tuple[int, ...]) -> Any:\n",
    "    \"\"\"Initialize parameters for the model.\"\"\"\n",
    "    param_shape = (input_shape[0], 1)  # Example shape for parameters\n",
    "    return random.normal(key, param_shape)  # Use explicit PRNG key\n",
    "\n",
    "def loss_fn(params: Any, inputs: jnp.ndarray, targets: jnp.ndarray) -> float:\n",
    "    \"\"\"Calculate the loss.\"\"\"\n",
    "    predictions = jnp.dot(inputs, params)  # Simulate predictions\n",
    "    return jnp.mean((predictions - targets) ** 2)  # Mean Squared Error\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main entry point for the program.\"\"\"\n",
    "    key = random.PRNGKey(0)  # Create an explicit PRNG key\n",
    "    input_shape = (5, 10)  # Define input shape\n",
    "    params = init_params(key, input_shape)  # Initialize parameters\n",
    "    inputs = jnp.ones((5, 10))  # Example input data\n",
    "    targets = jnp.ones((5,))  # Example target data\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_value = loss_fn(params, inputs, targets)  # Using loss function\n",
    "    print(f\"Loss: {loss_value}\")  # Displaying loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Entry point for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de99dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error Code:\n",
    "def init_params(key: Any, input_shape: Tuple[int, ...]) -> Any:\n",
    "    param_shape = (input_shape[0], 1)  # Example shape for parameters\n",
    "    return random.normal(key, param_shape)\n",
    "\n",
    "\n",
    "Error:\n",
    "dot_general requires contracting dimensions to have the same shape, got (10,) and (5,)\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Modify the init_params function so that the shape of the parameters matches the input data. \n",
    "The parameters should be initialized to (input_shape[1], 1)\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def init_params(key: Any, input_shape: Tuple[int, ...]) -> Any:\n",
    "    Initialize parameters for the model.\n",
    "    param_shape = (input_shape[1], 1)\n",
    "    return random.normal(key, param_shape)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def init_params(key: Any, input_shape: Tuple[int, ...]) -> Any:\n",
    "    param_shape = (input_shape[1], 1)\n",
    "    return random.normal(key, param_shape)\n",
    "    \n",
    "\n",
    "Error:\n",
    "The parameter initialization is incomplete. \n",
    "The four parameters (W1, b1, W2, b2) that need to be initialized in the two-layer network in the original code are inconsistent.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Remove the redundant input_shape parameter, use random.split to divide the 4 sub-keys, and then initialize the weights and biases of fc1 and the weights and biases of fc2 respectively.\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def init_params(key: Any) -> Any:\n",
    "    keys = random.split(key, 4)\n",
    "    W1 = random.uniform(keys[0], shape=(2, 10), minval=-1.0, maxval=1.0)\n",
    "    b1 = random.uniform(keys[1], shape=(10,), minval=-1.0, maxval=1.0)\n",
    "    W2 = random.uniform(keys[2], shape=(10, 1), minval=-1.0, maxval=1.0)\n",
    "    b2 = random.uniform(keys[3], shape=(1,), minval=-1.0, maxval=1.0)\n",
    "    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "params = init_params(key, input_shape)\n",
    "\n",
    "Error:\n",
    "The function init_params is defined to accept only one parameter (PRNG key)\n",
    "\n",
    "Fix Guide:\n",
    "Remove input_shape parameter from init_params function\n",
    "\n",
    "Correct Code:\n",
    "params = init_params(key)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "predictions = jnp.dot(inputs, params)\n",
    "\n",
    "Error:\n",
    "You cannot directly perform a dot product operation on params. \n",
    "The parameters are dictionaries, and the two-layer network needs to go through the hidden layer before calculating the output.\n",
    "\n",
    "Fix Guide:\n",
    "Define a predict function, first calculate the first layer linear transformation and use ReLU activation, then calculate the second layer linear transformation to get the final output\n",
    "\n",
    "Correct Code:\n",
    "def predict(params: Any, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    hidden = jnp.dot(x, params['W1']) + params['b1']\n",
    "    hidden = jax.nn.relu(hidden)\n",
    "    output = jnp.dot(hidden, params['W2']) + params['b2']\n",
    "    return output\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def loss_fn(params: Any, inputs: jnp.ndarray, targets: jnp.ndarray) -> float:\n",
    "    predictions = jnp.dot(inputs, params)  # Simulate predictions\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "\n",
    "Error:\n",
    "The params dictionary is incorrectly matrix multiplied directly, the newly defined predict function should be called\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Change the line that calculates the predicted value to call the predict function\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def loss_fn(params: Any, inputs: jnp.ndarray, targets: jnp.ndarray) -> float:\n",
    "    predictions = predict(params, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "input_shape = (5, 10)  # Define input shape\n",
    "inputs = jnp.ones((5, 10))  # Example input data\n",
    "targets = jnp.ones((5,))  # Example target data\n",
    "\n",
    "\n",
    "Error:\n",
    "The model expects 2 features as input, but 10 is used here\n",
    "The shape of the target data is (5,), but the predicted output shape is (5, 1), which indicates a dimension mismatch.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Reshape the input data to have 2 features and expand the target data into a 2D array\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "inputs = jnp.ones((5, 2))  # Example input data with 2 features\n",
    "targets = jnp.ones((5, 1))  # Example target data with shape (batch, 1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "hidden = jax.nn.relu(hidden)\n",
    "\n",
    "\n",
    "Error:\n",
    "jax.nn.relu is used, but the entire jax module is not imported in the file, resulting in jax being undefined\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add import jax at the beginning of the file\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "import jax\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "inputs = jnp.ones((5, 2))  # Example input data with 2 features\n",
    "targets = jnp.ones((5, 1))  # Example target data with shape (batch, 1)\n",
    "\n",
    "\n",
    "Error:\n",
    "Does not meet the synthetic data requirement of randomly generating 100 data points and adding noise in the original pytorch code\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Generate 100 2D data using random numbers and calculate the target value as X[:,0] + X[:,1] * 2 plus noise\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key = random.PRNGKey(42)\n",
    "key, subkey = random.split(key)\n",
    "X = random.uniform(key, shape=(100, 2), minval=0.0, maxval=1.0) * 10\n",
    "key, subkey = random.split(subkey)\n",
    "noise = random.normal(subkey, shape=(100, 1))\n",
    "y = (X[:, 0:1] + X[:, 1:2] * 2) + noise\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "# Calculate loss\n",
    "loss_value = loss_fn(params, inputs, targets)  # Using loss function\n",
    "print(f\"Loss: {loss_value}\")  # Displaying loss\n",
    "\n",
    "\n",
    "Error:\n",
    "There is no backpropagation (using jax.grad to calculate gradients) and parameter update steps in the jax code\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Add a training loop, define an update function, calculate the gradient through jax.grad(loss_fn)\n",
    "Use simple gradient descent to update the parameters, and print the current loss every certain epoch.\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "def update(params, inputs, targets, lr):\n",
    "    grads = jax.grad(loss_fn)(params, inputs, targets)\n",
    "    new_params = {k: params[k] - lr * grads[k] for k in params}\n",
    "    return new_params\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "lr = 0.01\n",
    "for epoch in range(epochs):\n",
    "    params = update(params, X, y, lr)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        current_loss = loss_fn(params, X, y)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "epochs = 1000\n",
    "    lr = 0.01\n",
    "    for epoch in range(epochs):\n",
    "        params = update(params, X, y, lr)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            current_loss = loss_fn(params, X, y)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "\n",
    "Error:\n",
    "The code does not include the part that makes predictions on the test data\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "After training is complete, add prediction code for test data and print the prediction results\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "epochs = 1000\n",
    "    lr = 0.01\n",
    "    for epoch in range(epochs):\n",
    "        params = update(params, X, y, lr)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            current_loss = loss_fn(params, X, y)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "X_test = jnp.array([[4.0, 3.0], [7.0, 8.0]])\n",
    "predictions = predict(params, X_test)\n",
    "print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "input_shape = (5, 10)  # Define input shape\n",
    "\n",
    "\n",
    "Error:\n",
    "The variable is not used and does not match the shape of the actual data\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Remove the useless input_shape variable or replace it with correct synthetic data generation code\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "# input_shape = (5, 10)  # Define input shape\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "key = random.PRNGKey(0)  # Create an explicit PRNG key\n",
    "# input_shape = (5, 10)  # Define input shape\n",
    "params = init_params(key)\n",
    "key = random.PRNGKey(42)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "\n",
    "Error:\n",
    "Different random seeds are used for model parameter initialization and data generation\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use the same random seed and split it appropriately to ensure that parameters and data generation are based on the same initial seed.\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key = random.PRNGKey(42) \n",
    "key, subkey = random.split(key)\n",
    "params = init_params(subkey)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "key, subkey = random.split(key)\n",
    "X = random.uniform(key, shape=(100, 2), minval=0.0, maxval=1.0) * 10\n",
    "key, subkey = random.split(subkey)\n",
    "noise = random.normal(subkey, shape=(100, 1))\n",
    "y = (X[:, 0:1] + X[:, 1:2] * 2) + noise\n",
    "\n",
    "\n",
    "Error:\n",
    "Reusing variable names when splitting keys can easily cause confusion, and using the split key and subkey at the same time is not clear enough\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Split the key continuously when generating data, and explicitly use the split key to generate each part of the data\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "key = random.PRNGKey(42)\n",
    "key, subkey_params = random.split(key)\n",
    "params = init_params(subkey_params)\n",
    "\n",
    "key, subkey_X = random.split(key)\n",
    "X = random.uniform(subkey_X, shape=(100, 2), minval=0.0, maxval=1.0) * 10\n",
    "key, subkey_noise = random.split(key)\n",
    "noise = random.normal(subkey_noise, shape=(100, 1))\n",
    "y = (X[:, 0:1] + X[:, 1:2] * 2) + noise\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Error Code:\n",
    "def update(params, inputs, targets, lr):\n",
    "    grads = jax.grad(loss_fn)(params, inputs, targets)\n",
    "    new_params = {k: params[k] - lr * grads[k] for k in params}\n",
    "    return new_params\n",
    "\n",
    "Error:\n",
    "The original PyTorch code uses the Adam optimizer, while the JAX code here only implements a simple gradient descent update.\n",
    "\n",
    "\n",
    "Fix Guide:\n",
    "Use the optax library commonly used in the JAX ecosystem to implement the Adam optimizer\n",
    "\n",
    "\n",
    "Correct Code:\n",
    "import optax\n",
    "\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        grads = jax.grad(loss_fn)(params, X, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            current_loss = loss_fn(params, X, y)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a26087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.2904\n",
      "Epoch [200/1000], Loss: 1.0354\n",
      "Epoch [300/1000], Loss: 0.9703\n",
      "Epoch [400/1000], Loss: 0.9253\n",
      "Epoch [500/1000], Loss: 0.8995\n",
      "Epoch [600/1000], Loss: 0.8899\n",
      "Epoch [700/1000], Loss: 0.8866\n",
      "Epoch [800/1000], Loss: 0.8850\n",
      "Epoch [900/1000], Loss: 0.8842\n",
      "Epoch [1000/1000], Loss: 0.8836\n",
      "Predictions for [[4.0, 3.0], [7.0, 8.0]]: [[9.865676879882812], [23.006179809570312]]\n"
     ]
    }
   ],
   "source": [
    "#Fixed Code\n",
    "import jax\n",
    "import jax.numpy as jnp  # MODIFIED: Ensured consistent import for jax.numpy as jnp\n",
    "from jax import random  # MODIFIED: Added necessary import for random functionality\n",
    "from typing import Any, Tuple\n",
    "\n",
    "def init_params(key: Any) -> Any:\n",
    "    keys = random.split(key, 4)\n",
    "    W1 = random.uniform(keys[0], shape=(2, 10), minval=-1.0, maxval=1.0)\n",
    "    b1 = random.uniform(keys[1], shape=(10,), minval=-1.0, maxval=1.0)\n",
    "    W2 = random.uniform(keys[2], shape=(10, 1), minval=-1.0, maxval=1.0)\n",
    "    b2 = random.uniform(keys[3], shape=(1,), minval=-1.0, maxval=1.0)\n",
    "    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "def predict(params: Any, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    hidden = jnp.dot(x, params['W1']) + params['b1']\n",
    "    hidden = jax.nn.relu(hidden)\n",
    "    output = jnp.dot(hidden, params['W2']) + params['b2']\n",
    "    return output\n",
    "\n",
    "def loss_fn(params: Any, inputs: jnp.ndarray, targets: jnp.ndarray) -> float:\n",
    "    predictions = predict(params, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "def update(params, inputs, targets, lr):\n",
    "    grads = jax.grad(loss_fn)(params, inputs, targets)\n",
    "    new_params = {k: params[k] - lr * grads[k] for k in params}\n",
    "    return new_params\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main entry point for the program.\"\"\"\n",
    "    key = random.PRNGKey(42)\n",
    "    key, subkey_params = random.split(key)\n",
    "    params = init_params(subkey_params)\n",
    "\n",
    "    key, subkey_X = random.split(key)\n",
    "    X = random.uniform(subkey_X, shape=(100, 2), minval=0.0, maxval=1.0) * 10\n",
    "    key, subkey_noise = random.split(key)\n",
    "    noise = random.normal(subkey_noise, shape=(100, 1))\n",
    "    y = (X[:, 0:1] + X[:, 1:2] * 2) + noise\n",
    "\n",
    "    epochs = 1000\n",
    "    lr = 0.01\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        grads = jax.grad(loss_fn)(params, X, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            current_loss = loss_fn(params, X, y)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}\")\n",
    "    \n",
    "    X_test = jnp.array([[4.0, 3.0], [7.0, 8.0]])\n",
    "    predictions = predict(params, X_test)\n",
    "    print(f\"Predictions for {X_test.tolist()}: {predictions.tolist()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Entry point for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad964ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
