## -----------------
Example_id = m1 
## -----------------

import jax
import jax.numpy as jnp
from jax import random
import optax
import matplotlib.pyplot as plt


# Generate synthetic data
def create_in_out_sequences(data, seq_length):
    in_seq = []
    out_seq = []
    for i in range(len(data) - seq_length):
        in_seq.append(data[i:i + seq_length])
        out_seq.append(data[i + seq_length])
    return jnp.stack(in_seq), jnp.stack(out_seq)


def generate_data(num_samples=100):
    key = random.PRNGKey(0)
    X = jnp.linspace(0, 4 * 3.14159, num_samples).reshape(-1, 1)
    y = jnp.sin(X)
    return X, y
X_seq, y_seq = create_in_out_sequences(generate_data()[1], 10)

# Define a simple model using JAX
class LSTMModel:
    def __init__(self, input_dim, hidden_units):
        self.hidden_units = hidden_units
        self.Wxi = jax.random.normal(random.PRNGKey(0), (input_dim, hidden_units))
        self.Whi = jax.random.normal(random.PRNGKey(1), (hidden_units, hidden_units))
        self.bi = jnp.zeros(hidden_units)
        self.input_dim = input_dim
        self.hidden_units = hidden_units
        
        # Initialize other weights similarly
    def init_params(self):
        key = random.PRNGKey(0)
        keys = random.split(key, 8)  # For all weights and biases
        return {
            'Wxi': random.normal(keys[0], (self.input_dim, self.hidden_units)),
            'Whi': random.normal(keys[1], (self.hidden_units, self.hidden_units)),
            'bi': jnp.zeros(self.hidden_units),
            'Wxf': random.normal(keys[2], (self.input_dim, self.hidden_units)),
            'Whf': random.normal(keys[3], (self.hidden_units, self.hidden_units)),
            'bf': jnp.zeros(self.hidden_units),
            'Wxo': random.normal(keys[4], (self.input_dim, self.hidden_units)),
            'Who': random.normal(keys[5], (self.hidden_units, self.hidden_units)),
            'bo': jnp.zeros(self.hidden_units),
            'Wxc': random.normal(keys[6], (self.input_dim, self.hidden_units)),
            'Whc': random.normal(keys[7], (self.hidden_units, self.hidden_units)),
            'bc': jnp.zeros(self.hidden_units),
        }
    def forward(self, inputs):
        batch_size, seq_len, _ = inputs.shape
        H = jnp.zeros((batch_size, self.hidden_units))
        C = jnp.zeros((batch_size, self.hidden_units))

        all_hidden_states = []
        for t in range(seq_len):
            X_t = inputs[:, t, :]
            I_t = jax.nn.sigmoid(jnp.dot(X_t, self.Wxi) + jnp.dot(H, self.Whi) + self.bi)
            # Add other gate computations (F_t, O_t, C_tilde)
            F_t = jax.nn.sigmoid(jnp.dot(X_t, params['Wxf']) + jnp.dot(H, params['Whf']) + params['bf'])
            # Output gate
            O_t = jax.nn.sigmoid(jnp.dot(X_t, params['Wxo']) + jnp.dot(H, params['Who']) + params['bo'])
            # Cell candidate
            C_tilde = jnp.tanh(jnp.dot(X_t, params['Wxc']) + jnp.dot(H, params['Whc']) + params['bc'])
            C = F_t * C + I_t * C_tilde
            H = O_t * jnp.tanh(C)
            all_hidden_states.append(H)

        return jnp.stack(all_hidden_states, axis=1)

# Define the LSTM model and other components in JAX
def loss_fn(params, inputs, targets):
    predictions = model.forward(inputs)
    return jnp.mean((predictions - targets) ** 2)


def train_step(params, X, y):
    grads = jax.grad(loss_fn)(params, X, y)
    new_params = {k: params[k] - 0.01 * grads[k] for k in params}
    return new_params


    # Training loop with output printing
def train_model(params, X, y, num_epochs=100):
    for epoch in range(num_epochs):
        params = train_step(params, X, y)
        loss = loss_fn(params, X, y)
        if epoch % 10 == 0:  # Print every 10 epochs
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
    return params

# Initialize and train model
model = LSTMModel(1, 50)
params = model.init_params()

# Reshape input for LSTM (batch_size, seq_len, input_dim)
X_seq = X_seq.reshape(X_seq.shape[0], X_seq.shape[1], 1)
y_seq = y_seq.reshape(y_seq.shape[0], 1, 1)

# Train and print outputs
params = train_model(params, X_seq, y_seq)

## -----------------
Example_id = m3
## -----------------

## m3 - Set D fixed code
import jax
import jax.numpy as jnp
import optax
from jax import random
from flax import linen as nn

# Define the CNN model using flax.linen
class VanillaCNNModel(nn.Module):
    def setup(self):
        self.conv1 = nn.Conv(32, kernel_size=(3, 3), strides=(1, 1), padding='SAME')
        self.conv2 = nn.Conv(64, kernel_size=(3, 3), strides=(1, 1), padding='SAME')
        self.pool = nn.max_pool
        self.fc1 = nn.Dense(128)
        self.fc2 = nn.Dense(10)
        self.relu = nn.relu

    def __call__(self, x):
        x = self.relu(self.conv1(x))
        x = self.pool(self.relu(self.conv2(x)), window_shape=(2, 2), strides=(2, 2))
        x = x.reshape((x.shape[0], -1))  # Flatten
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Loss function (Huber Loss)
def huber_loss(params, X, y, delta=1.0):
    preds = model(params, X)
    error = jnp.abs(preds - y)
    loss = jnp.where(error <= delta, 0.5 * error**2, delta * (error - 0.5 * delta))
    return jnp.mean(loss)

# Training step
def train_step(params, X, y, optimizer):
    loss, grads = jax.value_and_grad(huber_loss)(params, X, y)
    new_params = optimizer.apply_updates(params, grads)
    return new_params, loss

# Model training loop
def train_model(model, train_loader, epochs=10):
    optimizer = optax.adam(learning_rate=0.001)
    params = model.init(rng, X)  # Initialize parameters
    for epoch in range(epochs):
        for batch in train_loader:
            params, loss = train_step(params, batch['x'], batch['y'], optimizer)
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}")


## -----------------
Example_id = m4
## -----------------
import jax
import jax.numpy as jnp
from jax import random
import optax
import flax.linen as nn

# Initialize PRNG key
key = random.PRNGKey(42)

# Generate synthetic CT-scan data (batches, slices, RGB)
batch = 100
num_slices = 10
channels = 3
width = 256
height = 256

key, subkey = random.split(key)
ct_images = random.normal(subkey, shape=(batch, num_slices, channels, width, height))
segmentation_masks = (random.normal(subkey, shape=(batch, num_slices, 1, width, height)) > 0).astype(jnp.float32)

print(f"CT images (train examples) shape: {ct_images.shape}")
print(f"Segmentation binary masks (labels) shape: {segmentation_masks.shape}")

class MedCNN(nn.Module):
    # def __init__(self, backbone, out_channel=1):
    #     super().__init__()
    #     self.backbone = backbone
    #     self.conv1 = nn.Conv(64, (3, 3, 3), padding='SAME')
    #     self.conv2 = nn.Conv(64, (3, 3, 3), padding='SAME')
    #     self.conv_transpose1 = nn.ConvTranspose(32, (1, 4, 4), strides=(1, 4, 4))
    #     self.conv_transpose2 = nn.ConvTranspose(16, (1, 8, 8), strides=(1, 8, 8))
    #     self.final_conv = nn.Conv(out_channel, (1, 1, 1))
    backbone: nn.Module  # Fixed: Properly typed backbone as nn.Module
    out_channel: int = 1  # Fixed: Defined as class attribute

    def setup(self):  # Fixed: Moved layer definitions to setup from __init__ for Flax compatibility
        self.conv1 = nn.Conv(64, (3, 3, 3), padding='SAME')
        self.conv2 = nn.Conv(64, (3, 3, 3), padding='SAME')
        self.conv_transpose1 = nn.ConvTranspose(32, (1, 4, 4), strides=(1, 4, 4))
        self.conv_transpose2 = nn.ConvTranspose(16, (1, 8, 8), strides=(1, 8, 8))
        self.final_conv = nn.Conv(self.out_channel, (1, 1, 1))
    def __call__(self, x):
        b, d, c, w, h = x.shape
        x = x.reshape((b * d, c, w, h))
        features = self.backbone(x)
        _, new_c, new_w, new_h = features.shape
        x = features.reshape((b, d, new_c, new_w, new_h))
        x = jnp.transpose(x, (0, 2, 1, 3, 4))
        x = nn.relu(self.conv1(x))
        x = nn.relu(self.conv2(x))
        x = nn.relu(self.conv_transpose1(x))
        x = nn.relu(self.conv_transpose2(x))
        x = jax.nn.sigmoid(self.final_conv(x))
        return x

def compute_dice_loss(pred, labels, eps=1e-8):
    numerator = 2 * jnp.sum(pred * labels)
    denominator = jnp.sum(pred) + jnp.sum(labels) + eps
    return numerator / denominator
# Fixed: Replaced nn.Sequential with a proper Flax module to avoid TypeError
class ResNetBackbone(nn.Module):
    def setup(self):
        self.dense1 = nn.Dense(512)
        self.dense2 = nn.Dense(256)

    def __call__(self, x):
        x = self.dense1(x)
        x = nn.relu(x)
        x = self.dense2(x)
        return x
resnet_model = ResNetBackbone()

model = MedCNN(backbone=resnet_model)
params = model.init(key, ct_images)

# Optimizer and training loop
learning_rate = 0.01
optimizer = optax.adam(learning_rate)
opt_state = optimizer.init(params)

def update(params, ct_images, segmentation_masks, opt_state):
    loss, grads = jax.value_and_grad(compute_dice_loss)(params, ct_images, segmentation_masks)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss

# Training loop
epochs = 5
for epoch in range(epochs):
    params, opt_state, loss = update(params, ct_images, segmentation_masks, opt_state)
    print(f"Loss at epoch {epoch}: {loss}")

## -----------------
Example_id = m5
## -----------------
import jax
import jax.numpy as jnp
from jax import grad, jit, random
import optax

# Generate synthetic sequential data
def generate_data(num_samples=100):
    X = jnp.linspace(0, 4 * 3.14159, num_samples).reshape(-1, 1)
    y = jnp.sin(X)
    return X, y

# Prepare data for RNN
def create_in_out_sequences(data, seq_length):
    in_seq = []
    out_seq = []
    for i in range(len(data) - seq_length):
        in_seq.append(data[i:i + seq_length])
        out_seq.append(data[i + seq_length])
    return jnp.stack(in_seq), jnp.stack(out_seq)

X, y = generate_data()
sequence_length = 10
X_seq, y_seq = create_in_out_sequences(y, sequence_length)

# Fixed: Update rnn_model to accept params as a tuple
def rnn_model(params, x, hidden_state):
    """
    Process input sequence step-by-step, updating hidden state.
    params: tuple of (rnn_xh, rnn_hh, rnn_hy, rnn_bias)
    x: shape (batch_size, seq_length, input_dim)
    hidden_state: shape (hidden_dim,)
    """
    def step(carry, x_t):
        h = carry
        # Fixed: Access params as tuple indices instead of dictionary keys
        h = jnp.tanh(
            jnp.dot(h, params[1]) +  # Hidden-to-hidden (rnn_hh)
            jnp.dot(x_t, params[0]) +  # Input-to-hidden (rnn_xh)
            params[3]  # Bias (rnn_bias)
        )
        # Output: y_t = W_hy * h_t
        y = jnp.dot(h, params[2])  # Hidden-to-output (rnn_hy)
        return h, y

    # Correctly add batch dimension to hidden_state
    batch_size = x.shape[0]
    hidden_state = jnp.repeat(jnp.expand_dims(hidden_state, 0), batch_size, axis=0)  # Shape (1, hidden_dim) -> (batch_size, hidden_dim)
    
    # Process sequence
    _, outputs = jax.lax.scan(step, hidden_state, x.transpose(1, 0, 2))  # (seq_length, batch_size, input_dim)
    return outputs.transpose(1, 0, 2)  # (batch_size, seq_length, output_dim)

# Fixed: Update loss_fn to pass params as tuple
def loss_fn(params, X, y, hidden_state):
    preds = rnn_model(params, X, hidden_state)
    return jnp.mean((preds - y) ** 2)

# Fixed: Update compute_gradient to pass params as tuple
@jit
def compute_gradient(params, X, y, hidden_state):
    return grad(loss_fn)(params, X, y, hidden_state)

# Fixed: Update train_step to handle params and grads as tuples
@jit
def train_step(params, X, y, hidden_state, learning_rate=0.001):
    grads = compute_gradient(params, X, y, hidden_state)
    # Fixed: Update params tuple with corresponding gradient tuple
    new_params = tuple(
        p - learning_rate * g for p, g in zip(params, grads)
    )
    return new_params

# Model initialization
def init_model(key, input_dim=1, hidden_dim=50, output_dim=1):
    keys = random.split(key, 4)
    # Fixed: Return params as a tuple to match rnn_model expectation
    params = (
        random.normal(keys[0], (input_dim, hidden_dim)),  # rnn_xh: Input to hidden
        random.normal(keys[1], (hidden_dim, hidden_dim)),  # rnn_hh: Hidden to hidden
        random.normal(keys[2], (hidden_dim, output_dim)),  # rnn_hy: Hidden to output
        random.normal(keys[3], (hidden_dim,))  # rnn_bias: Bias for hidden state
    )
    hidden_state = jnp.zeros((hidden_dim,))
    return params, hidden_state

# Training loop
def train_model(X, y, epochs=500, learning_rate=0.001):
    key = random.PRNGKey(42)
    params, hidden_state = init_model(key)

    for epoch in range(epochs):
        for sequences, labels in zip(X_seq, y_seq):
            sequences = sequences.reshape(1, sequence_length, 1)
            labels = labels.reshape(1, 1, 1)
            params = train_step(params, sequences, labels, hidden_state, learning_rate)

        loss = loss_fn(params, sequences, labels, hidden_state)
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss}")

    return params, hidden_state

# Testing on new data
X_test = jnp.linspace(4 * 3.14159, 5 * 3.14159, sequence_length).reshape(1, sequence_length, 1)

params, hidden_state = train_model(X, y)

# Predictions
predictions = rnn_model(params, X_test, hidden_state)
print(f"Predictions for new sequence: {predictions.tolist()}")

## -----------------
Example_id = m6
## -----------------
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import numpy as np

# Simulate loading and transforming CIFAR-10 data
def load_data():
    # Placeholder for CIFAR-10 data: random images and labels
    key = jax.random.PRNGKey(42)
    images = jax.random.uniform(key, shape=(64, 32, 32, 3), minval=0,
                                maxval=1)  # Simulate CIFAR-10 images (64 batch size)
    labels = jax.random.randint(key, shape=(64,), minval=0, maxval=10)  # Simulate CIFAR-10 labels (10 classes)
    return images, labels

# Data Augmentation functions
def random_horizontal_flip(images):
    # Randomly flip images horizontally
    flip_mask = jax.random.randint(jax.random.PRNGKey(0), shape=(images.shape[0],), minval=0, maxval=2)  # 0 or 1 flip
    return jnp.array([jnp.fliplr(img) if flip == 1 else img for img, flip in zip(images, flip_mask)])

def random_crop(images, crop_size=32, padding=4):
    # Randomly crop images with padding (e.g., 32x32 image with padding 4)
    pad_images = jnp.pad(images, ((0, 0), (padding, padding), (padding, padding), (0, 0)), mode='constant',
                         constant_values=0)
    crop_start = jax.random.randint(jax.random.PRNGKey(1), shape=(images.shape[0], 2), minval=0, maxval=padding)
    return jnp.array([img[start[0]:start[0] + crop_size, start[1]:start[1] + crop_size, :] for img, start in
                      zip(pad_images, crop_start)])

def normalize(images, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):
    # Normalize images
    return (images - np.array(mean)) / np.array(std)

# Apply transformations
def apply_transformations(images):
    images = random_horizontal_flip(images)
    images = random_crop(images)
    images = normalize(images)
    return images

# Display a batch of images
def imshow(img):
    img = img / 2 + 0.5  # Unnormalize
    npimg = np.asarray(img)
    plt.imshow(npimg)
    plt.show()

# Main function to load data and apply transformations
def main():
    images, labels = load_data()  # Simulate loading CIFAR-10 data
    transformed_images = apply_transformations(images)  # Apply transformations

    # Display a batch of transformed images
    imshow(transformed_images[0])  # Display the first image in the batch

if __name__ == "__main__":
    main()

## -----------------
Example_id = m7
## -----------------

import jax
import jax.numpy as jnp
import numpy as np
import optax
from jax import random

# Load MNIST dataset in JAX (manually handle data loading)
def load_data():
    X = jnp.array(np.random.randn(60000, 28*28))  # Fixed: Convert to JAX array
    y = jnp.array(np.random.randint(0, 10, size=(60000,)))  # Fixed: Convert to JAX array
    return X, y

# Define the simple neural network model in JAX
class SimpleNN:
    def __init__(self, key):
        self.params = self.init_params(key)

    def init_params(self, key):
        keys = random.split(key, 2)
        w1 = random.normal(keys[0], (28*28, 128))  # Weights for the first layer
        b1 = jnp.zeros((128,))
        w2 = random.normal(keys[1], (128, 10))  # Weights for the second layer (output)
        b2 = jnp.zeros((10,))
        return {"w1": w1, "b1": b1, "w2": w2, "b2": b2}

    def __call__(self, x):
        x = jnp.dot(x, self.params["w1"]) + self.params["b1"]
        x = jax.nn.relu(x)
        return jnp.dot(x, self.params["w2"]) + self.params["b2"]

# Define the loss function (CrossEntropy Loss)
def loss_fn(params, X, y):
    logits = model(X)
    # Fixed: Convert integer labels to one-hot encoded labels
    y_one_hot = jax.nn.one_hot(y, num_classes=10)  # Shape: (batch_size, 10)
    return -jnp.mean(jnp.sum(y_one_hot * jax.nn.log_softmax(logits), axis=1))

# Initialize model, loss function, and optimizer
key = random.PRNGKey(0)
model = SimpleNN(key)
learning_rate = 0.01
optimizer = optax.sgd(learning_rate)
opt_state = optimizer.init(model.params)

# Training loop
epochs = 5
X_train, y_train = load_data()  # Load data

for epoch in range(epochs):
    # Simulate a batch of data for training
    batch_size = 64
    for i in range(0, len(X_train), batch_size):
        X_batch = X_train[i:i + batch_size]
        y_batch = y_train[i:i + batch_size]

        # Compute loss and gradients
        loss, grads = jax.value_and_grad(loss_fn)(model.params, X_batch, y_batch)

        # Update parameters using gradients
        updates, opt_state = optimizer.update(grads, opt_state)
        model.params = optax.apply_updates(model.params, updates)

    print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

# Testing loop
X_test = jnp.array(np.random.randn(100, 28*28))  # Fixed: Convert to JAX array, 100 test samples
y_test = jnp.array(np.random.randint(0, 10, size=(100,)))  # Fixed: Convert to JAX array
logits = model(X_test)
predictions = jnp.argmax(logits, axis=1)
accuracy = jnp.mean(predictions == y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

## -----------------
Example_id = m8
## -----------------

## m8 Set D
import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
from jax import random
import matplotlib.pyplot as plt

# Initialize PRNG key
key = random.PRNGKey(42)

# Define the Autoencoder model in JAX using flax.linen
class Autoencoder(nn.Module):
    @nn.compact
    def __call__(self, x):
        # Encoder
        x = nn.Conv(32, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='SAME')

        x = nn.Conv(64, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='SAME')

        # Decoder
        x = nn.ConvTranspose(32, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.ConvTranspose(1, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)
        return nn.sigmoid(x)  # To keep pixel values between 0 and 1

# Loss function (Mean Squared Error)
def loss_fn(params, model, images, targets):
    reconstructed = model.apply({'params': params}, images)
    return jnp.mean((reconstructed - targets) ** 2)

# Optimizer initialization
def create_optimizer(params):
    tx = optax.adam(learning_rate=0.001)
    return tx.init(params)

# Training step
def train_step(params, images, targets, model, optimizer):
    grads = jax.grad(loss_fn)(params, model, images, targets)
    updates, optimizer = optax.adam(learning_rate=0.001).update(grads, optimizer)
    params = optax.apply_updates(params, updates)
    return params, optimizer

# Main function to simulate the training process
def main():
    # Generate synthetic MNIST-like data for the demo (use real MNIST in practice)
    X = random.normal(key, (64, 28, 28, 1))  # Batch of 64 images of size 28x28
    y = X  # Autoencoder target is the input image

    # Initialize the model
    model = Autoencoder()

    # Initialize parameters using PRNG
    params = model.init(key, X)['params']# Use a dummy input to initialize the parameters

    # Initialize optimizer
    optimizer = create_optimizer(params)

    # Training loop (simplified)
    epochs = 10
    for epoch in range(epochs):
        params, optimizer = train_step(params, X, y, model, optimizer)
        if (epoch + 1) % 1 == 0:  # Print loss every epoch
            current_loss = loss_fn(params, model, X, y)
            print(f"Epoch [{epoch + 1}/{epochs}], Loss: {current_loss:.4f}")

    # Plotting (using matplotlib)
    plt.imshow(X[0, :, :, 0], cmap='gray')
    plt.title("Original Image")
    plt.show()

if __name__ == "__main__":
    main()

## -----------------
Example_id = h6
## -----------------

## h6 - Set D Fixed Code
import jax
import jax.numpy as jnp
from flax import linen as nn
import optax
from jax import random

# Define the Language Model in JAX
class LanguageModel(nn.Module):
    vocab_size: int
    embed_size: int
    hidden_size: int
    num_layers: int

    def setup(self):
        # Use embedding layer, LSTM cell, and dense layer
        self.embedding = nn.Embed(self.vocab_size, self.embed_size)
        # Fixed: Added features parameter to LSTMCell, set to hidden_size
        self.lstm = nn.LSTMCell(features=self.hidden_size)
        self.fc = nn.Dense(self.vocab_size)

    def __call__(self, x):
      # Initialize LSTM hidden and cell states
        embedded = self.embedding(x)  
        batch_size = x.shape[0]
        hidden = jnp.zeros((batch_size, self.hidden_size))
        cell = jnp.zeros((batch_size, self.hidden_size))
        
        def lstm_step(carry, x_t):
            hidden, cell = carry
            (hidden, cell), _ = self.lstm((hidden, cell), x_t)
            return (hidden, cell), hidden
        
        # Process sequence with scan
        _, hidden_states = jax.lax.scan(lstm_step, (hidden, cell), embedded.transpose(1, 0, 2))  # (seq_length, batch_size, hidden_size)
        hidden_states = hidden_states.transpose(1, 0, 2)  # (batch_size, seq_length, hidden_size)
        
        # Use final hidden state for output
        output = self.fc(hidden_states[:, -1, :])  # (batch_size, vocab_size)
        return output

# Example for initializing parameters
key = random.PRNGKey(0)
vocab_size = 50
embed_size = 64
hidden_size = 128
num_layers = 2
seq_length = 10
batch_size = 32
model = LanguageModel(vocab_size=vocab_size, embed_size=embed_size, hidden_size=hidden_size, num_layers=num_layers)
params = model.init(key, jnp.ones((batch_size, seq_length), dtype=jnp.int32))  # Random input shape (batch_size, seq_length)

# Create synthetic data
X_train = jnp.array(random.randint(key, (batch_size, seq_length), 0, vocab_size))
y_train = jnp.array(random.randint(key, (batch_size,), 0, vocab_size))

# Loss function
def loss_fn(params, model, inputs, targets):
    logits = model.apply({'params': params}, inputs)
    # Fixed: Convert integer targets to one-hot for cross-entropy loss
    targets_one_hot = jax.nn.one_hot(targets, vocab_size)
    return optax.softmax_cross_entropy(logits, targets_one_hot).mean()

# Optimizer setup (Adam)
optimizer = optax.adam(learning_rate=0.001)

# Update function
@jax.jit(static_argnums=(1, 4))
def update(params, model, inputs, targets, optimizer, optimizer_state):
    loss, grads = jax.value_and_grad(loss_fn)(params, model, inputs, targets)
    updates, optimizer_state = optimizer.update(grads, optimizer_state)
    new_params = optax.apply_updates(params, updates)
    return new_params, loss, optimizer_state

# Training loop
def train(model, params, X_train, y_train, optimizer, num_epochs=5):
    optimizer_state = optimizer.init(params)
    for epoch in range(num_epochs):
        params, loss, optimizer_state = update(params, model, X_train, y_train, optimizer, optimizer_state)
        print(f"Epoch {epoch + 1}/{num_epochs}: Loss: {loss}")
    return params

# Train the model
params = train(model, params, X_train, y_train, optimizer, num_epochs=5)

# Testing on new data
# Fixed: Ensure X_test matches training input shape (batch_size, seq_length)
X_test = jnp.array(random.randint(key, (2, seq_length), 0, vocab_size))  # Shape: (2, 10)
predictions = model.apply({'params': params}, X_test)
print(f"Predictions for {X_test.tolist()}: {predictions.tolist()}")

## -----------------
Example_id = h10
## -----------------
import jax
import jax.numpy as jnp
import flax.linen as nn
from jax import grad, jit, random
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

# Define the CNN Model in Flax (Simplified version)
class CNNModel(nn.Module):
    @nn.compact
    def __call__(self, x, return_conv=False):
      # FIX: Ensure conv output is correctly captured and returned
        conv_out = nn.Conv(64, (3, 3), padding="SAME", name="Conv_0")(x)  # Shape: (1, 224, 224, 64)
        conv_out = nn.relu(conv_out)  # Apply ReLU to conv output
        if return_conv:
            return conv_out  # Return conv output for Grad-CAM
        x = nn.Dense(10)(conv_out)  # Shape: (1, 10)
        # END FIX
        return x
def loss_fn(params, model, X, y):
    # FIX: Use params["params"] instead of params
    preds = model.apply({'params': params["params"]}, X)  # Get predictions
    # END FIX
    return jnp.mean((preds - y) ** 2)  # Mean Squared Error Loss

# Grad-CAM implementation in JAX
def grad_cam(model, params, X, target_class):
    # Get the output of the model and the gradients w.r.t the last convolution layer
    def compute_loss(params, X, y):
        # FIX: Use params["params"] instead of params
        preds = model.apply({'params': params["params"]}, X)
        # END FIX
        return jnp.mean((preds - y) ** 2)

    # FIX: Compute gradients and extract Conv_0/kernel gradient
    grads = grad(compute_loss)(params, X, target_class)
    # Extract the gradient for the convolutional layer's kernel
    conv_grad = grads["params"]["Conv_0"]["kernel"]  # Shape: (3, 3, 3, 64)
    # Average over input channels and kernel size to get feature map importance
    feature_weights = jnp.mean(conv_grad, axis=(0, 1, 2))  # Shape: (64,)
    # FIX: Get the Conv_0 layer output explicitly
    # FIX: Ensure conv_output is the Conv_0 output and verify shapes
    conv_output = model.apply({'params': params["params"]}, X, return_conv=True)  # Shape: (1, 224, 224, 64)
    # Debug: Print shapes to verify
    print(f"conv_output shape: {conv_output.shape}, feature_weights shape: {feature_weights.shape}")
    # Compute weighted sum of feature maps
    cam = jnp.einsum('...ijk,k->...ij', conv_output, feature_weights)  # Shape: (1, 224, 224)
    # END FIX
# Generate synthetic data (for testing)
key = random.PRNGKey(0)
X = random.uniform(key, shape=(1, 224, 224, 3))  # Example input data (224x224 RGB image)
y = jnp.array([[1]])  # Example target (class label)

# Initialize the model and parameters
model = CNNModel()
params = model.init(key, X)

# FIX: Use params["params"] instead of params
output = model.apply({'params': params["params"]}, X)
# END FIX
predicted_class = output.argmax()

# Compute Grad-CAM for the predicted class
grads = grad_cam(model, params, X, y)

# Visualize the Grad-CAM output (simplified)
# FIX: Update heatmap computation for the CAM output
heatmap = jnp.mean(grads, axis=-1)  # Average over any remaining dimensions if needed
heatmap = jnp.maximum(heatmap, 0)  # ReLU to keep positive values
heatmap = heatmap / jnp.max(heatmap)  # Normalize the heatmap
# END FIX

# Overlay heatmap on the image
plt.imshow(X[0, :, :, :], alpha=0.7)  # Original image
# Fixed: Resize heatmap to match image dimensions
heatmap_resized = jnp.array(Image.fromarray(np.array(heatmap[0])).resize((224, 224)))
plt.imshow(heatmap, alpha=0.5, cmap='jet')  # Grad-CAM heatmap
plt.title(f"Predicted Class: {predicted_class}")
plt.axis('off')
plt.show()