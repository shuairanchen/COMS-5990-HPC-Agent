{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "061ec0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core==0.3.33 in c:\\users\\15157\\anaconda3\\lib\\site-packages (0.3.33)\n",
      "Collecting langchain==0.3.16\n",
      "  Using cached langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain-community==0.3.16 in c:\\users\\15157\\anaconda3\\lib\\site-packages (0.3.16)\n",
      "Requirement already satisfied: langchain-text-splitters==0.3.5 in c:\\users\\15157\\anaconda3\\lib\\site-packages (0.3.5)\n",
      "Collecting langchain-openai==0.3.3\n",
      "  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pydantic==2.7.4\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-core==0.3.33) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-core==0.3.33) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-core==0.3.33) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-core==0.3.33) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-core==0.3.33) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-core==0.3.33) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain==0.3.16) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain==0.3.16) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain==0.3.16) (4.0.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain==0.3.16) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain==0.3.16) (2.31.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-community==0.3.16) (0.6.4)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-community==0.3.16) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-community==0.3.16) (2.7.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-openai==0.3.3) (1.61.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langchain-openai==0.3.3) (0.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from pydantic==2.7.4) (0.6.0)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic==2.7.4)\n",
      "  Downloading pydantic_core-2.18.4-cp39-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.16) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.16) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.16) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.16) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.16) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.16) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.16) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.33) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.33) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.33) (3.9.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.33) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.3) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.3) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.3) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\15157\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.3) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.3) (4.65.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.16) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.3.16) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.3.16) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.3.16) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.3.16) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.16) (2.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.3) (2023.10.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\15157\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core==0.3.33) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core==0.3.33) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\15157\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain-openai==0.3.3) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.16) (1.0.0)\n",
      "Using cached langchain-0.3.16-py3-none-any.whl (1.0 MB)\n",
      "Downloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/54.5 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 51.2/54.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 54.5/54.5 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 409.0/409.0 kB 12.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.4-cp39-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 30.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 30.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pydantic-core, pydantic, langchain-openai, langchain\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.27.2\n",
      "    Uninstalling pydantic_core-2.27.2:\n",
      "      Successfully uninstalled pydantic_core-2.27.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.21\n",
      "    Uninstalling pydantic-1.10.21:\n",
      "      Successfully uninstalled pydantic-1.10.21\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.4\n",
      "    Uninstalling langchain-openai-0.3.4:\n",
      "      Successfully uninstalled langchain-openai-0.3.4\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.17\n",
      "    Uninstalling langchain-0.3.17:\n",
      "      Successfully uninstalled langchain-0.3.17\n",
      "Successfully installed langchain-0.3.16 langchain-openai-0.3.3 pydantic-2.7.4 pydantic-core-2.18.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\15157\\anaconda3\\Lib\\site-packages\\~-dantic_core'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.7.4 which is incompatible.\n",
      "langchain-pinecone 0.1.0 requires langchain-core<0.2.0,>=0.1.40, but you have langchain-core 0.3.33 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade \n",
    "# !pip install --upgrade langchain-community openai python-dotenv pyyaml\n",
    "# !pip install langchain-core==0.3.33 langchain==0.3.16 langchain-community==0.3.16 langchain-text-splitters==0.3.5 langchain-openai==0.3.3 pydantic==2.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "910f98f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anaconda-cloud-auth==0.7.2\n",
      "  Downloading anaconda_cloud_auth-0.7.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting anaconda-cli-base>=0.4.0 (from anaconda-cloud-auth==0.7.2)\n",
      "  Downloading anaconda_cli_base-0.4.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (41.0.3)\n",
      "Requirement already satisfied: keyring in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (23.13.1)\n",
      "Requirement already satisfied: pkce in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (1.0.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (2.7.4)\n",
      "Requirement already satisfied: pyjwt in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (2.4.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (1.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (2.31.0)\n",
      "Requirement already satisfied: semver<4 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cloud-auth==0.7.2) (2.13.0)\n",
      "Requirement already satisfied: click in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (8.1.7)\n",
      "Requirement already satisfied: pydantic-settings>=2.3 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (2.7.1)\n",
      "Collecting readchar (from anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2)\n",
      "  Downloading readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (13.6.0)\n",
      "Requirement already satisfied: tomli in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (2.0.1)\n",
      "Requirement already satisfied: typer in c:\\users\\15157\\anaconda3\\lib\\site-packages (from anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (0.9.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from cryptography>=3.4.0->anaconda-cloud-auth==0.7.2) (1.15.1)\n",
      "Requirement already satisfied: jaraco.classes in c:\\users\\15157\\anaconda3\\lib\\site-packages (from keyring->anaconda-cloud-auth==0.7.2) (3.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from keyring->anaconda-cloud-auth==0.7.2) (6.0.0)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from keyring->anaconda-cloud-auth==0.7.2) (0.2.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from pydantic->anaconda-cloud-auth==0.7.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from pydantic->anaconda-cloud-auth==0.7.2) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from pydantic->anaconda-cloud-auth==0.7.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests->anaconda-cloud-auth==0.7.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests->anaconda-cloud-auth==0.7.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests->anaconda-cloud-auth==0.7.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from requests->anaconda-cloud-auth==0.7.2) (2023.7.22)\n",
      "Requirement already satisfied: pycparser in c:\\users\\15157\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.0->anaconda-cloud-auth==0.7.2) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.11.4->keyring->anaconda-cloud-auth==0.7.2) (3.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\15157\\anaconda3\\lib\\site-packages (from click->anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (0.4.6)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\15157\\anaconda3\\lib\\site-packages (from jaraco.classes->keyring->anaconda-cloud-auth==0.7.2) (8.12.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from rich->anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from rich->anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\15157\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->anaconda-cli-base>=0.4.0->anaconda-cloud-auth==0.7.2) (0.1.0)\n",
      "Downloading anaconda_cloud_auth-0.7.2-py3-none-any.whl (24 kB)\n",
      "Downloading anaconda_cli_base-0.4.2-py3-none-any.whl (11 kB)\n",
      "Downloading readchar-4.2.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: readchar, anaconda-cli-base, anaconda-cloud-auth\n",
      "  Attempting uninstall: anaconda-cloud-auth\n",
      "    Found existing installation: anaconda-cloud-auth 0.1.4\n",
      "    Uninstalling anaconda-cloud-auth-0.1.4:\n",
      "      Successfully uninstalled anaconda-cloud-auth-0.1.4\n",
      "Successfully installed anaconda-cli-base-0.4.2 anaconda-cloud-auth-0.7.2 readchar-4.2.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install anaconda-cloud-auth==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5aab88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Dict, List, Any, Tuple\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4adedc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "class CodeTranslationSystem:\n",
    "    def __init__(self, kb_path: Optional[str] = None):\n",
    "        self.knowledge_base: Dict[str, Dict[str, List[str]]] = {}\n",
    "        if kb_path:\n",
    "            self.load_knowledge_base(kb_path)\n",
    "        self.super_agent = self.create_super_agent()\n",
    "        self.translation_agent = self.create_translation_agent()\n",
    "        self.max_iterations = 3\n",
    "        self.execution_log: List[Dict[str, Any]] = []\n",
    "        \n",
    "    def load_knowledge_base(self, file_path: str):\n",
    "        \"\"\"Load a knowledge base from a YAML file\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.knowledge_base = yaml.safe_load(f)\n",
    "\n",
    "    def _get_code_rules(self, target_lang: str) -> str:\n",
    "        \"\"\"Generate code rule description in target language\"\"\"\n",
    "        if not self.knowledge_base:\n",
    "            return \"\"\n",
    "            \n",
    "        rules = self.knowledge_base.get(target_lang, {})\n",
    "        return \"\\n\".join(\n",
    "            [f\"# {cat.upper()}\\n\" + \"\\n\".join(f\"- {item}\" for item in items)\n",
    "             for cat, items in rules.items() if cat != \"analysis_rules\"]\n",
    "        )\n",
    "    \n",
    "    def _log_step(self, step_name: str, input_data: dict, output_data: Any):\n",
    "        \"\"\"Log each step's input and output\"\"\"\n",
    "        self.execution_log.append({\n",
    "            \"step\": step_name,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"input\": input_data,\n",
    "            \"output\": output_data\n",
    "        })\n",
    "        \n",
    "    def _get_analysis_rules(self, target_lang: str) -> str:\n",
    "        \"\"\"Get rules for requirements analysis\"\"\"\n",
    "        if not self.knowledge_base:\n",
    "            return \"\"\n",
    "            \n",
    "        analysis_rules = self.knowledge_base.get(target_lang, {}).get(\"analysis_rules\", [])\n",
    "        return \"Special Rules:\\n\" + \"\\n\".join([f\"- {rule}\" for rule in analysis_rules])\n",
    "\n",
    "    def create_super_agent(self) -> LLMChain:\n",
    "        \"\"\"Create requirement analysis agent\"\"\"\n",
    "        analysis_template = \"\"\"You are a senior code analysis expert. Perform these tasks:\n",
    "        1. Identify source programming language (C/C++/FORTRAN)\n",
    "        2. Identify target language (CUDA/OpenMP/JAX)\n",
    "        3. Extract code content needing conversion\n",
    "        4. Analyze potential conversion challenges\n",
    "        5. Generate code conversion task description\n",
    "        \n",
    "        {% if analysis_rules %}\n",
    "        {{ analysis_rules }}\n",
    "        {% endif %}\n",
    "\n",
    "        User input: {{user_input}}\n",
    "\n",
    "        Respond in this format:\n",
    "        Source Language: [detected source language]\n",
    "        Target Language: [detected target language]\n",
    "        Code Content: [extracted code block]\n",
    "        Potential Issues: \n",
    "        - [Issue1 description]\n",
    "        - [Issue2 description]\n",
    "        Task Description: \"Convert the following [source] code to [target]:\\n[code]\"\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=analysis_template,\n",
    "            input_variables=[\"user_input\"],\n",
    "            partial_variables={\"analysis_rules\": self._get_analysis_rules(\"generic\")},\n",
    "            template_format=\"jinja2\"\n",
    "        )\n",
    "        return LLMChain(\n",
    "            llm=ChatOpenAI(\n",
    "                temperature=0,\n",
    "                model=\"gpt-3.5-turbo\"\n",
    "            ),\n",
    "            prompt=prompt,\n",
    "            output_key=\"analysis\"\n",
    "        )\n",
    "\n",
    "    def create_translation_agent(self) -> LLMChain:\n",
    "        \"\"\"Create code translation agent\"\"\"\n",
    "        translation_template = \"\"\"You are an HPC code conversion expert. Convert this {{source_lang}} code to {{target_lang}}:\n",
    "        Requirements:\n",
    "        1. Maintain identical algorithmic logic\n",
    "        2. Follow target language's performance best practices\n",
    "        3. Add necessary comments explaining modifications\n",
    "        4. Ensure syntactic correctness\n",
    "\n",
    "        {{code_input}}\n",
    "\n",
    "        Return ONLY converted code without explanations.\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=translation_template,\n",
    "            input_variables=[\"source_lang\", \"target_lang\", \"code_input\"],\n",
    "            template_format=\"jinja2\"\n",
    "        )\n",
    "        return LLMChain(\n",
    "            llm=ChatOpenAI(\n",
    "                temperature=0,\n",
    "                model=\"gpt-3.5-turbo\"\n",
    "            ),\n",
    "            prompt=prompt,\n",
    "            output_key=\"translated_code\"\n",
    "        )\n",
    "\n",
    "    def parse_analysis(self, analysis: str) -> Tuple[str, str, str, List[str]]:\n",
    "        \"\"\"Enhanced analysis results interpretation\"\"\"\n",
    "        source_lang = target_lang = code_content = \"\"\n",
    "        potential_issues = []\n",
    "        in_issues_section = False\n",
    "\n",
    "        for line in analysis.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('Source Language:'):\n",
    "                source_lang = line.partition(':')[2].strip()\n",
    "            elif line.startswith('Target Language:'):\n",
    "                target_lang = line.partition(':')[2].strip()\n",
    "            elif line.startswith('Code Content:'):\n",
    "                code_content = line.partition(':')[2].strip()\n",
    "            elif line.lower().startswith('potential issues'):\n",
    "                in_issues_section = True\n",
    "            elif in_issues_section and line.startswith('-'):\n",
    "                potential_issues.append(line[2:].strip())\n",
    "\n",
    "        return source_lang, target_lang, code_content, potential_issues\n",
    "\n",
    "    def validate_code(self, code: str, target_lang: str) -> str:\n",
    "        \"\"\"Code validation workflow\"\"\"\n",
    "        code_rules = self._get_code_rules(target_lang)\n",
    "        \n",
    "        validation_template = \"\"\"Review this {{target_lang}} code:\n",
    "        {{code}}\n",
    "        \n",
    "        {% if code_rules %}\n",
    "        Code Rules:\n",
    "        {{code_rules}}\n",
    "        {% endif %}\n",
    "\n",
    "        Format your findings as:\n",
    "        Issues Found: [Yes/No]\n",
    "        Rule Violations:\n",
    "        - [Rule1] violation description (line X)\n",
    "        - [Rule2] violation description (line Y)\n",
    "        Suggestions: \n",
    "        - [Suggestion1]\n",
    "        - [Suggestion2]\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=validation_template,\n",
    "            input_variables=[\"target_lang\", \"code\"],\n",
    "            partial_variables={\"code_rules\": code_rules},\n",
    "            template_format=\"jinja2\"\n",
    "        )\n",
    "        \n",
    "        chain = LLMChain(\n",
    "            llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "            prompt=prompt\n",
    "        )\n",
    "        result = chain.run(target_lang=target_lang, code=code)\n",
    "        \n",
    "        self._log_step(\n",
    "            step_name=\"code_validation\",\n",
    "            input_data={\"target_lang\": target_lang, \"code\": code},\n",
    "            output_data=result\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def iterative_improvement(self, initial_code: str, target_lang: str) -> str:\n",
    "        \"\"\"Iterative code improvement\"\"\"\n",
    "        improvement_template = \"\"\"Improve the code based on this report:\n",
    "        {{validation_result}}\n",
    "\n",
    "        {% if code_rules %}\n",
    "        Must follow these rules:\n",
    "        {{code_rules}}\n",
    "        {% endif %}\n",
    "\n",
    "        Original Code:\n",
    "        {{current_code}}\n",
    "\n",
    "        Requirements:\n",
    "        1. Strictly follow the suggestions\n",
    "        2. Preserve functionality\n",
    "        3. Return complete revised code\n",
    "        \"\"\"\n",
    "\n",
    "        code_rules = self._get_code_rules(target_lang)\n",
    "        current_code = initial_code\n",
    "        iteration_log = []\n",
    "\n",
    "        for iter_num in range(self.max_iterations):\n",
    "            validation_result = self.validate_code(current_code, target_lang)\n",
    "\n",
    "            iteration_data = {\n",
    "                \"iteration\": iter_num + 1,\n",
    "                \"code_before\": current_code,\n",
    "                \"validation_result\": validation_result\n",
    "            }\n",
    "\n",
    "            if \"Issues Found: No\" in validation_result:\n",
    "                iteration_data[\"action\"] = \"validation_passed\"\n",
    "                iteration_log.append(iteration_data)\n",
    "                break\n",
    "\n",
    "            # 修复关键点：使用PromptTemplate代替f-string\n",
    "            prompt_template = PromptTemplate(\n",
    "                template=improvement_template,\n",
    "                input_variables=[\"validation_result\", \"current_code\"],\n",
    "                partial_variables={\"code_rules\": code_rules},\n",
    "                template_format=\"jinja2\"\n",
    "            )\n",
    "\n",
    "            improvement_prompt = prompt_template.format(\n",
    "                validation_result=validation_result,\n",
    "                current_code=current_code\n",
    "            )\n",
    "\n",
    "            self._log_step(\n",
    "                step_name=\"improvement_request\",\n",
    "                input_data={\n",
    "                    \"validation_result\": validation_result,\n",
    "                    \"previous_code\": current_code\n",
    "                },\n",
    "                output_data=improvement_prompt\n",
    "            )\n",
    "\n",
    "            new_code = self.translation_agent.run({\n",
    "                \"source_lang\": target_lang,\n",
    "                \"target_lang\": target_lang,\n",
    "                \"code_input\": improvement_prompt\n",
    "            })\n",
    "\n",
    "            self._log_step(\n",
    "                step_name=\"improvement_response\",\n",
    "                input_data={\"prompt\": improvement_prompt},\n",
    "                output_data=new_code\n",
    "            )\n",
    "\n",
    "            iteration_data.update({\n",
    "                \"action\": \"code_improved\",\n",
    "                \"code_after\": new_code\n",
    "            })\n",
    "            iteration_log.append(iteration_data)\n",
    "            current_code = new_code\n",
    "\n",
    "        self._log_step(\n",
    "            step_name=\"iteration_summary\",\n",
    "            input_data={\n",
    "                \"initial_code\": initial_code,\n",
    "                \"max_iterations\": self.max_iterations,\n",
    "                \"target_language\": target_lang\n",
    "            },\n",
    "            output_data={\n",
    "                \"final_code\": current_code,\n",
    "                \"iterations\": iteration_log,\n",
    "                \"total_iterations\": len(iteration_log)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return current_code\n",
    "\n",
    "    def process_request(self, user_input: str) -> Dict:\n",
    "        \"\"\"Process user request with full logging\"\"\"\n",
    "        self._log_step(\"user_input\", {}, user_input)\n",
    "        \n",
    "        # Requirement Analysis\n",
    "        analysis = self.super_agent.run(user_input=user_input)\n",
    "        self._log_step(\"requirement_analysis\", {\"user_input\": user_input}, analysis)\n",
    "        \n",
    "        # Parse Analysis\n",
    "        source_lang, target_lang, code_content, potential_issues = self.parse_analysis(analysis)\n",
    "        self._log_step(\n",
    "            \"analysis_parsing\",\n",
    "            {\"raw_analysis\": analysis},\n",
    "            {\n",
    "                \"source_lang\": source_lang,\n",
    "                \"target_lang\": target_lang,\n",
    "                \"code_content\": code_content,\n",
    "                \"potential_issues\": potential_issues\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Initial Translation\n",
    "        translated_code = self.translation_agent.run({\n",
    "            \"source_lang\": source_lang,\n",
    "            \"target_lang\": target_lang,\n",
    "            \"code_input\": code_content\n",
    "        })\n",
    "        self._log_step(\n",
    "            \"initial_translation\",\n",
    "            {\n",
    "                \"source\": source_lang,\n",
    "                \"target\": target_lang,\n",
    "                \"code\": code_content\n",
    "            },\n",
    "            translated_code\n",
    "        )\n",
    "\n",
    "        # Iterative Improvement\n",
    "        final_code = self.iterative_improvement(translated_code, target_lang)\n",
    "        self._log_step(\n",
    "            \"final_result\",\n",
    "            {\"iterations\": self.max_iterations},\n",
    "            final_code\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"source_language\": source_lang,\n",
    "            \"target_language\": target_lang,\n",
    "            \"original_code\": code_content,\n",
    "            \"translated_code\": final_code,\n",
    "            \"execution_log\": self.execution_log\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b172c488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Results:\n",
      "```cuda\n",
      "#include <stdio.h>\n",
      "\n",
      "__global__ void vecAdd(float *a, float *b, float *c, int n) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < n) {\n",
      "        c[i] = a[i] + b[i];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 100000;\n",
      "    float *h_a, *h_b, *h_c;\n",
      "    float *d_a, *d_b, *d_c;\n",
      "    \n",
      "    cudaMallocManaged(&h_a, n * sizeof(float));\n",
      "    cudaMallocManaged(&h_b, n * sizeof(float));\n",
      "    cudaMallocManaged(&h_c, n * sizeof(float));\n",
      "    \n",
      "    for (int i = 0; i < n; i++) {\n",
      "        h_a[i] = i;\n",
      "        h_b[i] = i * 2;\n",
      "    }\n",
      "    \n",
      "    int blockSize = 256;\n",
      "    int gridSize = (n + blockSize - 1) / blockSize;\n",
      "    \n",
      "    vecAdd<<<gridSize, blockSize>>>(h_a, h_b, h_c, n);\n",
      "    cudaDeviceSynchronize();\n",
      "    \n",
      "    for (int i = 0; i < 10; i++) {\n",
      "        printf(\"%f + %f = %f\\n\", h_a[i], h_b[i], h_c[i]);\n",
      "    }\n",
      "    \n",
      "    cudaFree(h_a);\n",
      "    cudaFree(h_b);\n",
      "    cudaFree(h_c);\n",
      "    \n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "=== Full Execution Log ===\n",
      "\n",
      "[user_input]\n",
      "Input: {}\n",
      "Output: \n",
      "    Please help me convert the following FORTRAN code into CUDA code:\n",
      "    PROGRAM VECTOR_ADD\n",
      "    INTEGER, PARAMETER :: N = 1000000\n",
      "    REAL :: A(N), B(N), C(N)\n",
      "    DO I = 1, N\n",
      "        C(I) = A(I) + B(I)\n",
      "    END DO\n",
      "    END PROGRAM\n",
      "    \n",
      "\n",
      "[requirement_analysis]\n",
      "Input: {'user_input': '\\n    Please help me convert the following FORTRAN code into CUDA code:\\n    PROGRAM VECTOR_ADD\\n    INTEGER, PARAMETER :: N = 1000000\\n    REAL :: A(N), B(N), C(N)\\n    DO I = 1, N\\n        C(I) = A(I) + B(I)\\n    END DO\\n    END PROGRAM\\n    '}\n",
      "Output: Source Language: FORTRAN\n",
      "Target Language: CUDA\n",
      "Code Content: \n",
      "```\n",
      "PROGRAM VECTOR_ADD\n",
      "INTEGER, PARAMETER :: N = 1000000\n",
      "REAL :: A(N), B(N), C(N)\n",
      "DO I = 1, N\n",
      "    C(I) = A(I) + B(I)\n",
      "END DO\n",
      "END PROGRAM\n",
      "```\n",
      "Potential Issues: \n",
      "- CUDA requires a different programming paradigm compared to FORTRAN\n",
      "- Memory management in CUDA is explicit and requires careful handling\n",
      "- Understanding CUDA kernel functions and memory hierarchy may be challenging\n",
      "\n",
      "Task Description: \"Convert the following FORTRAN code to CUDA:\n",
      "```\n",
      "// CUDA code conversion task will be performed here\n",
      "```\"\n",
      "\n",
      "[analysis_parsing]\n",
      "Input: {'raw_analysis': 'Source Language: FORTRAN\\nTarget Language: CUDA\\nCode Content: \\n```\\nPROGRAM VECTOR_ADD\\nINTEGER, PARAMETER :: N = 1000000\\nREAL :: A(N), B(N), C(N)\\nDO I = 1, N\\n    C(I) = A(I) + B(I)\\nEND DO\\nEND PROGRAM\\n```\\nPotential Issues: \\n- CUDA requires a different programming paradigm compared to FORTRAN\\n- Memory management in CUDA is explicit and requires careful handling\\n- Understanding CUDA kernel functions and memory hierarchy may be challenging\\n\\nTask Description: \"Convert the following FORTRAN code to CUDA:\\n```\\n// CUDA code conversion task will be performed here\\n```\"'}\n",
      "Output: {'source_lang': 'FORTRAN', 'target_lang': 'CUDA', 'code_content': '', 'potential_issues': ['CUDA requires a different programming paradigm compared to FORTRAN', 'Memory management in CUDA is explicit and requires careful handling', 'Understanding CUDA kernel functions and memory hierarchy may be challenging']}\n",
      "\n",
      "[initial_translation]\n",
      "Input: {'source': 'FORTRAN', 'target': 'CUDA', 'code': ''}\n",
      "Output: ```cuda\n",
      "#include <stdio.h>\n",
      "\n",
      "__global__ void vecAdd(float *a, float *b, float *c, int n) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < n) {\n",
      "        c[i] = a[i] + b[i];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 100000;\n",
      "    float *h_a, *h_b, *h_c;\n",
      "    float *d_a, *d_b, *d_c;\n",
      "    \n",
      "    h_a = (float*)malloc(n * sizeof(float));\n",
      "    h_b = (float*)malloc(n * sizeof(float));\n",
      "    h_c = (float*)malloc(n * sizeof(float));\n",
      "    \n",
      "    for (int i = 0; i < n; i++) {\n",
      "        h_a[i] = i;\n",
      "        h_b[i] = i * 2;\n",
      "    }\n",
      "    \n",
      "    cudaMalloc(&d_a, n * sizeof(float));\n",
      "    cudaMalloc(&d_b, n * sizeof(float));\n",
      "    cudaMalloc(&d_c, n * sizeof(float));\n",
      "    \n",
      "    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\n",
      "    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\n",
      "    \n",
      "    int blockSize = 256;\n",
      "    int gridSize = (int)ceil((float)n / blockSize);\n",
      "    \n",
      "    vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\n",
      "    \n",
      "    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\n",
      "    \n",
      "    for (int i = 0; i < 10; i++) {\n",
      "        printf(\"%f + %f = %f\\n\", h_a[i], h_b[i], h_c[i]);\n",
      "    }\n",
      "    \n",
      "    free(h_a);\n",
      "    free(h_b);\n",
      "    free(h_c);\n",
      "    cudaFree(d_a);\n",
      "    cudaFree(d_b);\n",
      "    cudaFree(d_c);\n",
      "    \n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "[code_validation]\n",
      "Input: {'target_lang': 'CUDA', 'code': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    h_a = (float*)malloc(n * sizeof(float));\\n    h_b = (float*)malloc(n * sizeof(float));\\n    h_c = (float*)malloc(n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    cudaMalloc(&d_a, n * sizeof(float));\\n    cudaMalloc(&d_b, n * sizeof(float));\\n    cudaMalloc(&d_c, n * sizeof(float));\\n    \\n    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\\n    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\\n    \\n    int blockSize = 256;\\n    int gridSize = (int)ceil((float)n / blockSize);\\n    \\n    vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\\n    \\n    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    free(h_a);\\n    free(h_b);\\n    free(h_c);\\n    cudaFree(d_a);\\n    cudaFree(d_b);\\n    cudaFree(d_c);\\n    \\n    return 0;\\n}\\n```'}\n",
      "Output: Issues Found: Yes\n",
      "Rule Violations:\n",
      "- [ARCHITECTURE] Unified Memory is not used (line 15, 16, 17)\n",
      "- [ARCHITECTURE] Kernel function parameters are not optimized for memory access (line 5)\n",
      "- [PERFORMANCE] The number of threads per block is not a multiple of 32 (line 27)\n",
      "- [PERFORMANCE] Global memory is used for atomic operations (line 5)\n",
      "Suggestions:\n",
      "- Use Unified Memory for memory management to simplify data transfers between host and device (line 15, 16, 17)\n",
      "- Optimize kernel function parameters to reduce memory access and improve performance (line 5)\n",
      "- Adjust the number of threads per block to be a multiple of 32 for better performance (line 27)\n",
      "- Avoid using global memory for atomic operations, consider using shared memory or other techniques (line 5)\n",
      "\n",
      "[improvement_request]\n",
      "Input: {'validation_result': 'Issues Found: Yes\\nRule Violations:\\n- [ARCHITECTURE] Unified Memory is not used (line 15, 16, 17)\\n- [ARCHITECTURE] Kernel function parameters are not optimized for memory access (line 5)\\n- [PERFORMANCE] The number of threads per block is not a multiple of 32 (line 27)\\n- [PERFORMANCE] Global memory is used for atomic operations (line 5)\\nSuggestions:\\n- Use Unified Memory for memory management to simplify data transfers between host and device (line 15, 16, 17)\\n- Optimize kernel function parameters to reduce memory access and improve performance (line 5)\\n- Adjust the number of threads per block to be a multiple of 32 for better performance (line 27)\\n- Avoid using global memory for atomic operations, consider using shared memory or other techniques (line 5)', 'previous_code': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    h_a = (float*)malloc(n * sizeof(float));\\n    h_b = (float*)malloc(n * sizeof(float));\\n    h_c = (float*)malloc(n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    cudaMalloc(&d_a, n * sizeof(float));\\n    cudaMalloc(&d_b, n * sizeof(float));\\n    cudaMalloc(&d_c, n * sizeof(float));\\n    \\n    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\\n    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\\n    \\n    int blockSize = 256;\\n    int gridSize = (int)ceil((float)n / blockSize);\\n    \\n    vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\\n    \\n    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    free(h_a);\\n    free(h_b);\\n    free(h_c);\\n    cudaFree(d_a);\\n    cudaFree(d_b);\\n    cudaFree(d_c);\\n    \\n    return 0;\\n}\\n```'}\n",
      "Output: Improve the code based on this report:\n",
      "        Issues Found: Yes\n",
      "Rule Violations:\n",
      "- [ARCHITECTURE] Unified Memory is not used (line 15, 16, 17)\n",
      "- [ARCHITECTURE] Kernel function parameters are not optimized for memory access (line 5)\n",
      "- [PERFORMANCE] The number of threads per block is not a multiple of 32 (line 27)\n",
      "- [PERFORMANCE] Global memory is used for atomic operations (line 5)\n",
      "Suggestions:\n",
      "- Use Unified Memory for memory management to simplify data transfers between host and device (line 15, 16, 17)\n",
      "- Optimize kernel function parameters to reduce memory access and improve performance (line 5)\n",
      "- Adjust the number of threads per block to be a multiple of 32 for better performance (line 27)\n",
      "- Avoid using global memory for atomic operations, consider using shared memory or other techniques (line 5)\n",
      "\n",
      "        \n",
      "        Must follow these rules:\n",
      "        # ARCHITECTURE\n",
      "- Unified Memory must be used\n",
      "- Kernel function parameters should be optimized to merge memory access\n",
      "# PERFORMANCE\n",
      "- The number of threads per block should be a multiple of 32\n",
      "- Avoid using global memory for atomic operations\n",
      "        \n",
      "\n",
      "        Original Code:\n",
      "        ```cuda\n",
      "#include <stdio.h>\n",
      "\n",
      "__global__ void vecAdd(float *a, float *b, float *c, int n) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < n) {\n",
      "        c[i] = a[i] + b[i];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 100000;\n",
      "    float *h_a, *h_b, *h_c;\n",
      "    float *d_a, *d_b, *d_c;\n",
      "    \n",
      "    h_a = (float*)malloc(n * sizeof(float));\n",
      "    h_b = (float*)malloc(n * sizeof(float));\n",
      "    h_c = (float*)malloc(n * sizeof(float));\n",
      "    \n",
      "    for (int i = 0; i < n; i++) {\n",
      "        h_a[i] = i;\n",
      "        h_b[i] = i * 2;\n",
      "    }\n",
      "    \n",
      "    cudaMalloc(&d_a, n * sizeof(float));\n",
      "    cudaMalloc(&d_b, n * sizeof(float));\n",
      "    cudaMalloc(&d_c, n * sizeof(float));\n",
      "    \n",
      "    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\n",
      "    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\n",
      "    \n",
      "    int blockSize = 256;\n",
      "    int gridSize = (int)ceil((float)n / blockSize);\n",
      "    \n",
      "    vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\n",
      "    \n",
      "    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\n",
      "    \n",
      "    for (int i = 0; i < 10; i++) {\n",
      "        printf(\"%f + %f = %f\\n\", h_a[i], h_b[i], h_c[i]);\n",
      "    }\n",
      "    \n",
      "    free(h_a);\n",
      "    free(h_b);\n",
      "    free(h_c);\n",
      "    cudaFree(d_a);\n",
      "    cudaFree(d_b);\n",
      "    cudaFree(d_c);\n",
      "    \n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "        Requirements:\n",
      "        1. Strictly follow the suggestions\n",
      "        2. Preserve functionality\n",
      "        3. Return complete revised code\n",
      "        \n",
      "\n",
      "[improvement_response]\n",
      "Input: {'prompt': 'Improve the code based on this report:\\n        Issues Found: Yes\\nRule Violations:\\n- [ARCHITECTURE] Unified Memory is not used (line 15, 16, 17)\\n- [ARCHITECTURE] Kernel function parameters are not optimized for memory access (line 5)\\n- [PERFORMANCE] The number of threads per block is not a multiple of 32 (line 27)\\n- [PERFORMANCE] Global memory is used for atomic operations (line 5)\\nSuggestions:\\n- Use Unified Memory for memory management to simplify data transfers between host and device (line 15, 16, 17)\\n- Optimize kernel function parameters to reduce memory access and improve performance (line 5)\\n- Adjust the number of threads per block to be a multiple of 32 for better performance (line 27)\\n- Avoid using global memory for atomic operations, consider using shared memory or other techniques (line 5)\\n\\n        \\n        Must follow these rules:\\n        # ARCHITECTURE\\n- Unified Memory must be used\\n- Kernel function parameters should be optimized to merge memory access\\n# PERFORMANCE\\n- The number of threads per block should be a multiple of 32\\n- Avoid using global memory for atomic operations\\n        \\n\\n        Original Code:\\n        ```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    h_a = (float*)malloc(n * sizeof(float));\\n    h_b = (float*)malloc(n * sizeof(float));\\n    h_c = (float*)malloc(n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    cudaMalloc(&d_a, n * sizeof(float));\\n    cudaMalloc(&d_b, n * sizeof(float));\\n    cudaMalloc(&d_c, n * sizeof(float));\\n    \\n    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\\n    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\\n    \\n    int blockSize = 256;\\n    int gridSize = (int)ceil((float)n / blockSize);\\n    \\n    vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\\n    \\n    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    free(h_a);\\n    free(h_b);\\n    free(h_c);\\n    cudaFree(d_a);\\n    cudaFree(d_b);\\n    cudaFree(d_c);\\n    \\n    return 0;\\n}\\n```\\n\\n        Requirements:\\n        1. Strictly follow the suggestions\\n        2. Preserve functionality\\n        3. Return complete revised code\\n        '}\n",
      "Output: ```cuda\n",
      "#include <stdio.h>\n",
      "\n",
      "__global__ void vecAdd(float *a, float *b, float *c, int n) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < n) {\n",
      "        c[i] = a[i] + b[i];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 100000;\n",
      "    float *h_a, *h_b, *h_c;\n",
      "    float *d_a, *d_b, *d_c;\n",
      "    \n",
      "    cudaMallocManaged(&h_a, n * sizeof(float));\n",
      "    cudaMallocManaged(&h_b, n * sizeof(float));\n",
      "    cudaMallocManaged(&h_c, n * sizeof(float));\n",
      "    \n",
      "    for (int i = 0; i < n; i++) {\n",
      "        h_a[i] = i;\n",
      "        h_b[i] = i * 2;\n",
      "    }\n",
      "    \n",
      "    int blockSize = 256;\n",
      "    int gridSize = (n + blockSize - 1) / blockSize;\n",
      "    \n",
      "    vecAdd<<<gridSize, blockSize>>>(h_a, h_b, h_c, n);\n",
      "    cudaDeviceSynchronize();\n",
      "    \n",
      "    for (int i = 0; i < 10; i++) {\n",
      "        printf(\"%f + %f = %f\\n\", h_a[i], h_b[i], h_c[i]);\n",
      "    }\n",
      "    \n",
      "    cudaFree(h_a);\n",
      "    cudaFree(h_b);\n",
      "    cudaFree(h_c);\n",
      "    \n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "[code_validation]\n",
      "Input: {'target_lang': 'CUDA', 'code': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    cudaMallocManaged(&h_a, n * sizeof(float));\\n    cudaMallocManaged(&h_b, n * sizeof(float));\\n    cudaMallocManaged(&h_c, n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    int blockSize = 256;\\n    int gridSize = (n + blockSize - 1) / blockSize;\\n    \\n    vecAdd<<<gridSize, blockSize>>>(h_a, h_b, h_c, n);\\n    cudaDeviceSynchronize();\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    cudaFree(h_a);\\n    cudaFree(h_b);\\n    cudaFree(h_c);\\n    \\n    return 0;\\n}\\n```'}\n",
      "Output: Issues Found: No\n",
      "Rule Violations:\n",
      "- [Rule1] No violation found\n",
      "- [Rule2] No violation found\n",
      "Suggestions: \n",
      "- The code follows the architecture and performance rules provided.\n",
      "\n",
      "[iteration_summary]\n",
      "Input: {'initial_code': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    h_a = (float*)malloc(n * sizeof(float));\\n    h_b = (float*)malloc(n * sizeof(float));\\n    h_c = (float*)malloc(n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    cudaMalloc(&d_a, n * sizeof(float));\\n    cudaMalloc(&d_b, n * sizeof(float));\\n    cudaMalloc(&d_c, n * sizeof(float));\\n    \\n    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\\n    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\\n    \\n    int blockSize = 256;\\n    int gridSize = (int)ceil((float)n / blockSize);\\n    \\n    vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\\n    \\n    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    free(h_a);\\n    free(h_b);\\n    free(h_c);\\n    cudaFree(d_a);\\n    cudaFree(d_b);\\n    cudaFree(d_c);\\n    \\n    return 0;\\n}\\n```', 'max_iterations': 3, 'target_language': 'CUDA'}\n",
      "Output: {'final_code': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    cudaMallocManaged(&h_a, n * sizeof(float));\\n    cudaMallocManaged(&h_b, n * sizeof(float));\\n    cudaMallocManaged(&h_c, n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    int blockSize = 256;\\n    int gridSize = (n + blockSize - 1) / blockSize;\\n    \\n    vecAdd<<<gridSize, blockSize>>>(h_a, h_b, h_c, n);\\n    cudaDeviceSynchronize();\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    cudaFree(h_a);\\n    cudaFree(h_b);\\n    cudaFree(h_c);\\n    \\n    return 0;\\n}\\n```', 'iterations': [{'iteration': 1, 'code_before': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    h_a = (float*)malloc(n * sizeof(float));\\n    h_b = (float*)malloc(n * sizeof(float));\\n    h_c = (float*)malloc(n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    cudaMalloc(&d_a, n * sizeof(float));\\n    cudaMalloc(&d_b, n * sizeof(float));\\n    cudaMalloc(&d_c, n * sizeof(float));\\n    \\n    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\\n    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\\n    \\n    int blockSize = 256;\\n    int gridSize = (int)ceil((float)n / blockSize);\\n    \\n    vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\\n    \\n    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    free(h_a);\\n    free(h_b);\\n    free(h_c);\\n    cudaFree(d_a);\\n    cudaFree(d_b);\\n    cudaFree(d_c);\\n    \\n    return 0;\\n}\\n```', 'validation_result': 'Issues Found: Yes\\nRule Violations:\\n- [ARCHITECTURE] Unified Memory is not used (line 15, 16, 17)\\n- [ARCHITECTURE] Kernel function parameters are not optimized for memory access (line 5)\\n- [PERFORMANCE] The number of threads per block is not a multiple of 32 (line 27)\\n- [PERFORMANCE] Global memory is used for atomic operations (line 5)\\nSuggestions:\\n- Use Unified Memory for memory management to simplify data transfers between host and device (line 15, 16, 17)\\n- Optimize kernel function parameters to reduce memory access and improve performance (line 5)\\n- Adjust the number of threads per block to be a multiple of 32 for better performance (line 27)\\n- Avoid using global memory for atomic operations, consider using shared memory or other techniques (line 5)', 'action': 'code_improved', 'code_after': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    cudaMallocManaged(&h_a, n * sizeof(float));\\n    cudaMallocManaged(&h_b, n * sizeof(float));\\n    cudaMallocManaged(&h_c, n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    int blockSize = 256;\\n    int gridSize = (n + blockSize - 1) / blockSize;\\n    \\n    vecAdd<<<gridSize, blockSize>>>(h_a, h_b, h_c, n);\\n    cudaDeviceSynchronize();\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    cudaFree(h_a);\\n    cudaFree(h_b);\\n    cudaFree(h_c);\\n    \\n    return 0;\\n}\\n```'}, {'iteration': 2, 'code_before': '```cuda\\n#include <stdio.h>\\n\\n__global__ void vecAdd(float *a, float *b, float *c, int n) {\\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (i < n) {\\n        c[i] = a[i] + b[i];\\n    }\\n}\\n\\nint main() {\\n    int n = 100000;\\n    float *h_a, *h_b, *h_c;\\n    float *d_a, *d_b, *d_c;\\n    \\n    cudaMallocManaged(&h_a, n * sizeof(float));\\n    cudaMallocManaged(&h_b, n * sizeof(float));\\n    cudaMallocManaged(&h_c, n * sizeof(float));\\n    \\n    for (int i = 0; i < n; i++) {\\n        h_a[i] = i;\\n        h_b[i] = i * 2;\\n    }\\n    \\n    int blockSize = 256;\\n    int gridSize = (n + blockSize - 1) / blockSize;\\n    \\n    vecAdd<<<gridSize, blockSize>>>(h_a, h_b, h_c, n);\\n    cudaDeviceSynchronize();\\n    \\n    for (int i = 0; i < 10; i++) {\\n        printf(\"%f + %f = %f\\\\n\", h_a[i], h_b[i], h_c[i]);\\n    }\\n    \\n    cudaFree(h_a);\\n    cudaFree(h_b);\\n    cudaFree(h_c);\\n    \\n    return 0;\\n}\\n```', 'validation_result': 'Issues Found: No\\nRule Violations:\\n- [Rule1] No violation found\\n- [Rule2] No violation found\\nSuggestions: \\n- The code follows the architecture and performance rules provided.', 'action': 'validation_passed'}], 'total_iterations': 2}\n",
      "\n",
      "[final_result]\n",
      "Input: {'iterations': 3}\n",
      "Output: ```cuda\n",
      "#include <stdio.h>\n",
      "\n",
      "__global__ void vecAdd(float *a, float *b, float *c, int n) {\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < n) {\n",
      "        c[i] = a[i] + b[i];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 100000;\n",
      "    float *h_a, *h_b, *h_c;\n",
      "    float *d_a, *d_b, *d_c;\n",
      "    \n",
      "    cudaMallocManaged(&h_a, n * sizeof(float));\n",
      "    cudaMallocManaged(&h_b, n * sizeof(float));\n",
      "    cudaMallocManaged(&h_c, n * sizeof(float));\n",
      "    \n",
      "    for (int i = 0; i < n; i++) {\n",
      "        h_a[i] = i;\n",
      "        h_b[i] = i * 2;\n",
      "    }\n",
      "    \n",
      "    int blockSize = 256;\n",
      "    int gridSize = (n + blockSize - 1) / blockSize;\n",
      "    \n",
      "    vecAdd<<<gridSize, blockSize>>>(h_a, h_b, h_c, n);\n",
      "    cudaDeviceSynchronize();\n",
      "    \n",
      "    for (int i = 0; i < 10; i++) {\n",
      "        printf(\"%f + %f = %f\\n\", h_a[i], h_b[i], h_c[i]);\n",
      "    }\n",
      "    \n",
      "    cudaFree(h_a);\n",
      "    cudaFree(h_b);\n",
      "    cudaFree(h_c);\n",
      "    \n",
      "    return 0;\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    system = CodeTranslationSystem(\"D:/Projects/HPCAgent/KB/code_rules.yaml\")\n",
    "    \n",
    "    # Example Input\n",
    "    user_input = \"\"\"\n",
    "    Please help me convert the following FORTRAN code into CUDA code:\n",
    "    PROGRAM VECTOR_ADD\n",
    "    INTEGER, PARAMETER :: N = 1000000\n",
    "    REAL :: A(N), B(N), C(N)\n",
    "    DO I = 1, N\n",
    "        C(I) = A(I) + B(I)\n",
    "    END DO\n",
    "    END PROGRAM\n",
    "    \"\"\"\n",
    "    \n",
    "    result = system.process_request(user_input)\n",
    "    print(\"Conversion Results:\")\n",
    "    print(result[\"translated_code\"])\n",
    "    \n",
    "    print(\"\\n=== Full Execution Log ===\")\n",
    "    for log in result[\"execution_log\"]:\n",
    "        print(f\"\\n[{log['step']}]\")\n",
    "        print(\"Input:\", log.get(\"input\"))\n",
    "        print(\"Output:\", log.get(\"output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9555db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
